[
  {
    "objectID": "posts/laptop_price_prediction/laptop_price_prediction.html",
    "href": "posts/laptop_price_prediction/laptop_price_prediction.html",
    "title": "Laptop Price Prediction",
    "section": "",
    "text": "This project aims to predict laptop prices based on various features such as specifications, brand, and market.\n\n\nThe dataset used for this project is sourced from Kaggle. It contains detailed information about various laptop models, including specifications such as processor type, RAM, storage, display size, and brand. The target variable is the price of the laptops.\n\n\n\n\n\nDevelop a model that can accurately predict laptop prices based on various features, helping our clients stay competitive in the market.\n\n\n\nUnderstand how different features contribute to pricing, enabling SmartTech Co. to strategically position its laptops in the market.\n\n\n\nAssess the impact of brand reputation on pricing, providing insights into brand perception and market demand.\n\n\n\n\n\n\nPandas, NumPy, Matplotlib, and Seaborn are used for data manipulation, numerical operations, and visualization.\n\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nImporting dataset\n\ndf = pd.read_csv(\"F:/Odin_school/Capstone_projects/ml_capstone/laptop.csv\")\n\ndf.head()\n\n\n\n\n\n\n\n\nUnnamed: 0.1\nUnnamed: 0\nCompany\nTypeName\nInches\nScreenResolution\nCpu\nRam\nMemory\nGpu\nOpSys\nWeight\nPrice\n\n\n\n\n0\n0\n0.0\nApple\nUltrabook\n13.3\nIPS Panel Retina Display 2560x1600\nIntel Core i5 2.3GHz\n8GB\n128GB SSD\nIntel Iris Plus Graphics 640\nmacOS\n1.37kg\n71378.6832\n\n\n1\n1\n1.0\nApple\nUltrabook\n13.3\n1440x900\nIntel Core i5 1.8GHz\n8GB\n128GB Flash Storage\nIntel HD Graphics 6000\nmacOS\n1.34kg\n47895.5232\n\n\n2\n2\n2.0\nHP\nNotebook\n15.6\nFull HD 1920x1080\nIntel Core i5 7200U 2.5GHz\n8GB\n256GB SSD\nIntel HD Graphics 620\nNo OS\n1.86kg\n30636.0000\n\n\n3\n3\n3.0\nApple\nUltrabook\n15.4\nIPS Panel Retina Display 2880x1800\nIntel Core i7 2.7GHz\n16GB\n512GB SSD\nAMD Radeon Pro 455\nmacOS\n1.83kg\n135195.3360\n\n\n4\n4\n4.0\nApple\nUltrabook\n13.3\nIPS Panel Retina Display 2560x1600\nIntel Core i5 3.1GHz\n8GB\n256GB SSD\nIntel Iris Plus Graphics 650\nmacOS\n1.37kg\n96095.8080\n\n\n\n\n\n\n\n\n\n\n\n\n\n\ndf.isnull().sum()\n\nUnnamed: 0.1         0\nUnnamed: 0          30\nCompany             30\nTypeName            30\nInches              30\nScreenResolution    30\nCpu                 30\nRam                 30\nMemory              30\nGpu                 30\nOpSys               30\nWeight              30\nPrice               30\ndtype: int64\n\n\nAll columns except Unnamed: 0.1 has missing values, which might mean that first column may just be an index column. Let’s drop it and check Unnamed: 0 column.\n\ndf = df.drop(['Unnamed: 0.1'], axis=1)\n\ndf.isnull().sum()\n\nUnnamed: 0          30\nCompany             30\nTypeName            30\nInches              30\nScreenResolution    30\nCpu                 30\nRam                 30\nMemory              30\nGpu                 30\nOpSys               30\nWeight              30\nPrice               30\ndtype: int64\n\n\nNull values are still present lets drop the rows with Null values.\n\ndf = df.dropna()\ndf.info()\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nIndex: 1273 entries, 0 to 1302\nData columns (total 12 columns):\n #   Column            Non-Null Count  Dtype  \n---  ------            --------------  -----  \n 0   Unnamed: 0        1273 non-null   float64\n 1   Company           1273 non-null   object \n 2   TypeName          1273 non-null   object \n 3   Inches            1273 non-null   object \n 4   ScreenResolution  1273 non-null   object \n 5   Cpu               1273 non-null   object \n 6   Ram               1273 non-null   object \n 7   Memory            1273 non-null   object \n 8   Gpu               1273 non-null   object \n 9   OpSys             1273 non-null   object \n 10  Weight            1273 non-null   object \n 11  Price             1273 non-null   float64\ndtypes: float64(2), object(10)\nmemory usage: 129.3+ KB\n\n\nThere are 1273 non-null entries in the dataset now, which means we have successfully removed rows with missing values. Let’s check if Unnamed: 0 column is just an index column or not.\n\ndf['Unnamed: 0'].nunique()\n\n1273\n\n\nIt seems that Unnamed: 0 column is just an index column, let’s drop it as well.\n\ndf = df.drop(['Unnamed: 0'], axis=1)\ndf.info()\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nIndex: 1273 entries, 0 to 1302\nData columns (total 11 columns):\n #   Column            Non-Null Count  Dtype  \n---  ------            --------------  -----  \n 0   Company           1273 non-null   object \n 1   TypeName          1273 non-null   object \n 2   Inches            1273 non-null   object \n 3   ScreenResolution  1273 non-null   object \n 4   Cpu               1273 non-null   object \n 5   Ram               1273 non-null   object \n 6   Memory            1273 non-null   object \n 7   Gpu               1273 non-null   object \n 8   OpSys             1273 non-null   object \n 9   Weight            1273 non-null   object \n 10  Price             1273 non-null   float64\ndtypes: float64(1), object(10)\nmemory usage: 119.3+ KB\n\n\nNow we have 1273 non-null entries in the dataset and no missing values. Let’s check for duplicates in the data.\n\nprint(df.duplicated().sum())\n\ndf[df.duplicated()].sort_values(by='Company', ascending=True).head(3)\n\n29\n\n\n\n\n\n\n\n\n\nCompany\nTypeName\nInches\nScreenResolution\nCpu\nRam\nMemory\nGpu\nOpSys\nWeight\nPrice\n\n\n\n\n1291\nAcer\nNotebook\n15.6\n1366x768\nIntel Celeron Dual Core N3060 1.6GHz\n4GB\n500GB HDD\nIntel HD Graphics 400\nLinux\n2.4kg\n15397.92\n\n\n1277\nAcer\nNotebook\n15.6\n1366x768\nIntel Celeron Dual Core N3060 1.6GHz\n4GB\n500GB HDD\nIntel HD Graphics 400\nLinux\n2.4kg\n15397.92\n\n\n1274\nAsus\nNotebook\n15.6\n1366x768\nIntel Celeron Dual Core N3050 1.6GHz\n4GB\n500GB HDD\nIntel HD Graphics\nWindows 10\n2.2kg\n19660.32\n\n\n\n\n\n\n\nThere are 29 duplicate records in the dataset, let’s drop them.\n\ndf = df.drop_duplicates()\n\ndf.info()\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nIndex: 1244 entries, 0 to 1273\nData columns (total 11 columns):\n #   Column            Non-Null Count  Dtype  \n---  ------            --------------  -----  \n 0   Company           1244 non-null   object \n 1   TypeName          1244 non-null   object \n 2   Inches            1244 non-null   object \n 3   ScreenResolution  1244 non-null   object \n 4   Cpu               1244 non-null   object \n 5   Ram               1244 non-null   object \n 6   Memory            1244 non-null   object \n 7   Gpu               1244 non-null   object \n 8   OpSys             1244 non-null   object \n 9   Weight            1244 non-null   object \n 10  Price             1244 non-null   float64\ndtypes: float64(1), object(10)\nmemory usage: 116.6+ KB\n\n\nNow we have 1244 non-null entries in the dataset and no missing values or duplicates.\nThere are 11 columns in the dataset, among which Price is the only column with folat64 data type rest are object data type.\n\n\n\n\nLet’s start with the basic statistics of the dataset.\n\ndf.describe(include='all')\n\n\n\n\n\n\n\n\nCompany\nTypeName\nInches\nScreenResolution\nCpu\nRam\nMemory\nGpu\nOpSys\nWeight\nPrice\n\n\n\n\ncount\n1244\n1244\n1244\n1244\n1244\n1244\n1244\n1244\n1244\n1244\n1244.000000\n\n\nunique\n19\n6\n25\n40\n118\n10\n40\n110\n9\n189\nNaN\n\n\ntop\nLenovo\nNotebook\n15.6\nFull HD 1920x1080\nIntel Core i5 7200U 2.5GHz\n8GB\n256GB SSD\nIntel HD Graphics 620\nWindows 10\n2.2kg\nNaN\n\n\nfreq\n282\n689\n621\n493\n183\n595\n401\n269\n1022\n106\nNaN\n\n\nmean\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n60606.224427\n\n\nstd\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n37424.636161\n\n\nmin\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n9270.720000\n\n\n25%\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n32655.445200\n\n\n50%\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n52693.920000\n\n\n75%\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n79813.440000\n\n\nmax\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n324954.720000\n\n\n\n\n\n\n\nThe dataset contains 11 columns with the following features: - Company: The brand of the laptop with 19 brands. - TypeName: The type of laptop, such as Ultrabook, Gaming, etc. - Inches: The size of the laptop screen in inches. - ScreenResolution: The resolution of the laptop screen. - Cpu: The type of CPU used in the laptop. - Ram: The amount of RAM in GB. - Memory: The type and size of storage memory (HDD/SSD). - Gpu: The type of GPU used in the laptop. - OpSys: The operating system installed on the laptop. - Weight: The weight of the laptop in kg. - Price: The price of the laptop in INR(Indian Rupees).\n\n\nLet’s extract some features from the existing columns to make the dataset more informative.\nFirst lets convert all the column names to lower case for consistency.\n\ndf.columns = df.columns.str.lower()\n\nprint(df.columns)\n\nIndex(['company', 'typename', 'inches', 'screenresolution', 'cpu', 'ram',\n       'memory', 'gpu', 'opsys', 'weight', 'price'],\n      dtype='object')\n\n\nLet’s check for the unique values in columns.\n\nprint(df.company.unique())\nprint(df.typename.unique())\nprint(df.ram.unique())\nprint(df.opsys.unique())\n\n['Apple' 'HP' 'Acer' 'Asus' 'Dell' 'Lenovo' 'Chuwi' 'MSI' 'Microsoft'\n 'Toshiba' 'Huawei' 'Xiaomi' 'Vero' 'Razer' 'Mediacom' 'Samsung' 'Google'\n 'Fujitsu' 'LG']\n['Ultrabook' 'Notebook' 'Gaming' '2 in 1 Convertible' 'Workstation'\n 'Netbook']\n['8GB' '16GB' '4GB' '2GB' '12GB' '64GB' '6GB' '32GB' '24GB' '1GB']\n['macOS' 'No OS' 'Windows 10' 'Mac OS X' 'Linux' 'Windows 10 S'\n 'Chrome OS' 'Windows 7' 'Android']\n\n\n\nprint(df.cpu[1])\nprint(df.screenresolution[6])\nprint(df.gpu[88])\n\nIntel Core i5 1.8GHz\nIPS Panel Retina Display 2880x1800\nNvidia GeForce GTX 1060\n\n\nwe can seperate values in columns to form new parameters i.e, - screenresolution can give us display_type, resolution & touchscreen - cpu can be seperated to form cpu and clockspeed - memory can be seperated to form memory and memory_type - gpu can be seperated to get gpu_company\n\n\n\n# cpu brand name\ndf['cpu_brand'] = df.cpu.str.split().str[0]\n\n# cpu name\ndf['cpu_name'] = df.cpu.str.replace(r'\\d+(?:\\.\\d+)?GHz', '', regex=True,).str.strip()\n# removing brand name\ndf['cpu_name'] = df.cpu_name.str.replace(r'^\\w+', '', regex=True).str.strip()\n\n# cpu clock speed\ndf['cpu_ghz'] = df.cpu.str.extract(r'(\\d+(?:\\.\\d+)?)GHz').astype('float64')\n\ndf[['cpu_brand', 'cpu_name', 'cpu_ghz']]\n\n\n\n\n\n\n\n\ncpu_brand\ncpu_name\ncpu_ghz\n\n\n\n\n0\nIntel\nCore i5\n2.3\n\n\n1\nIntel\nCore i5\n1.8\n\n\n2\nIntel\nCore i5 7200U\n2.5\n\n\n3\nIntel\nCore i7\n2.7\n\n\n4\nIntel\nCore i5\n3.1\n\n\n...\n...\n...\n...\n\n\n1269\nIntel\nCore i7 6500U\n2.5\n\n\n1270\nIntel\nCore i7 6500U\n2.5\n\n\n1271\nIntel\nCore i7 6500U\n2.5\n\n\n1272\nIntel\nCeleron Dual Core N3050\n1.6\n\n\n1273\nIntel\nCore i7 6500U\n2.5\n\n\n\n\n1244 rows × 3 columns\n\n\n\nNow, we have 3 columns act as seperate features for the price prediction.\n\n\n\n\nscreenresolution has many features ie., screen type, screen height, width, touch screen etc. Let’s extract all of them\n\n\n# display resolution\ndf['resolution'] = df['screenresolution'].str.extract(r'(\\d+x\\d+)')\n\n# touch screen or not\ndf['touchscreen'] = df['screenresolution'].apply(lambda x: 1 if 'Touchscreen' in x else 0)\n\n# Display type\ndf['display_type'] = df['screenresolution'].str.replace(r'\\d+x\\d+', \"\", regex = True).str.strip()\n\ndf['display_type'] = df['display_type'].str.replace(r'(Full HD|Quad HD|4K Ultra HD|/|\\+|Touchscreen)', '', regex = True).str.replace('/', '', regex = True).str.strip()\n\ndf[['resolution', 'touchscreen', 'display_type']]\n\n\n\n\n\n\n\n\nresolution\ntouchscreen\ndisplay_type\n\n\n\n\n0\n2560x1600\n0\nIPS Panel Retina Display\n\n\n1\n1440x900\n0\n\n\n\n2\n1920x1080\n0\n\n\n\n3\n2880x1800\n0\nIPS Panel Retina Display\n\n\n4\n2560x1600\n0\nIPS Panel Retina Display\n\n\n...\n...\n...\n...\n\n\n1269\n1366x768\n0\n\n\n\n1270\n1920x1080\n1\nIPS Panel\n\n\n1271\n3200x1800\n1\nIPS Panel\n\n\n1272\n1366x768\n0\n\n\n\n1273\n1366x768\n0\n\n\n\n\n\n1244 rows × 3 columns\n\n\n\nNow, we have another 3 columns to act as 3 seperate features.\n\ndf.touchscreen.sum()\n\nnp.int64(181)\n\n\n\n\n\nLet’s extarct gpu_brand and gpu_name from the column gpu\n\n# gpu brand\ndf['gpu_brand'] = df['gpu'].str.extract(r'^(\\w+)')\n\n# gpu name\ndf['gpu_name'] = df['gpu'].str.replace(r'^(\\w+)', '', regex = True).str.strip()\n\ndf[['gpu_brand', 'gpu_name']]\n\n\n\n\n\n\n\n\ngpu_brand\ngpu_name\n\n\n\n\n0\nIntel\nIris Plus Graphics 640\n\n\n1\nIntel\nHD Graphics 6000\n\n\n2\nIntel\nHD Graphics 620\n\n\n3\nAMD\nRadeon Pro 455\n\n\n4\nIntel\nIris Plus Graphics 650\n\n\n...\n...\n...\n\n\n1269\nNvidia\nGeForce 920M\n\n\n1270\nIntel\nHD Graphics 520\n\n\n1271\nIntel\nHD Graphics 520\n\n\n1272\nIntel\nHD Graphics\n\n\n1273\nAMD\nRadeon R5 M330\n\n\n\n\n1244 rows × 2 columns\n\n\n\n\n\n\nMost of the laptops have two drives which need to be seperated and type of memory is also in the memory so we need to seperate them both after seperating the drives.\n\nFirst replace the TB with GB(1TB ~ 1000GB)\n+ seperates two drives, str.split() function can be used to list the two memory drives and then they are slotted into seperate columns.\n\n\ndf.memory = df.memory.str.replace(r'1.0TB|1TB', \"1000GB\", regex = True)\ndf.memory = df.memory.str.replace(r'2.0TB|2TB', \"2000GB\", regex = True)\n\ndf.memory.unique()\n\narray(['128GB SSD', '128GB Flash Storage', '256GB SSD', '512GB SSD',\n       '500GB HDD', '256GB Flash Storage', '1000GB HDD',\n       '128GB SSD +  1000GB HDD', '256GB SSD +  256GB SSD',\n       '64GB Flash Storage', '32GB Flash Storage',\n       '256GB SSD +  1000GB HDD', '256GB SSD +  2000GB HDD', '32GB SSD',\n       '2000GB HDD', '64GB SSD', '1000GB Hybrid',\n       '512GB SSD +  1000GB HDD', '1000GB SSD', '256GB SSD +  500GB HDD',\n       '128GB SSD +  2000GB HDD', '512GB SSD +  512GB SSD', '16GB SSD',\n       '16GB Flash Storage', '512GB SSD +  256GB SSD',\n       '512GB SSD +  2000GB HDD', '64GB Flash Storage +  1000GB HDD',\n       '180GB SSD', '1000GB HDD +  1000GB HDD', '32GB HDD',\n       '1000GB SSD +  1000GB HDD', '?', '512GB Flash Storage',\n       '128GB HDD', '240GB SSD', '8GB SSD', '508GB Hybrid',\n       '512GB SSD +  1000GB Hybrid', '256GB SSD +  1000GB Hybrid'],\n      dtype=object)\n\n\n\ndf['memory_list'] = df.memory.str.split('+')\n\ndf['memory_1'] = df['memory_list'].str[0]\ndf['memory_2'] = df['memory_list'].str[1]\n\ndf[['memory_1', 'memory_2']]\n\n\n\n\n\n\n\n\nmemory_1\nmemory_2\n\n\n\n\n0\n128GB SSD\nNaN\n\n\n1\n128GB Flash Storage\nNaN\n\n\n2\n256GB SSD\nNaN\n\n\n3\n512GB SSD\nNaN\n\n\n4\n256GB SSD\nNaN\n\n\n...\n...\n...\n\n\n1269\n500GB HDD\nNaN\n\n\n1270\n128GB SSD\nNaN\n\n\n1271\n512GB SSD\nNaN\n\n\n1272\n64GB Flash Storage\nNaN\n\n\n1273\n1000GB HDD\nNaN\n\n\n\n\n1244 rows × 2 columns\n\n\n\nLet’s seperate df['memory_1'] into 2 seperate columns for memory_capacity and memory_type\n\ndf['memory_capacity_1'] = df['memory_1'].str.extract(r'(\\d+)').astype('float64')\ndf['memory_type_1'] = df['memory_1'].str.replace(r'(\\d+[A-Z]{2})', '', regex = True).str.strip()\n\ndf[['memory_capacity_1', 'memory_type_1']]\n\n\n\n\n\n\n\n\nmemory_capacity_1\nmemory_type_1\n\n\n\n\n0\n128.0\nSSD\n\n\n1\n128.0\nFlash Storage\n\n\n2\n256.0\nSSD\n\n\n3\n512.0\nSSD\n\n\n4\n256.0\nSSD\n\n\n...\n...\n...\n\n\n1269\n500.0\nHDD\n\n\n1270\n128.0\nSSD\n\n\n1271\n512.0\nSSD\n\n\n1272\n64.0\nFlash Storage\n\n\n1273\n1000.0\nHDD\n\n\n\n\n1244 rows × 2 columns\n\n\n\nLet’s repeat this for memory_2 also\n\ndf['memory_capacity_2'] = df['memory_2'].str.extract(r'(\\d+)').astype('float64')\ndf['memory_type_2'] = df['memory_2'].str.replace(r'(\\d+[A-Z]{2})', '', regex = True).str.strip()\n\ndf[['memory_capacity_2', 'memory_type_2']].dropna()\n\n\n\n\n\n\n\n\nmemory_capacity_2\nmemory_type_2\n\n\n\n\n21\n1000.0\nHDD\n\n\n28\n256.0\nSSD\n\n\n37\n1000.0\nHDD\n\n\n41\n1000.0\nHDD\n\n\n47\n1000.0\nHDD\n\n\n...\n...\n...\n\n\n1233\n1000.0\nHDD\n\n\n1238\n1000.0\nHDD\n\n\n1247\n1000.0\nHDD\n\n\n1256\n1000.0\nHDD\n\n\n1259\n1000.0\nHDD\n\n\n\n\n204 rows × 2 columns\n\n\n\n\n\n\nLet’s convert all the columns that can be numeric into numeric or float i.e, ram, inches, weight\n\ndf['ram_gb'] = df['ram'].str.replace('GB', '').astype('int')\n\ndf['inches_size'] = pd.to_numeric(df['inches'], errors= 'coerce')\n\ndf['weight_kg'] = df['weight'].replace('?', np.nan).str.replace('kg', '').astype('float64')\n\ndf[['ram_gb', 'inches_size', 'weight_kg']]\n\n\n\n\n\n\n\n\nram_gb\ninches_size\nweight_kg\n\n\n\n\n0\n8\n13.3\n1.37\n\n\n1\n8\n13.3\n1.34\n\n\n2\n8\n15.6\n1.86\n\n\n3\n16\n15.4\n1.83\n\n\n4\n8\n13.3\n1.37\n\n\n...\n...\n...\n...\n\n\n1269\n4\n15.6\n2.20\n\n\n1270\n4\n14.0\n1.80\n\n\n1271\n16\n13.3\n1.30\n\n\n1272\n2\n14.0\n1.50\n\n\n1273\n6\n15.6\n2.19\n\n\n\n\n1244 rows × 3 columns\n\n\n\nLet’s look at data once more\n\ndf.info()\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nIndex: 1244 entries, 0 to 1273\nData columns (total 29 columns):\n #   Column             Non-Null Count  Dtype  \n---  ------             --------------  -----  \n 0   company            1244 non-null   object \n 1   typename           1244 non-null   object \n 2   inches             1244 non-null   object \n 3   screenresolution   1244 non-null   object \n 4   cpu                1244 non-null   object \n 5   ram                1244 non-null   object \n 6   memory             1244 non-null   object \n 7   gpu                1244 non-null   object \n 8   opsys              1244 non-null   object \n 9   weight             1244 non-null   object \n 10  price              1244 non-null   float64\n 11  cpu_brand          1244 non-null   object \n 12  cpu_name           1244 non-null   object \n 13  cpu_ghz            1244 non-null   float64\n 14  resolution         1244 non-null   object \n 15  touchscreen        1244 non-null   int64  \n 16  display_type       1244 non-null   object \n 17  gpu_brand          1244 non-null   object \n 18  gpu_name           1244 non-null   object \n 19  memory_list        1244 non-null   object \n 20  memory_1           1244 non-null   object \n 21  memory_2           204 non-null    object \n 22  memory_capacity_1  1243 non-null   float64\n 23  memory_type_1      1244 non-null   object \n 24  memory_capacity_2  204 non-null    float64\n 25  memory_type_2      204 non-null    object \n 26  ram_gb             1244 non-null   int64  \n 27  inches_size        1243 non-null   float64\n 28  weight_kg          1243 non-null   float64\ndtypes: float64(6), int64(2), object(21)\nmemory usage: 323.9+ KB\n\n\n11 columns just were made into 29 columns among which repeated columns are not necessary to build a model so let’s remove them.\n\ndf_clean = df.drop(columns = ['ram','screenresolution', 'cpu', 'memory', 'memory_list',\n                              'memory_1', 'memory_2' ,'gpu', 'weight', 'inches'])\n\nprint(df_clean.info())\ndf_clean.head(5)\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nIndex: 1244 entries, 0 to 1273\nData columns (total 19 columns):\n #   Column             Non-Null Count  Dtype  \n---  ------             --------------  -----  \n 0   company            1244 non-null   object \n 1   typename           1244 non-null   object \n 2   opsys              1244 non-null   object \n 3   price              1244 non-null   float64\n 4   cpu_brand          1244 non-null   object \n 5   cpu_name           1244 non-null   object \n 6   cpu_ghz            1244 non-null   float64\n 7   resolution         1244 non-null   object \n 8   touchscreen        1244 non-null   int64  \n 9   display_type       1244 non-null   object \n 10  gpu_brand          1244 non-null   object \n 11  gpu_name           1244 non-null   object \n 12  memory_capacity_1  1243 non-null   float64\n 13  memory_type_1      1244 non-null   object \n 14  memory_capacity_2  204 non-null    float64\n 15  memory_type_2      204 non-null    object \n 16  ram_gb             1244 non-null   int64  \n 17  inches_size        1243 non-null   float64\n 18  weight_kg          1243 non-null   float64\ndtypes: float64(6), int64(2), object(11)\nmemory usage: 226.7+ KB\nNone\n\n\n\n\n\n\n\n\n\ncompany\ntypename\nopsys\nprice\ncpu_brand\ncpu_name\ncpu_ghz\nresolution\ntouchscreen\ndisplay_type\ngpu_brand\ngpu_name\nmemory_capacity_1\nmemory_type_1\nmemory_capacity_2\nmemory_type_2\nram_gb\ninches_size\nweight_kg\n\n\n\n\n0\nApple\nUltrabook\nmacOS\n71378.6832\nIntel\nCore i5\n2.3\n2560x1600\n0\nIPS Panel Retina Display\nIntel\nIris Plus Graphics 640\n128.0\nSSD\nNaN\nNaN\n8\n13.3\n1.37\n\n\n1\nApple\nUltrabook\nmacOS\n47895.5232\nIntel\nCore i5\n1.8\n1440x900\n0\n\nIntel\nHD Graphics 6000\n128.0\nFlash Storage\nNaN\nNaN\n8\n13.3\n1.34\n\n\n2\nHP\nNotebook\nNo OS\n30636.0000\nIntel\nCore i5 7200U\n2.5\n1920x1080\n0\n\nIntel\nHD Graphics 620\n256.0\nSSD\nNaN\nNaN\n8\n15.6\n1.86\n\n\n3\nApple\nUltrabook\nmacOS\n135195.3360\nIntel\nCore i7\n2.7\n2880x1800\n0\nIPS Panel Retina Display\nAMD\nRadeon Pro 455\n512.0\nSSD\nNaN\nNaN\n16\n15.4\n1.83\n\n\n4\nApple\nUltrabook\nmacOS\n96095.8080\nIntel\nCore i5\n3.1\n2560x1600\n0\nIPS Panel Retina Display\nIntel\nIris Plus Graphics 650\n256.0\nSSD\nNaN\nNaN\n8\n13.3\n1.37\n\n\n\n\n\n\n\nNow that dataset is clean let’s go for EDA.\n\n\n\n\nAs we have 8 numeric columns, let’s start with correlation plot.\n\nsns.heatmap(df_clean.select_dtypes(include = ['int64', 'float64']).corr(),\n                                   annot = True, cmap = 'coolwarm')\nplt.show()\n\n\n\n\nWe can see that price has a strong positive correlation with ram_gb and cpu_ghz.\nLet’s check the distribution of price column.\n\nsns.histplot(df_clean['price'], bins = 20, kde = True)\nplt.show()\n\n\n\n\nThe plot is right skewed, we can log transform the price column to make it more normal.\n\ndf_clean['price_log'] = np.log1p(df_clean['price'])\n\nsns.histplot(df_clean['price_log'], bins = 50, kde = True)\nplt.show()\n\n\n\n\nprice_log is more normally distributed, let’s check the correlation of price_log with other columns.\n\nsns.heatmap(df_clean.drop(columns = ['price']).select_dtypes(include = ['int64', 'float64']).corr(),\n                                    annot = True, cmap = 'coolwarm')\nplt.show()\n\n\n\n\nWe can see that price_log has a strong positive correlation with ram_gb and cpu_ghz.\nLet’s plot price_log in a boxplot to get the outliers.\n\nax = sns.boxplot(x='price_log', data=df_clean)\nmax = df_clean['price_log'].max()\nplt.text(max, 0, f'{max:.2f}', ha='center', va='bottom', color='red')\nplt.xlabel('Price_log')\nplt.title('Boxplot of Price_log')\nplt.show()\n\n\n\n\nThere is only one outlier in the data, let’s remove it.\n\ndf_clean = df_clean[df_clean['price_log'] &lt; 12.6]\n\ndf_clean['price_log'].max()\n\nnp.float64(12.587885975858104)\n\n\nNow we have removed the outliers from price_log column. Let’s look at object columns starting with companies.\n\nsns.barplot(x = df_clean.company.value_counts().index,\n            y = df_clean.company.value_counts().values)\nplt.xlabel('Company')\nplt.ylabel('Count')\nplt.title('Company Counts')\nplt.xticks(rotation = 90)\nplt.show()\n\n\n\n\nLenovo, Dell, HP are the top 3 companies in the dataset.\n\ncompany_counts = df_clean.company.value_counts()\n\nprint((company_counts[:3].sum()/len(df_clean)).round(3))\n\n0.662\n\n\n66.2% of the laptops are from Lenovo, Dell, HP.\nLet’s look at cpu and it’s features.\n\nprint(df_clean.cpu_brand.nunique())\nprint(df_clean.cpu_ghz.nunique())\nprint(df_clean.cpu_name.nunique())\n\n3\n25\n93\n\n\nThere are 3 unique values in cpu_brand, 25 unique values in cpu_ghz, 93 unique values in cpu_name.\n\ncpu_brand_counts = df_clean.cpu_brand.value_counts()\ncpu_ghz_counts = df_clean.cpu_ghz.value_counts().sort_values(ascending = False)\ncpu_name_counts = df_clean.cpu_name.value_counts()\n\nprint(cpu_brand_counts)\nprint(cpu_ghz_counts.head(5))\n\ncpu_brand\nIntel      1182\nAMD          60\nSamsung       1\nName: count, dtype: int64\ncpu_ghz\n2.5    278\n2.8    161\n2.7    158\n1.6    118\n2.3     84\nName: count, dtype: int64\n\n\nMost of the laptops have Intel CPU, 2.4GHz is the most common CPU clock speed, Intel Core i5 is the most common CPU name.\nSamsung has only one laptop in the dataset, which is not ideal for building a model, let’s remove it.\n\ndf_clean = df_clean[df_clean.cpu_brand != 'Samsung']\n\ndf_clean.cpu_brand.unique()\n\narray(['Intel', 'AMD'], dtype=object)\n\n\nLet’s plot cpu_ghz to know the distribution of CPU clock speed.\n\nsns.barplot(x=cpu_ghz_counts.index.astype(str),\n            y=cpu_ghz_counts.values)\nplt.xlabel('CPU GHz')\nplt.ylabel('Count')\nplt.title('CPU GHz Distribution')\nplt.xticks(rotation=90)\nplt.show()\n\n\n\n\n2.5GHz is the most common CPU clock speed, followed by 2.8GHz and 2.7GHz.\n\nsns.barplot(x=df_clean.inches_size.value_counts().index.astype(str),\n            y=df_clean.inches_size.value_counts().values)\nplt.xlabel('Screen Size')\nplt.ylabel('Count')\nplt.xticks(rotation=90)\nplt.title('Laptops with screen sizes')\nplt.show()\n\n\n\n\n15.6 is the most common screen size, followed by 14.0 and 17.3 inches.\n\nscreen_size_counts = df_clean.inches_size.value_counts().sort_values(ascending = False)\n\nprint(screen_size_counts.head(6).sum()/screen_size_counts.sum())\n\n0.9621273166800967\n\n\nOnly 4 sizes make up 96.21% of the laptops in the dataset, which means we can drop the other sizes.\n\ndf_clean = df_clean[df_clean.inches_size.isin([13.3, 14.0, 15.6, 17.3, 11.6, 12.5])]\n\ndf_clean.inches_size.unique()\n\narray([13.3, 15.6, 14. , 17.3, 12.5, 11.6])\n\n\nLet’s look at correlation once again.\n\nsns.heatmap(df_clean.select_dtypes(include = ['int64', 'float64']).corr(),\n            annot = True, cmap = 'coolwarm')\nplt.show()\n\n\n\n\n\n\n\n\nWe have gone through different parameters of the data now it’s time to put that to building a model.\nI am going to build 2 models 1. Random Forest Regressor 2. Linear Regression Model\nand compare them to find the best model.\n\n\nImporting libraries for model building and evaluation with sklearn.\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.metrics import mean_squared_error, r2_score\n\n\n\n\nRandomForestRegressor can deal with Null values, so we don’t need to handle them for this model, but we need to handle them for LinearRegression model.\nLet’s create a copy of the dataset and drop the price column.\nThen we need to label encode the cpu_ghz, inches_size, ram_gb, memory_capacity_1, memory_capacity_2, resolution columns as they are ordinal data.\n\ndf_model = df_clean.copy().drop(columns=['price'])\n\nordinal_cols = ['cpu_ghz', 'inches_size', 'ram_gb', 'memory_capacity_1', 'memory_capacity_2', 'resolution', 'weight_kg']\n\nfor col in ordinal_cols:\n    le = LabelEncoder()\n    df_model[col] = le.fit_transform(df_model[col])\n\ndf_model.head()\n\n\n\n\n\n\n\n\ncompany\ntypename\nopsys\ncpu_brand\ncpu_name\ncpu_ghz\nresolution\ntouchscreen\ndisplay_type\ngpu_brand\ngpu_name\nmemory_capacity_1\nmemory_type_1\nmemory_capacity_2\nmemory_type_2\nram_gb\ninches_size\nweight_kg\nprice_log\n\n\n\n\n0\nApple\nUltrabook\nmacOS\nIntel\nCore i5\n13\n6\n0\nIPS Panel Retina Display\nIntel\nIris Plus Graphics 640\n4\nSSD\n5\nNaN\n4\n2\n35\n11.175769\n\n\n1\nApple\nUltrabook\nmacOS\nIntel\nCore i5\n7\n1\n0\n\nIntel\nHD Graphics 6000\n4\nFlash Storage\n5\nNaN\n4\n2\n32\n10.776798\n\n\n2\nHP\nNotebook\nNo OS\nIntel\nCore i5 7200U\n15\n3\n0\n\nIntel\nHD Graphics 620\n7\nSSD\n5\nNaN\n4\n4\n69\n10.329964\n\n\n4\nApple\nUltrabook\nmacOS\nIntel\nCore i5\n21\n6\n0\nIPS Panel Retina Display\nIntel\nIris Plus Graphics 650\n7\nSSD\n5\nNaN\n4\n2\n35\n11.473111\n\n\n5\nAcer\nNotebook\nWindows 10\nAMD\nA9-Series 9420\n20\n0\n0\n\nAMD\nRadeon R5\n8\nHDD\n5\nNaN\n2\n4\n90\n9.967072\n\n\n\n\n\n\n\nWe need to one hot encode the cpu_brand, gpu_brand, company, display_type, touchscreen, cpu_name, gpu_name columns as they are categorical data.\n\nnominal_cols = ['cpu_brand', 'gpu_brand', 'company', 'display_type', 'touchscreen', 'cpu_name', 'gpu_name', 'typename', 'opsys', 'memory_type_1', 'memory_type_2']\n\ndf_model = pd.get_dummies(df_model, columns=nominal_cols, drop_first=False)\n\nprint(df_model.shape)\ndf_model.head()\n\n(1194, 236)\n\n\n\n\n\n\n\n\n\ncpu_ghz\nresolution\nmemory_capacity_1\nmemory_capacity_2\nram_gb\ninches_size\nweight_kg\nprice_log\ncpu_brand_AMD\ncpu_brand_Intel\n...\nopsys_Windows 7\nopsys_macOS\nmemory_type_1_?\nmemory_type_1_Flash Storage\nmemory_type_1_HDD\nmemory_type_1_Hybrid\nmemory_type_1_SSD\nmemory_type_2_HDD\nmemory_type_2_Hybrid\nmemory_type_2_SSD\n\n\n\n\n0\n13\n6\n4\n5\n4\n2\n35\n11.175769\nFalse\nTrue\n...\nFalse\nTrue\nFalse\nFalse\nFalse\nFalse\nTrue\nFalse\nFalse\nFalse\n\n\n1\n7\n1\n4\n5\n4\n2\n32\n10.776798\nFalse\nTrue\n...\nFalse\nTrue\nFalse\nTrue\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\n\n\n2\n15\n3\n7\n5\n4\n4\n69\n10.329964\nFalse\nTrue\n...\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\nTrue\nFalse\nFalse\nFalse\n\n\n4\n21\n6\n7\n5\n4\n2\n35\n11.473111\nFalse\nTrue\n...\nFalse\nTrue\nFalse\nFalse\nFalse\nFalse\nTrue\nFalse\nFalse\nFalse\n\n\n5\n20\n0\n8\n5\n2\n4\n90\n9.967072\nTrue\nFalse\n...\nFalse\nFalse\nFalse\nFalse\nTrue\nFalse\nFalse\nFalse\nFalse\nFalse\n\n\n\n\n5 rows × 236 columns\n\n\n\n\n\n\n\n\nLet’s split the data into training and testing sets.\n\nx = df_model.drop(columns = ['price_log'])\ny = df_model['price_log']\n\nx_train, x_test, y_train, y_test = train_test_split(x, y, test_size = 0.2, random_state = 42)\n\n\n\n\nTraining the model with RandomForestRegressor and evaluating it with mean_squared_error and r2_score.\n\n#\n\nrf_model = RandomForestRegressor(n_estimators= 100, max_depth = 200, max_features = 20)\nrf_model.fit(x_train, y_train)\n\n\nRandomForestRegressor(max_depth=200, max_features=20)In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.RandomForestRegressor?Documentation for RandomForestRegressoriFitted\n        \n            \n                Parameters\n                \n\n\n\n\nn_estimators \n100\n\n\n\ncriterion \n'squared_error'\n\n\n\nmax_depth \n200\n\n\n\nmin_samples_split \n2\n\n\n\nmin_samples_leaf \n1\n\n\n\nmin_weight_fraction_leaf \n0.0\n\n\n\nmax_features \n20\n\n\n\nmax_leaf_nodes \nNone\n\n\n\nmin_impurity_decrease \n0.0\n\n\n\nbootstrap \nTrue\n\n\n\noob_score \nFalse\n\n\n\nn_jobs \nNone\n\n\n\nrandom_state \nNone\n\n\n\nverbose \n0\n\n\n\nwarm_start \nFalse\n\n\n\nccp_alpha \n0.0\n\n\n\nmax_samples \nNone\n\n\n\nmonotonic_cst \nNone\n\n\n\n\n            \n        \n    \n\n\n\ny_pred = rf_model.predict(x_test)\n\nmse = mean_squared_error(y_test, y_pred)\nr2 = r2_score(y_test, y_pred)\n\nprint(f'Mean Squared Error: {mse:.2f}')\nprint(f'R2 Score: {r2:.2f}')\n\nMean Squared Error: 0.04\nR2 Score: 0.89\n\n\nThe R2 Score is 0.88, which is good and Mean Squared Error is 0.04 which is also good for this model.\nLet’s plot the predicted vs actual values.\n\nsns.scatterplot(x=y_test, y=y_pred)\nplt.xlabel('Actual Price (log)')\nplt.ylabel('Predicted Price (log)')\nplt.title('Predicted vs Actual Price (log)')\nplt.plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], color='red', linestyle='--')\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\ndf_lr = df_clean.copy().drop(columns=['price'])\n\ndf_lr.info()\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nIndex: 1194 entries, 0 to 1273\nData columns (total 19 columns):\n #   Column             Non-Null Count  Dtype  \n---  ------             --------------  -----  \n 0   company            1194 non-null   object \n 1   typename           1194 non-null   object \n 2   opsys              1194 non-null   object \n 3   cpu_brand          1194 non-null   object \n 4   cpu_name           1194 non-null   object \n 5   cpu_ghz            1194 non-null   float64\n 6   resolution         1194 non-null   object \n 7   touchscreen        1194 non-null   int64  \n 8   display_type       1194 non-null   object \n 9   gpu_brand          1194 non-null   object \n 10  gpu_name           1194 non-null   object \n 11  memory_capacity_1  1193 non-null   float64\n 12  memory_type_1      1194 non-null   object \n 13  memory_capacity_2  201 non-null    float64\n 14  memory_type_2      201 non-null    object \n 15  ram_gb             1194 non-null   int64  \n 16  inches_size        1194 non-null   float64\n 17  weight_kg          1193 non-null   float64\n 18  price_log          1194 non-null   float64\ndtypes: float64(6), int64(2), object(11)\nmemory usage: 186.6+ KB\n\n\nWe nedd to deal with the missing values in the dataset for LinearRegression model.\n\ndf_lr.isnull().sum()\n\ncompany                0\ntypename               0\nopsys                  0\ncpu_brand              0\ncpu_name               0\ncpu_ghz                0\nresolution             0\ntouchscreen            0\ndisplay_type           0\ngpu_brand              0\ngpu_name               0\nmemory_capacity_1      1\nmemory_type_1          0\nmemory_capacity_2    993\nmemory_type_2        993\nram_gb                 0\ninches_size            0\nweight_kg              1\nprice_log              0\ndtype: int64\n\n\nThere are 1 missing values in weight_kg column and 1 missing value in memory_capacity_1, let’s fill it with the median and mean of the columns.\n\ndf_lr['weight_kg'] = df_lr['weight_kg'].fillna(df_lr['weight_kg'].median())\ndf_lr['memory_capacity_1'] = df_lr['memory_capacity_1'].fillna(df_lr['memory_capacity_1'].mean())\n\nmemory_capacity_2 and memory_type_2 are lots of missing values, let’s fill them with 0s respectively.\n\ndf_lr['memory_capacity_2'] = df_lr['memory_capacity_2'].fillna(0)\ndf_lr['memory_type_2'] = df_lr['memory_type_2'].fillna('None')\ndf_lr['memory_type_1'] = df_lr['memory_type_1'].replace({0: 'None', np.nan: 'None'})\n\nNow we have filled the missing values in the dataset for LinearRegression model. Let’s encode the data for LinearRegression model.\n\nordinal_cols = ['cpu_ghz', 'inches_size', 'ram_gb', 'memory_capacity_1', 'memory_capacity_2', 'resolution', 'weight_kg']\n\nfor col in ordinal_cols:\n    le = LabelEncoder()\n    df_lr[col] = le.fit_transform(df_lr[col])\n\ndf_lr.head(3)\n\n\n\n\n\n\n\n\ncompany\ntypename\nopsys\ncpu_brand\ncpu_name\ncpu_ghz\nresolution\ntouchscreen\ndisplay_type\ngpu_brand\ngpu_name\nmemory_capacity_1\nmemory_type_1\nmemory_capacity_2\nmemory_type_2\nram_gb\ninches_size\nweight_kg\nprice_log\n\n\n\n\n0\nApple\nUltrabook\nmacOS\nIntel\nCore i5\n13\n6\n0\nIPS Panel Retina Display\nIntel\nIris Plus Graphics 640\n4\nSSD\n0\nNone\n4\n2\n35\n11.175769\n\n\n1\nApple\nUltrabook\nmacOS\nIntel\nCore i5\n7\n1\n0\n\nIntel\nHD Graphics 6000\n4\nFlash Storage\n0\nNone\n4\n2\n32\n10.776798\n\n\n2\nHP\nNotebook\nNo OS\nIntel\nCore i5 7200U\n15\n3\n0\n\nIntel\nHD Graphics 620\n7\nSSD\n0\nNone\n4\n4\n69\n10.329964\n\n\n\n\n\n\n\nWe need to encode nominal data for LinearRegression model.\n\nnominal_cols = ['cpu_brand', 'gpu_brand', 'company', 'display_type', 'touchscreen', 'cpu_name', 'gpu_name', 'typename', 'opsys', 'memory_type_1', 'memory_type_2']\n\ndf_lr = pd.get_dummies(df_lr, columns=nominal_cols, drop_first=False)\n\ndf_lr.head(3)\n\n\n\n\n\n\n\n\ncpu_ghz\nresolution\nmemory_capacity_1\nmemory_capacity_2\nram_gb\ninches_size\nweight_kg\nprice_log\ncpu_brand_AMD\ncpu_brand_Intel\n...\nopsys_macOS\nmemory_type_1_?\nmemory_type_1_Flash Storage\nmemory_type_1_HDD\nmemory_type_1_Hybrid\nmemory_type_1_SSD\nmemory_type_2_HDD\nmemory_type_2_Hybrid\nmemory_type_2_None\nmemory_type_2_SSD\n\n\n\n\n0\n13\n6\n4\n0\n4\n2\n35\n11.175769\nFalse\nTrue\n...\nTrue\nFalse\nFalse\nFalse\nFalse\nTrue\nFalse\nFalse\nTrue\nFalse\n\n\n1\n7\n1\n4\n0\n4\n2\n32\n10.776798\nFalse\nTrue\n...\nTrue\nFalse\nTrue\nFalse\nFalse\nFalse\nFalse\nFalse\nTrue\nFalse\n\n\n2\n15\n3\n7\n0\n4\n4\n69\n10.329964\nFalse\nTrue\n...\nFalse\nFalse\nFalse\nFalse\nFalse\nTrue\nFalse\nFalse\nTrue\nFalse\n\n\n\n\n3 rows × 237 columns\n\n\n\nLet’s split the data into training and testing sets.\n\nx = df_lr.drop(columns = ['price_log'])\ny = df_lr['price_log']\n\nx_train, x_test, y_train, y_test = train_test_split(x, y, test_size = 0.2, random_state = 42)\n\n\n\n\nTraining the model with LinearRegression and evaluating it with mean_squared_error and r2_score.\n\nlr_model = LinearRegression()\n\nlr_model.fit(x_train, y_train)\n\n\nLinearRegression()In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.LinearRegression?Documentation for LinearRegressioniFitted\n        \n            \n                Parameters\n                \n\n\n\n\nfit_intercept \nTrue\n\n\n\ncopy_X \nTrue\n\n\n\ntol \n1e-06\n\n\n\nn_jobs \nNone\n\n\n\npositive \nFalse\n\n\n\n\n            \n        \n    \n\n\nModel evaluation\n\ny_pred = lr_model.predict(x_test)\n\nmse = mean_squared_error(y_test, y_pred)\nr2 = r2_score(y_test, y_pred)\n\n\nprint(f'Mean Squared Error: {mse:.2f}')\nprint(f'R2 Score: {r2:.2f}')\n\nMean Squared Error: 0.06\nR2 Score: 0.85\n\n\nThe R2 Score is 0.85, which is good and Mean Squared Error is 0.06 which is also good for this model.\nLet’s plot the predicted vs actual values.\n\nsns.scatterplot(x=y_test, y=y_pred)\nplt.xlabel('Actual Price (log)')\nplt.ylabel('Predicted Price (log)')\nplt.title('Predicted vs Actual Price (log)')\nplt.plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], color='red', linestyle='--')\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\nWe know that RandomForestRegressor is a tree based model, so we can use feature_importances_ to get the importance of each feature.\n\nfeature_importances = pd.DataFrame({'feature': df_model.drop(columns=['price_log']).columns, 'importance': rf_model.feature_importances_.round(4)})\nfeature_importances = feature_importances.sort_values('importance', ascending=False)\nfeature_importances.head(10)\n\n\n\n\n\n\n\n\nfeature\nimportance\n\n\n\n\n4\nram_gb\n0.1551\n\n\n0\ncpu_ghz\n0.0818\n\n\n1\nresolution\n0.0742\n\n\n231\nmemory_type_1_SSD\n0.0699\n\n\n6\nweight_kg\n0.0613\n\n\n216\ntypename_Notebook\n0.0601\n\n\n2\nmemory_capacity_1\n0.0600\n\n\n229\nmemory_type_1_HDD\n0.0389\n\n\n5\ninches_size\n0.0261\n\n\n11\ngpu_brand_Nvidia\n0.0227\n\n\n\n\n\n\n\n\n\n\nWe know that LinearRegression is a linear model, so we can use coef_ to get the importance of each feature.\n\nfeature_importances = pd.DataFrame({'feature': df_lr.drop(columns=['price_log']).columns, 'importance': lr_model.coef_.round(4)})\nfeature_importances = feature_importances.sort_values('importance', ascending=False)\nfeature_importances.head(10)\n\n\n\n\n\n\n\n\nfeature\nimportance\n\n\n\n\n177\ngpu_name_Quadro M3000M\n0.7603\n\n\n117\ngpu_name_FirePro W6150M\n0.7565\n\n\n181\ngpu_name_Quadro M620M\n0.6756\n\n\n91\ncpu_name_Core i7 7820HK\n0.5666\n\n\n113\ncpu_name_Xeon E3-1535M v5\n0.4989\n\n\n174\ngpu_name_Quadro M2000M\n0.4989\n\n\n85\ncpu_name_Core i7 6820HQ\n0.4953\n\n\n149\ngpu_name_GeForce GTX1080\n0.4902\n\n\n59\ncpu_name_Core M 6Y54\n0.4829\n\n\n110\ncpu_name_Ryzen 1600\n0.4685\n\n\n\n\n\n\n\n\n\n\nI will choose RandomForestRegressor for hyperparameter tuning as it has features which are easily explainable and a tree based model can be easily tunable.\nGridSearchCV is used to tune the hyperparameters of the model. n_estimators is the number of trees in the forest, max_depth is the maximum depth of the tree, max_features is the number of features to consider when looking for the best split.\n\nfrom sklearn.model_selection import GridSearchCV\n\nparam_grid = {\n    'n_estimators': [50, 100],\n    'max_depth': [20, 30],\n    'max_features': [5, 10, 15]\n}\n\ngrid_search = GridSearchCV(estimator = rf_model, param_grid = param_grid, cv = 5, scoring = 'neg_mean_squared_error', verbose = 2)\ngrid_search.fit(x_train, y_train)\n\n\nprint(grid_search.best_params_)\nprint(grid_search.best_score_)\n\n{'max_depth': 30, 'max_features': 15, 'n_estimators': 50}\n-0.043978114678904986\n\n\ngrid_search.best_params_ gives the best parameters for the model depending on the scoring metric which is neg_mean_squared_error in this case.\nThe best parameters are n_estimators = 100, max_depth = 30, max_features = 15 and the best score is -0.042.\n\nrf_model = RandomForestRegressor(n_estimators = 100, max_depth = 30, max_features = 15)\nrf_model.fit(x_train, y_train)\n\n\nRandomForestRegressor(max_depth=30, max_features=15)In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.RandomForestRegressor?Documentation for RandomForestRegressoriFitted\n        \n            \n                Parameters\n                \n\n\n\n\nn_estimators \n100\n\n\n\ncriterion \n'squared_error'\n\n\n\nmax_depth \n30\n\n\n\nmin_samples_split \n2\n\n\n\nmin_samples_leaf \n1\n\n\n\nmin_weight_fraction_leaf \n0.0\n\n\n\nmax_features \n15\n\n\n\nmax_leaf_nodes \nNone\n\n\n\nmin_impurity_decrease \n0.0\n\n\n\nbootstrap \nTrue\n\n\n\noob_score \nFalse\n\n\n\nn_jobs \nNone\n\n\n\nrandom_state \nNone\n\n\n\nverbose \n0\n\n\n\nwarm_start \nFalse\n\n\n\nccp_alpha \n0.0\n\n\n\nmax_samples \nNone\n\n\n\nmonotonic_cst \nNone\n\n\n\n\n            \n        \n    \n\n\n\ny_pred = rf_model.predict(x_test)\nmse = mean_squared_error(y_test, y_pred)\nr2 = r2_score(y_test, y_pred)\nprint(f'Mean Squared Error: {mse:.2f}')\nprint(f'R2 Score: {r2:.2f}')\n\nMean Squared Error: 0.05\nR2 Score: 0.88\n\n\nEven after tuning the hyper parameters, the R2 Score is 0.88 and Mean Squared Error is 0.04 which is same as the previous model, but by using this model we can save memory and time.\n\n\n\n\nIn this project, we successfully built a model to predict laptop prices based on various features. We explored the dataset, cleaned it, and extracted useful features. We built two models, RandomForestRegressor and LinearRegression, and compared their performance. The RandomForestRegressor performed better with an R2 Score of 0.88 and a Mean Squared Error of 0.04. We also tuned the hyperparameters of the model to improve its performance.\n\n\n\nThe final was deployed as a Streamlit app, which allows users to input laptop specifications and get the predicted price. The app is available at Laptop Price Prediction App.\nCheck out the app to see how it works and try it out with different laptop specifications."
  },
  {
    "objectID": "posts/laptop_price_prediction/laptop_price_prediction.html#data-description",
    "href": "posts/laptop_price_prediction/laptop_price_prediction.html#data-description",
    "title": "Laptop Price Prediction",
    "section": "",
    "text": "The dataset used for this project is sourced from Kaggle. It contains detailed information about various laptop models, including specifications such as processor type, RAM, storage, display size, and brand. The target variable is the price of the laptops."
  },
  {
    "objectID": "posts/laptop_price_prediction/laptop_price_prediction.html#clients-objectives",
    "href": "posts/laptop_price_prediction/laptop_price_prediction.html#clients-objectives",
    "title": "Laptop Price Prediction",
    "section": "",
    "text": "Develop a model that can accurately predict laptop prices based on various features, helping our clients stay competitive in the market.\n\n\n\nUnderstand how different features contribute to pricing, enabling SmartTech Co. to strategically position its laptops in the market.\n\n\n\nAssess the impact of brand reputation on pricing, providing insights into brand perception and market demand."
  },
  {
    "objectID": "posts/laptop_price_prediction/laptop_price_prediction.html#loading-libraries-and-dataset",
    "href": "posts/laptop_price_prediction/laptop_price_prediction.html#loading-libraries-and-dataset",
    "title": "Laptop Price Prediction",
    "section": "",
    "text": "Pandas, NumPy, Matplotlib, and Seaborn are used for data manipulation, numerical operations, and visualization.\n\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nImporting dataset\n\ndf = pd.read_csv(\"F:/Odin_school/Capstone_projects/ml_capstone/laptop.csv\")\n\ndf.head()\n\n\n\n\n\n\n\n\nUnnamed: 0.1\nUnnamed: 0\nCompany\nTypeName\nInches\nScreenResolution\nCpu\nRam\nMemory\nGpu\nOpSys\nWeight\nPrice\n\n\n\n\n0\n0\n0.0\nApple\nUltrabook\n13.3\nIPS Panel Retina Display 2560x1600\nIntel Core i5 2.3GHz\n8GB\n128GB SSD\nIntel Iris Plus Graphics 640\nmacOS\n1.37kg\n71378.6832\n\n\n1\n1\n1.0\nApple\nUltrabook\n13.3\n1440x900\nIntel Core i5 1.8GHz\n8GB\n128GB Flash Storage\nIntel HD Graphics 6000\nmacOS\n1.34kg\n47895.5232\n\n\n2\n2\n2.0\nHP\nNotebook\n15.6\nFull HD 1920x1080\nIntel Core i5 7200U 2.5GHz\n8GB\n256GB SSD\nIntel HD Graphics 620\nNo OS\n1.86kg\n30636.0000\n\n\n3\n3\n3.0\nApple\nUltrabook\n15.4\nIPS Panel Retina Display 2880x1800\nIntel Core i7 2.7GHz\n16GB\n512GB SSD\nAMD Radeon Pro 455\nmacOS\n1.83kg\n135195.3360\n\n\n4\n4\n4.0\nApple\nUltrabook\n13.3\nIPS Panel Retina Display 2560x1600\nIntel Core i5 3.1GHz\n8GB\n256GB SSD\nIntel Iris Plus Graphics 650\nmacOS\n1.37kg\n96095.8080"
  },
  {
    "objectID": "posts/laptop_price_prediction/laptop_price_prediction.html#data-cleansing",
    "href": "posts/laptop_price_prediction/laptop_price_prediction.html#data-cleansing",
    "title": "Laptop Price Prediction",
    "section": "",
    "text": "df.isnull().sum()\n\nUnnamed: 0.1         0\nUnnamed: 0          30\nCompany             30\nTypeName            30\nInches              30\nScreenResolution    30\nCpu                 30\nRam                 30\nMemory              30\nGpu                 30\nOpSys               30\nWeight              30\nPrice               30\ndtype: int64\n\n\nAll columns except Unnamed: 0.1 has missing values, which might mean that first column may just be an index column. Let’s drop it and check Unnamed: 0 column.\n\ndf = df.drop(['Unnamed: 0.1'], axis=1)\n\ndf.isnull().sum()\n\nUnnamed: 0          30\nCompany             30\nTypeName            30\nInches              30\nScreenResolution    30\nCpu                 30\nRam                 30\nMemory              30\nGpu                 30\nOpSys               30\nWeight              30\nPrice               30\ndtype: int64\n\n\nNull values are still present lets drop the rows with Null values.\n\ndf = df.dropna()\ndf.info()\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nIndex: 1273 entries, 0 to 1302\nData columns (total 12 columns):\n #   Column            Non-Null Count  Dtype  \n---  ------            --------------  -----  \n 0   Unnamed: 0        1273 non-null   float64\n 1   Company           1273 non-null   object \n 2   TypeName          1273 non-null   object \n 3   Inches            1273 non-null   object \n 4   ScreenResolution  1273 non-null   object \n 5   Cpu               1273 non-null   object \n 6   Ram               1273 non-null   object \n 7   Memory            1273 non-null   object \n 8   Gpu               1273 non-null   object \n 9   OpSys             1273 non-null   object \n 10  Weight            1273 non-null   object \n 11  Price             1273 non-null   float64\ndtypes: float64(2), object(10)\nmemory usage: 129.3+ KB\n\n\nThere are 1273 non-null entries in the dataset now, which means we have successfully removed rows with missing values. Let’s check if Unnamed: 0 column is just an index column or not.\n\ndf['Unnamed: 0'].nunique()\n\n1273\n\n\nIt seems that Unnamed: 0 column is just an index column, let’s drop it as well.\n\ndf = df.drop(['Unnamed: 0'], axis=1)\ndf.info()\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nIndex: 1273 entries, 0 to 1302\nData columns (total 11 columns):\n #   Column            Non-Null Count  Dtype  \n---  ------            --------------  -----  \n 0   Company           1273 non-null   object \n 1   TypeName          1273 non-null   object \n 2   Inches            1273 non-null   object \n 3   ScreenResolution  1273 non-null   object \n 4   Cpu               1273 non-null   object \n 5   Ram               1273 non-null   object \n 6   Memory            1273 non-null   object \n 7   Gpu               1273 non-null   object \n 8   OpSys             1273 non-null   object \n 9   Weight            1273 non-null   object \n 10  Price             1273 non-null   float64\ndtypes: float64(1), object(10)\nmemory usage: 119.3+ KB\n\n\nNow we have 1273 non-null entries in the dataset and no missing values. Let’s check for duplicates in the data.\n\nprint(df.duplicated().sum())\n\ndf[df.duplicated()].sort_values(by='Company', ascending=True).head(3)\n\n29\n\n\n\n\n\n\n\n\n\nCompany\nTypeName\nInches\nScreenResolution\nCpu\nRam\nMemory\nGpu\nOpSys\nWeight\nPrice\n\n\n\n\n1291\nAcer\nNotebook\n15.6\n1366x768\nIntel Celeron Dual Core N3060 1.6GHz\n4GB\n500GB HDD\nIntel HD Graphics 400\nLinux\n2.4kg\n15397.92\n\n\n1277\nAcer\nNotebook\n15.6\n1366x768\nIntel Celeron Dual Core N3060 1.6GHz\n4GB\n500GB HDD\nIntel HD Graphics 400\nLinux\n2.4kg\n15397.92\n\n\n1274\nAsus\nNotebook\n15.6\n1366x768\nIntel Celeron Dual Core N3050 1.6GHz\n4GB\n500GB HDD\nIntel HD Graphics\nWindows 10\n2.2kg\n19660.32\n\n\n\n\n\n\n\nThere are 29 duplicate records in the dataset, let’s drop them.\n\ndf = df.drop_duplicates()\n\ndf.info()\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nIndex: 1244 entries, 0 to 1273\nData columns (total 11 columns):\n #   Column            Non-Null Count  Dtype  \n---  ------            --------------  -----  \n 0   Company           1244 non-null   object \n 1   TypeName          1244 non-null   object \n 2   Inches            1244 non-null   object \n 3   ScreenResolution  1244 non-null   object \n 4   Cpu               1244 non-null   object \n 5   Ram               1244 non-null   object \n 6   Memory            1244 non-null   object \n 7   Gpu               1244 non-null   object \n 8   OpSys             1244 non-null   object \n 9   Weight            1244 non-null   object \n 10  Price             1244 non-null   float64\ndtypes: float64(1), object(10)\nmemory usage: 116.6+ KB\n\n\nNow we have 1244 non-null entries in the dataset and no missing values or duplicates.\nThere are 11 columns in the dataset, among which Price is the only column with folat64 data type rest are object data type."
  },
  {
    "objectID": "posts/laptop_price_prediction/laptop_price_prediction.html#exploratory-data-analysis",
    "href": "posts/laptop_price_prediction/laptop_price_prediction.html#exploratory-data-analysis",
    "title": "Laptop Price Prediction",
    "section": "",
    "text": "Let’s start with the basic statistics of the dataset.\n\ndf.describe(include='all')\n\n\n\n\n\n\n\n\nCompany\nTypeName\nInches\nScreenResolution\nCpu\nRam\nMemory\nGpu\nOpSys\nWeight\nPrice\n\n\n\n\ncount\n1244\n1244\n1244\n1244\n1244\n1244\n1244\n1244\n1244\n1244\n1244.000000\n\n\nunique\n19\n6\n25\n40\n118\n10\n40\n110\n9\n189\nNaN\n\n\ntop\nLenovo\nNotebook\n15.6\nFull HD 1920x1080\nIntel Core i5 7200U 2.5GHz\n8GB\n256GB SSD\nIntel HD Graphics 620\nWindows 10\n2.2kg\nNaN\n\n\nfreq\n282\n689\n621\n493\n183\n595\n401\n269\n1022\n106\nNaN\n\n\nmean\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n60606.224427\n\n\nstd\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n37424.636161\n\n\nmin\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n9270.720000\n\n\n25%\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n32655.445200\n\n\n50%\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n52693.920000\n\n\n75%\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n79813.440000\n\n\nmax\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n324954.720000\n\n\n\n\n\n\n\nThe dataset contains 11 columns with the following features: - Company: The brand of the laptop with 19 brands. - TypeName: The type of laptop, such as Ultrabook, Gaming, etc. - Inches: The size of the laptop screen in inches. - ScreenResolution: The resolution of the laptop screen. - Cpu: The type of CPU used in the laptop. - Ram: The amount of RAM in GB. - Memory: The type and size of storage memory (HDD/SSD). - Gpu: The type of GPU used in the laptop. - OpSys: The operating system installed on the laptop. - Weight: The weight of the laptop in kg. - Price: The price of the laptop in INR(Indian Rupees).\n\n\nLet’s extract some features from the existing columns to make the dataset more informative.\nFirst lets convert all the column names to lower case for consistency.\n\ndf.columns = df.columns.str.lower()\n\nprint(df.columns)\n\nIndex(['company', 'typename', 'inches', 'screenresolution', 'cpu', 'ram',\n       'memory', 'gpu', 'opsys', 'weight', 'price'],\n      dtype='object')\n\n\nLet’s check for the unique values in columns.\n\nprint(df.company.unique())\nprint(df.typename.unique())\nprint(df.ram.unique())\nprint(df.opsys.unique())\n\n['Apple' 'HP' 'Acer' 'Asus' 'Dell' 'Lenovo' 'Chuwi' 'MSI' 'Microsoft'\n 'Toshiba' 'Huawei' 'Xiaomi' 'Vero' 'Razer' 'Mediacom' 'Samsung' 'Google'\n 'Fujitsu' 'LG']\n['Ultrabook' 'Notebook' 'Gaming' '2 in 1 Convertible' 'Workstation'\n 'Netbook']\n['8GB' '16GB' '4GB' '2GB' '12GB' '64GB' '6GB' '32GB' '24GB' '1GB']\n['macOS' 'No OS' 'Windows 10' 'Mac OS X' 'Linux' 'Windows 10 S'\n 'Chrome OS' 'Windows 7' 'Android']\n\n\n\nprint(df.cpu[1])\nprint(df.screenresolution[6])\nprint(df.gpu[88])\n\nIntel Core i5 1.8GHz\nIPS Panel Retina Display 2880x1800\nNvidia GeForce GTX 1060\n\n\nwe can seperate values in columns to form new parameters i.e, - screenresolution can give us display_type, resolution & touchscreen - cpu can be seperated to form cpu and clockspeed - memory can be seperated to form memory and memory_type - gpu can be seperated to get gpu_company\n\n\n\n# cpu brand name\ndf['cpu_brand'] = df.cpu.str.split().str[0]\n\n# cpu name\ndf['cpu_name'] = df.cpu.str.replace(r'\\d+(?:\\.\\d+)?GHz', '', regex=True,).str.strip()\n# removing brand name\ndf['cpu_name'] = df.cpu_name.str.replace(r'^\\w+', '', regex=True).str.strip()\n\n# cpu clock speed\ndf['cpu_ghz'] = df.cpu.str.extract(r'(\\d+(?:\\.\\d+)?)GHz').astype('float64')\n\ndf[['cpu_brand', 'cpu_name', 'cpu_ghz']]\n\n\n\n\n\n\n\n\ncpu_brand\ncpu_name\ncpu_ghz\n\n\n\n\n0\nIntel\nCore i5\n2.3\n\n\n1\nIntel\nCore i5\n1.8\n\n\n2\nIntel\nCore i5 7200U\n2.5\n\n\n3\nIntel\nCore i7\n2.7\n\n\n4\nIntel\nCore i5\n3.1\n\n\n...\n...\n...\n...\n\n\n1269\nIntel\nCore i7 6500U\n2.5\n\n\n1270\nIntel\nCore i7 6500U\n2.5\n\n\n1271\nIntel\nCore i7 6500U\n2.5\n\n\n1272\nIntel\nCeleron Dual Core N3050\n1.6\n\n\n1273\nIntel\nCore i7 6500U\n2.5\n\n\n\n\n1244 rows × 3 columns\n\n\n\nNow, we have 3 columns act as seperate features for the price prediction.\n\n\n\n\nscreenresolution has many features ie., screen type, screen height, width, touch screen etc. Let’s extract all of them\n\n\n# display resolution\ndf['resolution'] = df['screenresolution'].str.extract(r'(\\d+x\\d+)')\n\n# touch screen or not\ndf['touchscreen'] = df['screenresolution'].apply(lambda x: 1 if 'Touchscreen' in x else 0)\n\n# Display type\ndf['display_type'] = df['screenresolution'].str.replace(r'\\d+x\\d+', \"\", regex = True).str.strip()\n\ndf['display_type'] = df['display_type'].str.replace(r'(Full HD|Quad HD|4K Ultra HD|/|\\+|Touchscreen)', '', regex = True).str.replace('/', '', regex = True).str.strip()\n\ndf[['resolution', 'touchscreen', 'display_type']]\n\n\n\n\n\n\n\n\nresolution\ntouchscreen\ndisplay_type\n\n\n\n\n0\n2560x1600\n0\nIPS Panel Retina Display\n\n\n1\n1440x900\n0\n\n\n\n2\n1920x1080\n0\n\n\n\n3\n2880x1800\n0\nIPS Panel Retina Display\n\n\n4\n2560x1600\n0\nIPS Panel Retina Display\n\n\n...\n...\n...\n...\n\n\n1269\n1366x768\n0\n\n\n\n1270\n1920x1080\n1\nIPS Panel\n\n\n1271\n3200x1800\n1\nIPS Panel\n\n\n1272\n1366x768\n0\n\n\n\n1273\n1366x768\n0\n\n\n\n\n\n1244 rows × 3 columns\n\n\n\nNow, we have another 3 columns to act as 3 seperate features.\n\ndf.touchscreen.sum()\n\nnp.int64(181)\n\n\n\n\n\nLet’s extarct gpu_brand and gpu_name from the column gpu\n\n# gpu brand\ndf['gpu_brand'] = df['gpu'].str.extract(r'^(\\w+)')\n\n# gpu name\ndf['gpu_name'] = df['gpu'].str.replace(r'^(\\w+)', '', regex = True).str.strip()\n\ndf[['gpu_brand', 'gpu_name']]\n\n\n\n\n\n\n\n\ngpu_brand\ngpu_name\n\n\n\n\n0\nIntel\nIris Plus Graphics 640\n\n\n1\nIntel\nHD Graphics 6000\n\n\n2\nIntel\nHD Graphics 620\n\n\n3\nAMD\nRadeon Pro 455\n\n\n4\nIntel\nIris Plus Graphics 650\n\n\n...\n...\n...\n\n\n1269\nNvidia\nGeForce 920M\n\n\n1270\nIntel\nHD Graphics 520\n\n\n1271\nIntel\nHD Graphics 520\n\n\n1272\nIntel\nHD Graphics\n\n\n1273\nAMD\nRadeon R5 M330\n\n\n\n\n1244 rows × 2 columns\n\n\n\n\n\n\nMost of the laptops have two drives which need to be seperated and type of memory is also in the memory so we need to seperate them both after seperating the drives.\n\nFirst replace the TB with GB(1TB ~ 1000GB)\n+ seperates two drives, str.split() function can be used to list the two memory drives and then they are slotted into seperate columns.\n\n\ndf.memory = df.memory.str.replace(r'1.0TB|1TB', \"1000GB\", regex = True)\ndf.memory = df.memory.str.replace(r'2.0TB|2TB', \"2000GB\", regex = True)\n\ndf.memory.unique()\n\narray(['128GB SSD', '128GB Flash Storage', '256GB SSD', '512GB SSD',\n       '500GB HDD', '256GB Flash Storage', '1000GB HDD',\n       '128GB SSD +  1000GB HDD', '256GB SSD +  256GB SSD',\n       '64GB Flash Storage', '32GB Flash Storage',\n       '256GB SSD +  1000GB HDD', '256GB SSD +  2000GB HDD', '32GB SSD',\n       '2000GB HDD', '64GB SSD', '1000GB Hybrid',\n       '512GB SSD +  1000GB HDD', '1000GB SSD', '256GB SSD +  500GB HDD',\n       '128GB SSD +  2000GB HDD', '512GB SSD +  512GB SSD', '16GB SSD',\n       '16GB Flash Storage', '512GB SSD +  256GB SSD',\n       '512GB SSD +  2000GB HDD', '64GB Flash Storage +  1000GB HDD',\n       '180GB SSD', '1000GB HDD +  1000GB HDD', '32GB HDD',\n       '1000GB SSD +  1000GB HDD', '?', '512GB Flash Storage',\n       '128GB HDD', '240GB SSD', '8GB SSD', '508GB Hybrid',\n       '512GB SSD +  1000GB Hybrid', '256GB SSD +  1000GB Hybrid'],\n      dtype=object)\n\n\n\ndf['memory_list'] = df.memory.str.split('+')\n\ndf['memory_1'] = df['memory_list'].str[0]\ndf['memory_2'] = df['memory_list'].str[1]\n\ndf[['memory_1', 'memory_2']]\n\n\n\n\n\n\n\n\nmemory_1\nmemory_2\n\n\n\n\n0\n128GB SSD\nNaN\n\n\n1\n128GB Flash Storage\nNaN\n\n\n2\n256GB SSD\nNaN\n\n\n3\n512GB SSD\nNaN\n\n\n4\n256GB SSD\nNaN\n\n\n...\n...\n...\n\n\n1269\n500GB HDD\nNaN\n\n\n1270\n128GB SSD\nNaN\n\n\n1271\n512GB SSD\nNaN\n\n\n1272\n64GB Flash Storage\nNaN\n\n\n1273\n1000GB HDD\nNaN\n\n\n\n\n1244 rows × 2 columns\n\n\n\nLet’s seperate df['memory_1'] into 2 seperate columns for memory_capacity and memory_type\n\ndf['memory_capacity_1'] = df['memory_1'].str.extract(r'(\\d+)').astype('float64')\ndf['memory_type_1'] = df['memory_1'].str.replace(r'(\\d+[A-Z]{2})', '', regex = True).str.strip()\n\ndf[['memory_capacity_1', 'memory_type_1']]\n\n\n\n\n\n\n\n\nmemory_capacity_1\nmemory_type_1\n\n\n\n\n0\n128.0\nSSD\n\n\n1\n128.0\nFlash Storage\n\n\n2\n256.0\nSSD\n\n\n3\n512.0\nSSD\n\n\n4\n256.0\nSSD\n\n\n...\n...\n...\n\n\n1269\n500.0\nHDD\n\n\n1270\n128.0\nSSD\n\n\n1271\n512.0\nSSD\n\n\n1272\n64.0\nFlash Storage\n\n\n1273\n1000.0\nHDD\n\n\n\n\n1244 rows × 2 columns\n\n\n\nLet’s repeat this for memory_2 also\n\ndf['memory_capacity_2'] = df['memory_2'].str.extract(r'(\\d+)').astype('float64')\ndf['memory_type_2'] = df['memory_2'].str.replace(r'(\\d+[A-Z]{2})', '', regex = True).str.strip()\n\ndf[['memory_capacity_2', 'memory_type_2']].dropna()\n\n\n\n\n\n\n\n\nmemory_capacity_2\nmemory_type_2\n\n\n\n\n21\n1000.0\nHDD\n\n\n28\n256.0\nSSD\n\n\n37\n1000.0\nHDD\n\n\n41\n1000.0\nHDD\n\n\n47\n1000.0\nHDD\n\n\n...\n...\n...\n\n\n1233\n1000.0\nHDD\n\n\n1238\n1000.0\nHDD\n\n\n1247\n1000.0\nHDD\n\n\n1256\n1000.0\nHDD\n\n\n1259\n1000.0\nHDD\n\n\n\n\n204 rows × 2 columns\n\n\n\n\n\n\nLet’s convert all the columns that can be numeric into numeric or float i.e, ram, inches, weight\n\ndf['ram_gb'] = df['ram'].str.replace('GB', '').astype('int')\n\ndf['inches_size'] = pd.to_numeric(df['inches'], errors= 'coerce')\n\ndf['weight_kg'] = df['weight'].replace('?', np.nan).str.replace('kg', '').astype('float64')\n\ndf[['ram_gb', 'inches_size', 'weight_kg']]\n\n\n\n\n\n\n\n\nram_gb\ninches_size\nweight_kg\n\n\n\n\n0\n8\n13.3\n1.37\n\n\n1\n8\n13.3\n1.34\n\n\n2\n8\n15.6\n1.86\n\n\n3\n16\n15.4\n1.83\n\n\n4\n8\n13.3\n1.37\n\n\n...\n...\n...\n...\n\n\n1269\n4\n15.6\n2.20\n\n\n1270\n4\n14.0\n1.80\n\n\n1271\n16\n13.3\n1.30\n\n\n1272\n2\n14.0\n1.50\n\n\n1273\n6\n15.6\n2.19\n\n\n\n\n1244 rows × 3 columns\n\n\n\nLet’s look at data once more\n\ndf.info()\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nIndex: 1244 entries, 0 to 1273\nData columns (total 29 columns):\n #   Column             Non-Null Count  Dtype  \n---  ------             --------------  -----  \n 0   company            1244 non-null   object \n 1   typename           1244 non-null   object \n 2   inches             1244 non-null   object \n 3   screenresolution   1244 non-null   object \n 4   cpu                1244 non-null   object \n 5   ram                1244 non-null   object \n 6   memory             1244 non-null   object \n 7   gpu                1244 non-null   object \n 8   opsys              1244 non-null   object \n 9   weight             1244 non-null   object \n 10  price              1244 non-null   float64\n 11  cpu_brand          1244 non-null   object \n 12  cpu_name           1244 non-null   object \n 13  cpu_ghz            1244 non-null   float64\n 14  resolution         1244 non-null   object \n 15  touchscreen        1244 non-null   int64  \n 16  display_type       1244 non-null   object \n 17  gpu_brand          1244 non-null   object \n 18  gpu_name           1244 non-null   object \n 19  memory_list        1244 non-null   object \n 20  memory_1           1244 non-null   object \n 21  memory_2           204 non-null    object \n 22  memory_capacity_1  1243 non-null   float64\n 23  memory_type_1      1244 non-null   object \n 24  memory_capacity_2  204 non-null    float64\n 25  memory_type_2      204 non-null    object \n 26  ram_gb             1244 non-null   int64  \n 27  inches_size        1243 non-null   float64\n 28  weight_kg          1243 non-null   float64\ndtypes: float64(6), int64(2), object(21)\nmemory usage: 323.9+ KB\n\n\n11 columns just were made into 29 columns among which repeated columns are not necessary to build a model so let’s remove them.\n\ndf_clean = df.drop(columns = ['ram','screenresolution', 'cpu', 'memory', 'memory_list',\n                              'memory_1', 'memory_2' ,'gpu', 'weight', 'inches'])\n\nprint(df_clean.info())\ndf_clean.head(5)\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nIndex: 1244 entries, 0 to 1273\nData columns (total 19 columns):\n #   Column             Non-Null Count  Dtype  \n---  ------             --------------  -----  \n 0   company            1244 non-null   object \n 1   typename           1244 non-null   object \n 2   opsys              1244 non-null   object \n 3   price              1244 non-null   float64\n 4   cpu_brand          1244 non-null   object \n 5   cpu_name           1244 non-null   object \n 6   cpu_ghz            1244 non-null   float64\n 7   resolution         1244 non-null   object \n 8   touchscreen        1244 non-null   int64  \n 9   display_type       1244 non-null   object \n 10  gpu_brand          1244 non-null   object \n 11  gpu_name           1244 non-null   object \n 12  memory_capacity_1  1243 non-null   float64\n 13  memory_type_1      1244 non-null   object \n 14  memory_capacity_2  204 non-null    float64\n 15  memory_type_2      204 non-null    object \n 16  ram_gb             1244 non-null   int64  \n 17  inches_size        1243 non-null   float64\n 18  weight_kg          1243 non-null   float64\ndtypes: float64(6), int64(2), object(11)\nmemory usage: 226.7+ KB\nNone\n\n\n\n\n\n\n\n\n\ncompany\ntypename\nopsys\nprice\ncpu_brand\ncpu_name\ncpu_ghz\nresolution\ntouchscreen\ndisplay_type\ngpu_brand\ngpu_name\nmemory_capacity_1\nmemory_type_1\nmemory_capacity_2\nmemory_type_2\nram_gb\ninches_size\nweight_kg\n\n\n\n\n0\nApple\nUltrabook\nmacOS\n71378.6832\nIntel\nCore i5\n2.3\n2560x1600\n0\nIPS Panel Retina Display\nIntel\nIris Plus Graphics 640\n128.0\nSSD\nNaN\nNaN\n8\n13.3\n1.37\n\n\n1\nApple\nUltrabook\nmacOS\n47895.5232\nIntel\nCore i5\n1.8\n1440x900\n0\n\nIntel\nHD Graphics 6000\n128.0\nFlash Storage\nNaN\nNaN\n8\n13.3\n1.34\n\n\n2\nHP\nNotebook\nNo OS\n30636.0000\nIntel\nCore i5 7200U\n2.5\n1920x1080\n0\n\nIntel\nHD Graphics 620\n256.0\nSSD\nNaN\nNaN\n8\n15.6\n1.86\n\n\n3\nApple\nUltrabook\nmacOS\n135195.3360\nIntel\nCore i7\n2.7\n2880x1800\n0\nIPS Panel Retina Display\nAMD\nRadeon Pro 455\n512.0\nSSD\nNaN\nNaN\n16\n15.4\n1.83\n\n\n4\nApple\nUltrabook\nmacOS\n96095.8080\nIntel\nCore i5\n3.1\n2560x1600\n0\nIPS Panel Retina Display\nIntel\nIris Plus Graphics 650\n256.0\nSSD\nNaN\nNaN\n8\n13.3\n1.37\n\n\n\n\n\n\n\nNow that dataset is clean let’s go for EDA.\n\n\n\n\nAs we have 8 numeric columns, let’s start with correlation plot.\n\nsns.heatmap(df_clean.select_dtypes(include = ['int64', 'float64']).corr(),\n                                   annot = True, cmap = 'coolwarm')\nplt.show()\n\n\n\n\nWe can see that price has a strong positive correlation with ram_gb and cpu_ghz.\nLet’s check the distribution of price column.\n\nsns.histplot(df_clean['price'], bins = 20, kde = True)\nplt.show()\n\n\n\n\nThe plot is right skewed, we can log transform the price column to make it more normal.\n\ndf_clean['price_log'] = np.log1p(df_clean['price'])\n\nsns.histplot(df_clean['price_log'], bins = 50, kde = True)\nplt.show()\n\n\n\n\nprice_log is more normally distributed, let’s check the correlation of price_log with other columns.\n\nsns.heatmap(df_clean.drop(columns = ['price']).select_dtypes(include = ['int64', 'float64']).corr(),\n                                    annot = True, cmap = 'coolwarm')\nplt.show()\n\n\n\n\nWe can see that price_log has a strong positive correlation with ram_gb and cpu_ghz.\nLet’s plot price_log in a boxplot to get the outliers.\n\nax = sns.boxplot(x='price_log', data=df_clean)\nmax = df_clean['price_log'].max()\nplt.text(max, 0, f'{max:.2f}', ha='center', va='bottom', color='red')\nplt.xlabel('Price_log')\nplt.title('Boxplot of Price_log')\nplt.show()\n\n\n\n\nThere is only one outlier in the data, let’s remove it.\n\ndf_clean = df_clean[df_clean['price_log'] &lt; 12.6]\n\ndf_clean['price_log'].max()\n\nnp.float64(12.587885975858104)\n\n\nNow we have removed the outliers from price_log column. Let’s look at object columns starting with companies.\n\nsns.barplot(x = df_clean.company.value_counts().index,\n            y = df_clean.company.value_counts().values)\nplt.xlabel('Company')\nplt.ylabel('Count')\nplt.title('Company Counts')\nplt.xticks(rotation = 90)\nplt.show()\n\n\n\n\nLenovo, Dell, HP are the top 3 companies in the dataset.\n\ncompany_counts = df_clean.company.value_counts()\n\nprint((company_counts[:3].sum()/len(df_clean)).round(3))\n\n0.662\n\n\n66.2% of the laptops are from Lenovo, Dell, HP.\nLet’s look at cpu and it’s features.\n\nprint(df_clean.cpu_brand.nunique())\nprint(df_clean.cpu_ghz.nunique())\nprint(df_clean.cpu_name.nunique())\n\n3\n25\n93\n\n\nThere are 3 unique values in cpu_brand, 25 unique values in cpu_ghz, 93 unique values in cpu_name.\n\ncpu_brand_counts = df_clean.cpu_brand.value_counts()\ncpu_ghz_counts = df_clean.cpu_ghz.value_counts().sort_values(ascending = False)\ncpu_name_counts = df_clean.cpu_name.value_counts()\n\nprint(cpu_brand_counts)\nprint(cpu_ghz_counts.head(5))\n\ncpu_brand\nIntel      1182\nAMD          60\nSamsung       1\nName: count, dtype: int64\ncpu_ghz\n2.5    278\n2.8    161\n2.7    158\n1.6    118\n2.3     84\nName: count, dtype: int64\n\n\nMost of the laptops have Intel CPU, 2.4GHz is the most common CPU clock speed, Intel Core i5 is the most common CPU name.\nSamsung has only one laptop in the dataset, which is not ideal for building a model, let’s remove it.\n\ndf_clean = df_clean[df_clean.cpu_brand != 'Samsung']\n\ndf_clean.cpu_brand.unique()\n\narray(['Intel', 'AMD'], dtype=object)\n\n\nLet’s plot cpu_ghz to know the distribution of CPU clock speed.\n\nsns.barplot(x=cpu_ghz_counts.index.astype(str),\n            y=cpu_ghz_counts.values)\nplt.xlabel('CPU GHz')\nplt.ylabel('Count')\nplt.title('CPU GHz Distribution')\nplt.xticks(rotation=90)\nplt.show()\n\n\n\n\n2.5GHz is the most common CPU clock speed, followed by 2.8GHz and 2.7GHz.\n\nsns.barplot(x=df_clean.inches_size.value_counts().index.astype(str),\n            y=df_clean.inches_size.value_counts().values)\nplt.xlabel('Screen Size')\nplt.ylabel('Count')\nplt.xticks(rotation=90)\nplt.title('Laptops with screen sizes')\nplt.show()\n\n\n\n\n15.6 is the most common screen size, followed by 14.0 and 17.3 inches.\n\nscreen_size_counts = df_clean.inches_size.value_counts().sort_values(ascending = False)\n\nprint(screen_size_counts.head(6).sum()/screen_size_counts.sum())\n\n0.9621273166800967\n\n\nOnly 4 sizes make up 96.21% of the laptops in the dataset, which means we can drop the other sizes.\n\ndf_clean = df_clean[df_clean.inches_size.isin([13.3, 14.0, 15.6, 17.3, 11.6, 12.5])]\n\ndf_clean.inches_size.unique()\n\narray([13.3, 15.6, 14. , 17.3, 12.5, 11.6])\n\n\nLet’s look at correlation once again.\n\nsns.heatmap(df_clean.select_dtypes(include = ['int64', 'float64']).corr(),\n            annot = True, cmap = 'coolwarm')\nplt.show()"
  },
  {
    "objectID": "posts/laptop_price_prediction/laptop_price_prediction.html#model-building",
    "href": "posts/laptop_price_prediction/laptop_price_prediction.html#model-building",
    "title": "Laptop Price Prediction",
    "section": "",
    "text": "We have gone through different parameters of the data now it’s time to put that to building a model.\nI am going to build 2 models 1. Random Forest Regressor 2. Linear Regression Model\nand compare them to find the best model.\n\n\nImporting libraries for model building and evaluation with sklearn.\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.metrics import mean_squared_error, r2_score\n\n\n\n\nRandomForestRegressor can deal with Null values, so we don’t need to handle them for this model, but we need to handle them for LinearRegression model.\nLet’s create a copy of the dataset and drop the price column.\nThen we need to label encode the cpu_ghz, inches_size, ram_gb, memory_capacity_1, memory_capacity_2, resolution columns as they are ordinal data.\n\ndf_model = df_clean.copy().drop(columns=['price'])\n\nordinal_cols = ['cpu_ghz', 'inches_size', 'ram_gb', 'memory_capacity_1', 'memory_capacity_2', 'resolution', 'weight_kg']\n\nfor col in ordinal_cols:\n    le = LabelEncoder()\n    df_model[col] = le.fit_transform(df_model[col])\n\ndf_model.head()\n\n\n\n\n\n\n\n\ncompany\ntypename\nopsys\ncpu_brand\ncpu_name\ncpu_ghz\nresolution\ntouchscreen\ndisplay_type\ngpu_brand\ngpu_name\nmemory_capacity_1\nmemory_type_1\nmemory_capacity_2\nmemory_type_2\nram_gb\ninches_size\nweight_kg\nprice_log\n\n\n\n\n0\nApple\nUltrabook\nmacOS\nIntel\nCore i5\n13\n6\n0\nIPS Panel Retina Display\nIntel\nIris Plus Graphics 640\n4\nSSD\n5\nNaN\n4\n2\n35\n11.175769\n\n\n1\nApple\nUltrabook\nmacOS\nIntel\nCore i5\n7\n1\n0\n\nIntel\nHD Graphics 6000\n4\nFlash Storage\n5\nNaN\n4\n2\n32\n10.776798\n\n\n2\nHP\nNotebook\nNo OS\nIntel\nCore i5 7200U\n15\n3\n0\n\nIntel\nHD Graphics 620\n7\nSSD\n5\nNaN\n4\n4\n69\n10.329964\n\n\n4\nApple\nUltrabook\nmacOS\nIntel\nCore i5\n21\n6\n0\nIPS Panel Retina Display\nIntel\nIris Plus Graphics 650\n7\nSSD\n5\nNaN\n4\n2\n35\n11.473111\n\n\n5\nAcer\nNotebook\nWindows 10\nAMD\nA9-Series 9420\n20\n0\n0\n\nAMD\nRadeon R5\n8\nHDD\n5\nNaN\n2\n4\n90\n9.967072\n\n\n\n\n\n\n\nWe need to one hot encode the cpu_brand, gpu_brand, company, display_type, touchscreen, cpu_name, gpu_name columns as they are categorical data.\n\nnominal_cols = ['cpu_brand', 'gpu_brand', 'company', 'display_type', 'touchscreen', 'cpu_name', 'gpu_name', 'typename', 'opsys', 'memory_type_1', 'memory_type_2']\n\ndf_model = pd.get_dummies(df_model, columns=nominal_cols, drop_first=False)\n\nprint(df_model.shape)\ndf_model.head()\n\n(1194, 236)\n\n\n\n\n\n\n\n\n\ncpu_ghz\nresolution\nmemory_capacity_1\nmemory_capacity_2\nram_gb\ninches_size\nweight_kg\nprice_log\ncpu_brand_AMD\ncpu_brand_Intel\n...\nopsys_Windows 7\nopsys_macOS\nmemory_type_1_?\nmemory_type_1_Flash Storage\nmemory_type_1_HDD\nmemory_type_1_Hybrid\nmemory_type_1_SSD\nmemory_type_2_HDD\nmemory_type_2_Hybrid\nmemory_type_2_SSD\n\n\n\n\n0\n13\n6\n4\n5\n4\n2\n35\n11.175769\nFalse\nTrue\n...\nFalse\nTrue\nFalse\nFalse\nFalse\nFalse\nTrue\nFalse\nFalse\nFalse\n\n\n1\n7\n1\n4\n5\n4\n2\n32\n10.776798\nFalse\nTrue\n...\nFalse\nTrue\nFalse\nTrue\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\n\n\n2\n15\n3\n7\n5\n4\n4\n69\n10.329964\nFalse\nTrue\n...\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\nTrue\nFalse\nFalse\nFalse\n\n\n4\n21\n6\n7\n5\n4\n2\n35\n11.473111\nFalse\nTrue\n...\nFalse\nTrue\nFalse\nFalse\nFalse\nFalse\nTrue\nFalse\nFalse\nFalse\n\n\n5\n20\n0\n8\n5\n2\n4\n90\n9.967072\nTrue\nFalse\n...\nFalse\nFalse\nFalse\nFalse\nTrue\nFalse\nFalse\nFalse\nFalse\nFalse\n\n\n\n\n5 rows × 236 columns\n\n\n\n\n\n\n\n\nLet’s split the data into training and testing sets.\n\nx = df_model.drop(columns = ['price_log'])\ny = df_model['price_log']\n\nx_train, x_test, y_train, y_test = train_test_split(x, y, test_size = 0.2, random_state = 42)\n\n\n\n\nTraining the model with RandomForestRegressor and evaluating it with mean_squared_error and r2_score.\n\n#\n\nrf_model = RandomForestRegressor(n_estimators= 100, max_depth = 200, max_features = 20)\nrf_model.fit(x_train, y_train)\n\n\nRandomForestRegressor(max_depth=200, max_features=20)In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.RandomForestRegressor?Documentation for RandomForestRegressoriFitted\n        \n            \n                Parameters\n                \n\n\n\n\nn_estimators \n100\n\n\n\ncriterion \n'squared_error'\n\n\n\nmax_depth \n200\n\n\n\nmin_samples_split \n2\n\n\n\nmin_samples_leaf \n1\n\n\n\nmin_weight_fraction_leaf \n0.0\n\n\n\nmax_features \n20\n\n\n\nmax_leaf_nodes \nNone\n\n\n\nmin_impurity_decrease \n0.0\n\n\n\nbootstrap \nTrue\n\n\n\noob_score \nFalse\n\n\n\nn_jobs \nNone\n\n\n\nrandom_state \nNone\n\n\n\nverbose \n0\n\n\n\nwarm_start \nFalse\n\n\n\nccp_alpha \n0.0\n\n\n\nmax_samples \nNone\n\n\n\nmonotonic_cst \nNone\n\n\n\n\n            \n        \n    \n\n\n\ny_pred = rf_model.predict(x_test)\n\nmse = mean_squared_error(y_test, y_pred)\nr2 = r2_score(y_test, y_pred)\n\nprint(f'Mean Squared Error: {mse:.2f}')\nprint(f'R2 Score: {r2:.2f}')\n\nMean Squared Error: 0.04\nR2 Score: 0.89\n\n\nThe R2 Score is 0.88, which is good and Mean Squared Error is 0.04 which is also good for this model.\nLet’s plot the predicted vs actual values.\n\nsns.scatterplot(x=y_test, y=y_pred)\nplt.xlabel('Actual Price (log)')\nplt.ylabel('Predicted Price (log)')\nplt.title('Predicted vs Actual Price (log)')\nplt.plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], color='red', linestyle='--')\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\ndf_lr = df_clean.copy().drop(columns=['price'])\n\ndf_lr.info()\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nIndex: 1194 entries, 0 to 1273\nData columns (total 19 columns):\n #   Column             Non-Null Count  Dtype  \n---  ------             --------------  -----  \n 0   company            1194 non-null   object \n 1   typename           1194 non-null   object \n 2   opsys              1194 non-null   object \n 3   cpu_brand          1194 non-null   object \n 4   cpu_name           1194 non-null   object \n 5   cpu_ghz            1194 non-null   float64\n 6   resolution         1194 non-null   object \n 7   touchscreen        1194 non-null   int64  \n 8   display_type       1194 non-null   object \n 9   gpu_brand          1194 non-null   object \n 10  gpu_name           1194 non-null   object \n 11  memory_capacity_1  1193 non-null   float64\n 12  memory_type_1      1194 non-null   object \n 13  memory_capacity_2  201 non-null    float64\n 14  memory_type_2      201 non-null    object \n 15  ram_gb             1194 non-null   int64  \n 16  inches_size        1194 non-null   float64\n 17  weight_kg          1193 non-null   float64\n 18  price_log          1194 non-null   float64\ndtypes: float64(6), int64(2), object(11)\nmemory usage: 186.6+ KB\n\n\nWe nedd to deal with the missing values in the dataset for LinearRegression model.\n\ndf_lr.isnull().sum()\n\ncompany                0\ntypename               0\nopsys                  0\ncpu_brand              0\ncpu_name               0\ncpu_ghz                0\nresolution             0\ntouchscreen            0\ndisplay_type           0\ngpu_brand              0\ngpu_name               0\nmemory_capacity_1      1\nmemory_type_1          0\nmemory_capacity_2    993\nmemory_type_2        993\nram_gb                 0\ninches_size            0\nweight_kg              1\nprice_log              0\ndtype: int64\n\n\nThere are 1 missing values in weight_kg column and 1 missing value in memory_capacity_1, let’s fill it with the median and mean of the columns.\n\ndf_lr['weight_kg'] = df_lr['weight_kg'].fillna(df_lr['weight_kg'].median())\ndf_lr['memory_capacity_1'] = df_lr['memory_capacity_1'].fillna(df_lr['memory_capacity_1'].mean())\n\nmemory_capacity_2 and memory_type_2 are lots of missing values, let’s fill them with 0s respectively.\n\ndf_lr['memory_capacity_2'] = df_lr['memory_capacity_2'].fillna(0)\ndf_lr['memory_type_2'] = df_lr['memory_type_2'].fillna('None')\ndf_lr['memory_type_1'] = df_lr['memory_type_1'].replace({0: 'None', np.nan: 'None'})\n\nNow we have filled the missing values in the dataset for LinearRegression model. Let’s encode the data for LinearRegression model.\n\nordinal_cols = ['cpu_ghz', 'inches_size', 'ram_gb', 'memory_capacity_1', 'memory_capacity_2', 'resolution', 'weight_kg']\n\nfor col in ordinal_cols:\n    le = LabelEncoder()\n    df_lr[col] = le.fit_transform(df_lr[col])\n\ndf_lr.head(3)\n\n\n\n\n\n\n\n\ncompany\ntypename\nopsys\ncpu_brand\ncpu_name\ncpu_ghz\nresolution\ntouchscreen\ndisplay_type\ngpu_brand\ngpu_name\nmemory_capacity_1\nmemory_type_1\nmemory_capacity_2\nmemory_type_2\nram_gb\ninches_size\nweight_kg\nprice_log\n\n\n\n\n0\nApple\nUltrabook\nmacOS\nIntel\nCore i5\n13\n6\n0\nIPS Panel Retina Display\nIntel\nIris Plus Graphics 640\n4\nSSD\n0\nNone\n4\n2\n35\n11.175769\n\n\n1\nApple\nUltrabook\nmacOS\nIntel\nCore i5\n7\n1\n0\n\nIntel\nHD Graphics 6000\n4\nFlash Storage\n0\nNone\n4\n2\n32\n10.776798\n\n\n2\nHP\nNotebook\nNo OS\nIntel\nCore i5 7200U\n15\n3\n0\n\nIntel\nHD Graphics 620\n7\nSSD\n0\nNone\n4\n4\n69\n10.329964\n\n\n\n\n\n\n\nWe need to encode nominal data for LinearRegression model.\n\nnominal_cols = ['cpu_brand', 'gpu_brand', 'company', 'display_type', 'touchscreen', 'cpu_name', 'gpu_name', 'typename', 'opsys', 'memory_type_1', 'memory_type_2']\n\ndf_lr = pd.get_dummies(df_lr, columns=nominal_cols, drop_first=False)\n\ndf_lr.head(3)\n\n\n\n\n\n\n\n\ncpu_ghz\nresolution\nmemory_capacity_1\nmemory_capacity_2\nram_gb\ninches_size\nweight_kg\nprice_log\ncpu_brand_AMD\ncpu_brand_Intel\n...\nopsys_macOS\nmemory_type_1_?\nmemory_type_1_Flash Storage\nmemory_type_1_HDD\nmemory_type_1_Hybrid\nmemory_type_1_SSD\nmemory_type_2_HDD\nmemory_type_2_Hybrid\nmemory_type_2_None\nmemory_type_2_SSD\n\n\n\n\n0\n13\n6\n4\n0\n4\n2\n35\n11.175769\nFalse\nTrue\n...\nTrue\nFalse\nFalse\nFalse\nFalse\nTrue\nFalse\nFalse\nTrue\nFalse\n\n\n1\n7\n1\n4\n0\n4\n2\n32\n10.776798\nFalse\nTrue\n...\nTrue\nFalse\nTrue\nFalse\nFalse\nFalse\nFalse\nFalse\nTrue\nFalse\n\n\n2\n15\n3\n7\n0\n4\n4\n69\n10.329964\nFalse\nTrue\n...\nFalse\nFalse\nFalse\nFalse\nFalse\nTrue\nFalse\nFalse\nTrue\nFalse\n\n\n\n\n3 rows × 237 columns\n\n\n\nLet’s split the data into training and testing sets.\n\nx = df_lr.drop(columns = ['price_log'])\ny = df_lr['price_log']\n\nx_train, x_test, y_train, y_test = train_test_split(x, y, test_size = 0.2, random_state = 42)\n\n\n\n\nTraining the model with LinearRegression and evaluating it with mean_squared_error and r2_score.\n\nlr_model = LinearRegression()\n\nlr_model.fit(x_train, y_train)\n\n\nLinearRegression()In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.LinearRegression?Documentation for LinearRegressioniFitted\n        \n            \n                Parameters\n                \n\n\n\n\nfit_intercept \nTrue\n\n\n\ncopy_X \nTrue\n\n\n\ntol \n1e-06\n\n\n\nn_jobs \nNone\n\n\n\npositive \nFalse\n\n\n\n\n            \n        \n    \n\n\nModel evaluation\n\ny_pred = lr_model.predict(x_test)\n\nmse = mean_squared_error(y_test, y_pred)\nr2 = r2_score(y_test, y_pred)\n\n\nprint(f'Mean Squared Error: {mse:.2f}')\nprint(f'R2 Score: {r2:.2f}')\n\nMean Squared Error: 0.06\nR2 Score: 0.85\n\n\nThe R2 Score is 0.85, which is good and Mean Squared Error is 0.06 which is also good for this model.\nLet’s plot the predicted vs actual values.\n\nsns.scatterplot(x=y_test, y=y_pred)\nplt.xlabel('Actual Price (log)')\nplt.ylabel('Predicted Price (log)')\nplt.title('Predicted vs Actual Price (log)')\nplt.plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], color='red', linestyle='--')\nplt.show()"
  },
  {
    "objectID": "posts/laptop_price_prediction/laptop_price_prediction.html#model-featue-importance",
    "href": "posts/laptop_price_prediction/laptop_price_prediction.html#model-featue-importance",
    "title": "Laptop Price Prediction",
    "section": "",
    "text": "We know that RandomForestRegressor is a tree based model, so we can use feature_importances_ to get the importance of each feature.\n\nfeature_importances = pd.DataFrame({'feature': df_model.drop(columns=['price_log']).columns, 'importance': rf_model.feature_importances_.round(4)})\nfeature_importances = feature_importances.sort_values('importance', ascending=False)\nfeature_importances.head(10)\n\n\n\n\n\n\n\n\nfeature\nimportance\n\n\n\n\n4\nram_gb\n0.1551\n\n\n0\ncpu_ghz\n0.0818\n\n\n1\nresolution\n0.0742\n\n\n231\nmemory_type_1_SSD\n0.0699\n\n\n6\nweight_kg\n0.0613\n\n\n216\ntypename_Notebook\n0.0601\n\n\n2\nmemory_capacity_1\n0.0600\n\n\n229\nmemory_type_1_HDD\n0.0389\n\n\n5\ninches_size\n0.0261\n\n\n11\ngpu_brand_Nvidia\n0.0227\n\n\n\n\n\n\n\n\n\n\nWe know that LinearRegression is a linear model, so we can use coef_ to get the importance of each feature.\n\nfeature_importances = pd.DataFrame({'feature': df_lr.drop(columns=['price_log']).columns, 'importance': lr_model.coef_.round(4)})\nfeature_importances = feature_importances.sort_values('importance', ascending=False)\nfeature_importances.head(10)\n\n\n\n\n\n\n\n\nfeature\nimportance\n\n\n\n\n177\ngpu_name_Quadro M3000M\n0.7603\n\n\n117\ngpu_name_FirePro W6150M\n0.7565\n\n\n181\ngpu_name_Quadro M620M\n0.6756\n\n\n91\ncpu_name_Core i7 7820HK\n0.5666\n\n\n113\ncpu_name_Xeon E3-1535M v5\n0.4989\n\n\n174\ngpu_name_Quadro M2000M\n0.4989\n\n\n85\ncpu_name_Core i7 6820HQ\n0.4953\n\n\n149\ngpu_name_GeForce GTX1080\n0.4902\n\n\n59\ncpu_name_Core M 6Y54\n0.4829\n\n\n110\ncpu_name_Ryzen 1600\n0.4685\n\n\n\n\n\n\n\n\n\n\nI will choose RandomForestRegressor for hyperparameter tuning as it has features which are easily explainable and a tree based model can be easily tunable.\nGridSearchCV is used to tune the hyperparameters of the model. n_estimators is the number of trees in the forest, max_depth is the maximum depth of the tree, max_features is the number of features to consider when looking for the best split.\n\nfrom sklearn.model_selection import GridSearchCV\n\nparam_grid = {\n    'n_estimators': [50, 100],\n    'max_depth': [20, 30],\n    'max_features': [5, 10, 15]\n}\n\ngrid_search = GridSearchCV(estimator = rf_model, param_grid = param_grid, cv = 5, scoring = 'neg_mean_squared_error', verbose = 2)\ngrid_search.fit(x_train, y_train)\n\n\nprint(grid_search.best_params_)\nprint(grid_search.best_score_)\n\n{'max_depth': 30, 'max_features': 15, 'n_estimators': 50}\n-0.043978114678904986\n\n\ngrid_search.best_params_ gives the best parameters for the model depending on the scoring metric which is neg_mean_squared_error in this case.\nThe best parameters are n_estimators = 100, max_depth = 30, max_features = 15 and the best score is -0.042.\n\nrf_model = RandomForestRegressor(n_estimators = 100, max_depth = 30, max_features = 15)\nrf_model.fit(x_train, y_train)\n\n\nRandomForestRegressor(max_depth=30, max_features=15)In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.RandomForestRegressor?Documentation for RandomForestRegressoriFitted\n        \n            \n                Parameters\n                \n\n\n\n\nn_estimators \n100\n\n\n\ncriterion \n'squared_error'\n\n\n\nmax_depth \n30\n\n\n\nmin_samples_split \n2\n\n\n\nmin_samples_leaf \n1\n\n\n\nmin_weight_fraction_leaf \n0.0\n\n\n\nmax_features \n15\n\n\n\nmax_leaf_nodes \nNone\n\n\n\nmin_impurity_decrease \n0.0\n\n\n\nbootstrap \nTrue\n\n\n\noob_score \nFalse\n\n\n\nn_jobs \nNone\n\n\n\nrandom_state \nNone\n\n\n\nverbose \n0\n\n\n\nwarm_start \nFalse\n\n\n\nccp_alpha \n0.0\n\n\n\nmax_samples \nNone\n\n\n\nmonotonic_cst \nNone\n\n\n\n\n            \n        \n    \n\n\n\ny_pred = rf_model.predict(x_test)\nmse = mean_squared_error(y_test, y_pred)\nr2 = r2_score(y_test, y_pred)\nprint(f'Mean Squared Error: {mse:.2f}')\nprint(f'R2 Score: {r2:.2f}')\n\nMean Squared Error: 0.05\nR2 Score: 0.88\n\n\nEven after tuning the hyper parameters, the R2 Score is 0.88 and Mean Squared Error is 0.04 which is same as the previous model, but by using this model we can save memory and time."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Projects with Python",
    "section": "",
    "text": "This is a blog for projects completed successfully by me with Python programming language. This blog will include projects from basic “Exploratory Data Analysis(EDA)” to complex “Machine Learning(ML)” projects.\n\nTo find all the other projects using other tools visit my Data Science Portfolio Website\n\n\n\n\n\n\n\n\n\n  \n\n\n\n\nLaptop Price Prediction\n\n\n\n\n\n\n\nCode\n\n\nEDA\n\n\nML\n\n\nRandom Forest\n\n\n\n\n\n\n\n\n\n\n\nJun 18, 2025\n\n\nAjay Shankar A\n\n\n\n\n\n\n  \n\n\n\n\nCYCLIST BIKE SHARE\n\n\n\n\n\n\n\nAnalysis\n\n\nCode\n\n\nEDA\n\n\n\n\n\n\n\n\n\n\n\nJun 17, 2025\n\n\nAjay Shankar A\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "Ajay Shankar A",
    "section": "",
    "text": "This is a blog for projects completed successfully by me with Python programming language. This blog will include projects from basic “Exploratory Data Analysis(EDA)” to complex “Machine Learning(ML)” projects."
  },
  {
    "objectID": "about.html#education",
    "href": "about.html#education",
    "title": "Ajay Shankar A",
    "section": "1 EDUCATION",
    "text": "1 EDUCATION\n\nUniversity of Agricultural Sciences, Dharwad | Dharwad, Karnataka | Masters in Forest Biology and Tree Improvement | Sept 2019 - Nov 2022\nCollege of Forestry, Sirsi | Uttara Kannada, Karnataka | B.Sc in Forestry | Aug 2015 - April 2019"
  },
  {
    "objectID": "about.html#experience",
    "href": "about.html#experience",
    "title": "Ajay Shankar A",
    "section": "2 Experience",
    "text": "2 Experience\n\nTechnical Assistant | Social Forest Department, Siruguppa | Dec 2022 - Present\nResearch Associate | EMPRI | May 2022 - Aug 2022"
  },
  {
    "objectID": "about.html#citationsprojects",
    "href": "about.html#citationsprojects",
    "title": "Ajay Shankar A",
    "section": "3 Citations(Projects)",
    "text": "3 Citations(Projects)\n\nAvailability of Wood for Handicrafts in Karnataka - Strengthening livelihoods and job creation.\nAn Assessment of Wood Availability in Karnataka"
  },
  {
    "objectID": "posts/cyclist_trip_analysis/cyc_trip_analysis.html",
    "href": "posts/cyclist_trip_analysis/cyc_trip_analysis.html",
    "title": "CYCLIST BIKE SHARE",
    "section": "",
    "text": "The analysis is done on Cyclist Trip Data obtained from Coursera Google Data Analytics course as part of Cap Stone Project.\nThe data contains month wise travel usage of bikes from the year of 2015-2023. We will be concentrating on data gathered in between July-2022 to June-2023 which will cover an entire year.\nLet’s load the required packages first\n\nLoading the required packages i.e., pandas, numpy, matplotlib, and seaborn.\n\n\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns"
  },
  {
    "objectID": "posts/cyclist_trip_analysis/cyc_trip_analysis.html#introduction",
    "href": "posts/cyclist_trip_analysis/cyc_trip_analysis.html#introduction",
    "title": "CYCLIST BIKE SHARE",
    "section": "",
    "text": "The analysis is done on Cyclist Trip Data obtained from Coursera Google Data Analytics course as part of Cap Stone Project.\nThe data contains month wise travel usage of bikes from the year of 2015-2023. We will be concentrating on data gathered in between July-2022 to June-2023 which will cover an entire year.\nLet’s load the required packages first\n\nLoading the required packages i.e., pandas, numpy, matplotlib, and seaborn.\n\n\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns"
  },
  {
    "objectID": "posts/cyclist_trip_analysis/cyc_trip_analysis.html#loading-and-formatting-data",
    "href": "posts/cyclist_trip_analysis/cyc_trip_analysis.html#loading-and-formatting-data",
    "title": "CYCLIST BIKE SHARE",
    "section": "2 Loading and Formatting Data",
    "text": "2 Loading and Formatting Data\n\nLet’s look at the structure of the data in one of the downloaded .csv files.\n\n\ntrp_jul_22 = pd.read_csv(\"F:/Data_Sci/Cap_Stone_Project/Cyclist_trip_data/202207-divvy-tripdata/202207-divvy-tripdata.csv\")\n\ntrp_jul_22.info()\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nRangeIndex: 823488 entries, 0 to 823487\nData columns (total 13 columns):\n #   Column              Non-Null Count   Dtype  \n---  ------              --------------   -----  \n 0   ride_id             823488 non-null  object \n 1   rideable_type       823488 non-null  object \n 2   started_at          823488 non-null  object \n 3   ended_at            823488 non-null  object \n 4   start_station_name  711457 non-null  object \n 5   start_station_id    711457 non-null  object \n 6   end_station_name    702537 non-null  object \n 7   end_station_id      702537 non-null  object \n 8   start_lat           823488 non-null  float64\n 9   start_lng           823488 non-null  float64\n 10  end_lat             822541 non-null  float64\n 11  end_lng             822541 non-null  float64\n 12  member_casual       823488 non-null  object \ndtypes: float64(4), object(9)\nmemory usage: 81.7+ MB\n\n\n\nLet’s look at the columns and try to understand what they represent\n\nride_id is the unique identification token generated for each ride that was initiated.\nrideable_type indicates the type of bike used for the ride.\nstarted_at and ended_at give us the time when the ride began and the ride ended respectively.\nstart_station_name and end_station_name give us the names of stations where ride began and ended respectively.\nstart_station_id and end_station_id are unique ID’s given to stations.\nstart_lat and start_lng represent co-ordinates where the ride began.\nend_lat and end_lng represent co-ordinates where the ride stopped.\nmember_casual identifies if the rider is a member or casual rider of the bike.\n\nLets load data of remaining 11 months.\n\n\ntrp_aug_22 = pd.read_csv(\"F:/Data_Sci/Cap_Stone_Project/Cyclist_trip_data/202208-divvy-tripdata/202208-divvy-tripdata.csv\")\ntrp_sep_22 = pd.read_csv(\"F:/Data_Sci/Cap_Stone_Project/Cyclist_trip_data/202209-divvy-tripdata/202209-divvy-publictripdata.csv\")\ntrp_oct_22 = pd.read_csv(\"F:/Data_Sci/Cap_Stone_Project/Cyclist_trip_data/202210-divvy-tripdata/202210-divvy-tripdata_raw.csv\")\ntrp_nov_22 = pd.read_csv(\"F:/Data_Sci/Cap_Stone_Project/Cyclist_trip_data/202211-divvy-tripdata/202211-divvy-tripdata.csv\")\ntrp_dec_22 = pd.read_csv(\"F:/Data_Sci/Cap_Stone_Project/Cyclist_trip_data/202212-divvy-tripdata/202212-divvy-tripdata.csv\")\ntrp_jan_23 = pd.read_csv(\"F:/Data_Sci/Cap_Stone_Project/Cyclist_trip_data/202301-divvy-tripdata/202301-divvy-tripdata.csv\")\ntrp_feb_23 = pd.read_csv(\"F:/Data_Sci/Cap_Stone_Project/Cyclist_trip_data/202302-divvy-tripdata/202302-divvy-tripdata.csv\")\ntrp_mar_23 = pd.read_csv(\"F:/Data_Sci/Cap_Stone_Project/Cyclist_trip_data/202303-divvy-tripdata/202303-divvy-tripdata.csv\")\ntrp_apr_23 = pd.read_csv(\"F:/Data_Sci/Cap_Stone_Project/Cyclist_trip_data/202304-divvy-tripdata/202304-divvy-tripdata.csv\")\ntrp_may_23 = pd.read_csv(\"F:/Data_Sci/Cap_Stone_Project/Cyclist_trip_data/202305-divvy-tripdata/202305-divvy-tripdata.csv\")\ntrp_jun_23 = pd.read_csv(\"F:/Data_Sci/Cap_Stone_Project/Cyclist_trip_data/202306-divvy-tripdata/202306-divvy-tripdata.csv\")\n\nAs structure of .csv’s is same across the all the files lets combine all the .csv files into a single data frame which contains data of all 12 months.\n\nCombining all the monthly data to one previous year data(df_1year).\n\n\ndf_1year_raw = pd.concat([trp_jul_22, trp_aug_22, trp_sep_22, trp_oct_22, trp_nov_22, \n                      trp_dec_22, trp_jan_23, trp_feb_23, trp_mar_23, \n                      trp_apr_23, trp_may_23, trp_jun_23], ignore_index=True)\n\ndf_1year_raw.info()\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nRangeIndex: 5779444 entries, 0 to 5779443\nData columns (total 13 columns):\n #   Column              Dtype  \n---  ------              -----  \n 0   ride_id             object \n 1   rideable_type       object \n 2   started_at          object \n 3   ended_at            object \n 4   start_station_name  object \n 5   start_station_id    object \n 6   end_station_name    object \n 7   end_station_id      object \n 8   start_lat           float64\n 9   start_lng           float64\n 10  end_lat             float64\n 11  end_lng             float64\n 12  member_casual       object \ndtypes: float64(4), object(9)\nmemory usage: 573.2+ MB\n\n\n\ndf_1year data frame contains data from the month of July-2022 to June-2023."
  },
  {
    "objectID": "posts/cyclist_trip_analysis/cyc_trip_analysis.html#cleaning-the-data",
    "href": "posts/cyclist_trip_analysis/cyc_trip_analysis.html#cleaning-the-data",
    "title": "CYCLIST BIKE SHARE",
    "section": "3 Cleaning the Data",
    "text": "3 Cleaning the Data\n\nChecking and counting “NA” in each column of the data frame. Data is much better without “NA” as they can cause problems while aggregating data and calculating averages and sums. We can use map function to perform a function to all of the columns.\n\n\ndf_1year = df_1year_raw.copy()\n\ndf_1year.isna().sum()\n\nride_id                    0\nrideable_type              0\nstarted_at                 0\nended_at                   0\nstart_station_name    857860\nstart_station_id      857992\nend_station_name      915655\nend_station_id        915796\nstart_lat                  0\nstart_lng                  0\nend_lat                 5795\nend_lng                 5795\nmember_casual              0\ndtype: int64\n\n\n\nAs NA’s are not present in the times columns i.e, started_at and ended_at we don’t need to worry ourselves about NA during aggregation and manipulation of data but it is a good practice to do so.\nFinding the length or duration of the rides by making a new column ride_length in minutes and making sure that the ride_length is not negative by using if_else function. Eliminating stations where station names and longitude and latitude co-ordinates are not present.\n\n\n# Converting 'started_at' and 'ended_at' to datetime format\ndf_1year = df_1year.astype({'started_at': 'datetime64[ns]', 'ended_at': 'datetime64[ns]'})\n\n# Calculating ride length in minutes\ndf_1year['ride_length'] = (df_1year['ended_at'] - df_1year['started_at']).dt.total_seconds() / 60\n\n# Replacing negative ride lengths with NaN\ndf_1year['ride_length'] = df_1year['ride_length'].apply(lambda x:0 if x &lt; 0 else x)\n\n# Dropping rows with NaN values in 'ride_length', 'start_station_name',\n#  'end_station_name', 'start_lat', 'start_lng', 'end_lat', 'end_lng'\n\ndf_1year = ( df_1year[\n                    (df_1year['ride_length'] &gt; 0) & \n                    (df_1year['start_station_name'].notna()) & \n                    (df_1year['end_station_name'].notna()) & \n                    (df_1year['start_lat'].notna()) & \n                    (df_1year['start_lng'].notna()) & \n                    (df_1year['end_lat'].notna()) & \n                    (df_1year['end_lng'].notna())\n                    ].sort_values('ride_length', ascending=False)\n                    )\n\ndf_1year.info()\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nIndex: 4408996 entries, 717461 to 4375674\nData columns (total 14 columns):\n #   Column              Dtype         \n---  ------              -----         \n 0   ride_id             object        \n 1   rideable_type       object        \n 2   started_at          datetime64[ns]\n 3   ended_at            datetime64[ns]\n 4   start_station_name  object        \n 5   start_station_id    object        \n 6   end_station_name    object        \n 7   end_station_id      object        \n 8   start_lat           float64       \n 9   start_lng           float64       \n 10  end_lat             float64       \n 11  end_lng             float64       \n 12  member_casual       object        \n 13  ride_length         float64       \ndtypes: datetime64[ns](2), float64(5), object(7)\nmemory usage: 504.6+ MB"
  },
  {
    "objectID": "posts/cyclist_trip_analysis/cyc_trip_analysis.html#analysis-of-data",
    "href": "posts/cyclist_trip_analysis/cyc_trip_analysis.html#analysis-of-data",
    "title": "CYCLIST BIKE SHARE",
    "section": "4 Analysis of Data",
    "text": "4 Analysis of Data\n\nAggregating data by Rider type and Bike type.\n\nAggregating data to see “Average minutes per ride” grouped by “bike type” and “rider type” after removing rides less than 2 minutes (As rides less than 2 minutes tend to have the same start and stop stations).\n\n\ndf_1year_agg = (df_1year[df_1year['ride_length'] &gt;= 2]\n                     .groupby(['rideable_type', 'member_casual'])\n                     .agg(avg_ride_length=('ride_length', 'mean'),\n                          total_rides=('ride_length', 'count'),\n                          max_ride_length=('ride_length', 'max'))\n                     .round(2)\n                     .sort_values(by='avg_ride_length', ascending=False)\n                     .reset_index()\n                     )\n\ndf_1year_agg\n\n\n\n\n\n\nTable 1: Average minutes per ride\n\n\n\nrideable_type\nmember_casual\navg_ride_length\ntotal_rides\nmax_ride_length\n\n\n\n\n0\ndocked_bike\ncasual\n50.44\n136794\n32035.45\n\n\n1\nclassic_bike\ncasual\n24.80\n781530\n1497.75\n\n\n2\nelectric_bike\ncasual\n16.03\n709649\n479.98\n\n\n3\nclassic_bike\nmember\n13.49\n1630991\n1497.87\n\n\n4\nelectric_bike\nmember\n11.14\n984688\n480.00\n\n\n\n\n\n\n\n\nWe can clearly notice in Table 1 that member riders have more number of rides with both classic and electric bikes while the average ride length is higher with casual riders.\n\nCalculating and visualizing Average ride length by “Rider type”.\n\n\navg_ride_by_rideable_type = (\n    df_1year.rename(columns={'rideable_type': 'Bike Type', 'member_casual': 'Rider Type'})\n    .groupby(['Bike Type', 'Rider Type'])\n    .agg(\n        avg_ride_by_rideable_type=('ride_length', 'mean'),\n        total_rides=('ride_length', 'count')\n    )\n    .reset_index()\n)\n\nsns.set(rc={'figure.figsize':(10, 6)})\nsns.barplot(data=avg_ride_by_rideable_type,\n            x='Rider Type', y='avg_ride_by_rideable_type', hue='Bike Type')\nplt.title('Average Ride Length by Rider Type and Bike Type')\nplt.xlabel('Rider Type')\nplt.ylabel('Average Ride Length (minutes)')\nplt.legend(title='Bike Type')\nplt.xticks(rotation=45)\nplt.tight_layout(rect=[0, 0, 1, 0.96])\nplt.show()\n\n\n\n\nFigure 1: Average Ride Length by Rider type and Member type\n\n\n\n\nThe above Figure 1 clearly shows that members average ride lengths between bike types doesn’t differ much for member riders but differs with casual riders upto 8 minutes.\n\n\n\n\n\n\nNote\n\n\n\nFurther down in the analysis “docked_bike” type is dropped as no proper documentation is available in the course.\n\n\n\n\n4.1 Analysing data by Time of the year and Ride Length\n\nRide Patterns Across the Weeks and Months of the Year\n\nCalculating and visualizing ride patterns in a week for number of rides.\n\n\n# Define the order for rideable_type\nrideable_order = [\"classic_bike\", \"electric_bike\", \"docked_bike\"]\n\n# Filter out 'docked_bike'\ndf_1year_filtered = df_1year[df_1year['rideable_type'] != 'docked_bike'].copy()\n\n# Extracting month and weekday names \n\ndf_1year_filtered['month'] = df_1year_filtered['started_at'].dt.month_name()\ndf_1year_filtered['weekday'] = df_1year_filtered['started_at'].dt.day_name()\n\n# Set categorical order for rideable_type, member_casual, and month\n\ndf_1year_filtered['rideable_type'] = pd.Categorical(df_1year_filtered['rideable_type'], categories=rideable_order, ordered=True)\n\n# Set categorical order for member_casual to control legend order\nmember_order = ['member', 'casual']\ndf_1year_filtered['member_casual'] = pd.Categorical(df_1year_filtered['member_casual'], categories=member_order, ordered=True)\n\nmonth_order = ['July', 'August', 'September', 'October', 'November', 'December',\n             'January', 'February', 'March', 'April', 'May', 'June']\n\nweekday_order = ['Monday', 'Tuesday', 'Wednesday', 'Thursday', 'Friday', 'Saturday', 'Sunday']\n\ndf_1year_filtered['month'] = pd.Categorical(df_1year_filtered['month'], categories=month_order, ordered=True)\ndf_1year_filtered['weekday'] = pd.Categorical(df_1year_filtered['weekday'], categories=weekday_order, ordered=True)\n\n# Plot \ng = sns.catplot(\n    data = df_1year_filtered,\n    x = 'weekday',\n    kind= 'count',\n    hue = 'member_casual',\n    col= 'month',\n    col_wrap= 4,\n    height= 4,\n    aspect= 1.5,\n    palette= 'Set2',\n    dodge = True\n)\n\ng.set_axis_labels(\"Weekday\", \"Number of Rides\")\ng.set_titles(col_template=\"{col_name}\")\ng.fig.suptitle(\n    \"Ride Patterns on Weekdays of each Month \\n From July-2022 to June-2023\", fontsize=18\n)\ng.add_legend(title=\"Rider Type\")\nplt.tight_layout()\nplt.show()\n\n\n\n\nFigure 2: Ride Patterns Across the Weeks of the Year\n\n\n\n\nThe above Figure 2 clearly shows how the number of rides change due to seasons. In winters the number of rides decrease very drastically may be because of temperature and snow. In Summers the number of rides are at its peak.\nThe number of rides driven by member riders increases through the week especially in working week days but for casual riders the rides increase in the weekends. The Figure 2 shows number of rides on Saturdays and Sundays by casual members overtake membership riders in the months of July and August.\nAggregating data for the visualization.\n\ndf_1year['month'] = df_1year['started_at'].dt.month_name()\ndf_1year['weekday'] = df_1year['started_at'].dt.day_name()\n\n# Set categorical order for month\nmonth_order = ['July', 'August', 'September', 'October', 'November', 'December',\n             'January', 'February', 'March', 'April', 'May', 'June']\ndf_1year['month'] = pd.Categorical(df_1year['month'], categories=month_order, ordered=True)\n\n# Set categorical order for weekday\nweekday_order = ['Monday', 'Tuesday', 'Wednesday', 'Thursday', 'Friday', 'Saturday', 'Sunday']\ndf_1year['weekday'] = pd.Categorical(df_1year['weekday'], categories=weekday_order, ordered=True)\n\n\nrides_on_days = df_1year.groupby(['month', 'weekday', 'member_casual']).agg(\n    avg_ride_length=('ride_length', 'mean'),\n    total_ride_length=('ride_length', 'sum'),\n    total_rides=('ride_length', 'count')\n).reset_index().sort_values(by=['month', 'weekday', 'member_casual']).round(2)\n\nrides_on_days.head(5)\n\n\n\n\n\n\n\n\nmonth\nweekday\nmember_casual\navg_ride_length\ntotal_ride_length\ntotal_rides\n\n\n\n\n0\nJuly\nMonday\ncasual\n27.10\n918148.73\n33874\n\n\n1\nJuly\nMonday\nmember\n13.22\n531467.68\n40209\n\n\n2\nJuly\nTuesday\ncasual\n22.48\n705945.77\n31404\n\n\n3\nJuly\nTuesday\nmember\n12.69\n588326.98\n46353\n\n\n4\nJuly\nWednesday\ncasual\n21.66\n704094.58\n32514\n\n\n\n\n\n\n\n\nsns.barplot(data=rides_on_days, x='weekday', y='avg_ride_length', hue='member_casual', \n            palette='Set1', errorbar=None, estimator=np.mean)\nplt.title('Average Ride Length by Weekday and Rider Type')\nplt.xlabel('Weekday')\nplt.ylabel('Average Ride Length (minutes)')\nplt.legend(title='Rider Type')\nplt.xticks(rotation=45)\nplt.show()\n\n\n\n\nFigure 3: Average Ride Length by Weekday and Rider Type\n\n\n\n\nThe above Figure 3 shows that the average ride length is higher for casual riders than member riders on all the days of the week. The average ride length is highest on Saturdays and Sundays for both the rider types.\n\nsns.barplot(data=rides_on_days, x='month', y='avg_ride_length', hue='member_casual', \n            palette='Set1', errorbar=None)\nplt.title('Average Ride Length by Month and Rider Type')\nplt.xlabel('Month')\nplt.ylabel('Average Ride Length (minutes)')\nplt.legend(title='Rider Type')\nplt.xticks(rotation=45)\nplt.show()\n\n\n\n\nFigure 4: Average Ride Length by Month and Rider Type\n\n\n\n\nThe above Figure 4 shows that the average ride length is higher for casual riders than member riders in all the months of the year. The average ride length is highest in the month of August for both the rider types.\n\nsns.lineplot(data=rides_on_days, x='weekday', y='total_rides', hue='member_casual', \n            palette='Set1', errorbar=None)\nplt.title('Number of Rides by Weekday and Rider Type')\nplt.xlabel('Weekday')\nplt.ylabel('Number of Rides')\nplt.legend(title='Rider Type')\nplt.show()\n\n\n\n\nFigure 5: Number of Rides by Weekday and Rider Type\n\n\n\n\nThe above Figure 5 shows that the number of rides is higher for member riders than casual riders on all the days of the week. The number of rides is meet on Saturdays for both the rider types.\n\nsns.lineplot(data=rides_on_days, x='month', y='total_rides', hue='member_casual', \n            palette='Set1', errorbar=None)\nplt.title('Number of Rides by Month and Rider Type')\nplt.xlabel('Month')\nplt.ylabel('Number of Rides')\nplt.legend(title='Rider Type')\nplt.show()\n\n\n\n\nFigure 6: Number of Rides by Month and Rider Type\n\n\n\n\nThe above Figure 6 shows that the number of rides is higher for member riders than casual riders in all the months of the year. The number of rides is highest in the month of August for both the rider types.\n\n\nRide Patterns Across the Hours of the Day\n\nCalculating and visualizing ride patterns in a day for number of rides.\n\n\n# Extracting hour from started_at\ndf_1year['hour'] = df_1year['started_at'].dt.hour.astype(str).str.zfill(2)  # Format hour as two digits\n\n# Set categorical order for hour\nhour_order = [f\"{hour:02d}\" for hour in range(24)]\n\ndf_1year['hour'] = pd.Categorical(df_1year['hour'], categories=hour_order, ordered=True)\n\nrides_by_hour = df_1year.groupby(['hour', 'member_casual']).agg(total_rides=('ride_length', 'count')).reset_index()\n\nsns.barplot(data=rides_by_hour, x='hour', y='total_rides', hue='member_casual', \n            palette='Set1', errorbar=None)\nplt.title('Ride Patterns Across the Hours of the Day')\nplt.xlabel('Hour of the Day')\nplt.ylabel('Number of Rides')\nplt.legend(title='Rider Type')\nplt.show()\n\n\n\n\nFigure 7: Ride Patterns Across the Hours of the Day\n\n\n\n\nThe above Figure 7 shows that the number of rides is higher for member riders than casual riders in the morning hours and evening hours. The number of rides is highest in the evening hours for both the rider types."
  },
  {
    "objectID": "posts/cyclist_trip_analysis/cyc_trip_analysis.html#conclusion",
    "href": "posts/cyclist_trip_analysis/cyc_trip_analysis.html#conclusion",
    "title": "CYCLIST BIKE SHARE",
    "section": "5 Conclusion",
    "text": "5 Conclusion\nThe analysis of the cyclist trip data reveals several key insights:\n\n5.1 Rider Patterns\n\nMember riders tend to use bikes more frequently than casual riders, especially during weekdays.\nCasual riders show a preference for weekends, with a significant increase in rides during Saturdays and Sundays.\n\n\n\n5.2 Ride Length\n\nThe average ride length is generally longer for casual riders compared to member riders.\nThe longest average ride lengths occur on weekends, particularly for casual riders.\n\n\n\n5.3 Seasonal Trends\n\nThe number of rides fluctuates significantly throughout the year, with peaks in summer months (July and August) and a noticeable drop in winter months (January and February).\nThe analysis indicates that weather and seasonal changes have a substantial impact on cycling patterns.\nThe data suggests that member riders maintain a more consistent usage pattern throughout the year compared to casual riders.\n\n\n\n5.4 Temporal Patterns\n\nRide patterns vary by time of day, with peak usage in the morning and evening hours.\nThe analysis highlights the importance of understanding temporal patterns to optimize bike availability and station placements."
  },
  {
    "objectID": "posts/cyclist_trip_analysis/cyc_trip_analysis.html#recommendations",
    "href": "posts/cyclist_trip_analysis/cyc_trip_analysis.html#recommendations",
    "title": "CYCLIST BIKE SHARE",
    "section": "6 Recommendations",
    "text": "6 Recommendations\n\nInfrastructure Improvements: Consider adding more bike stations in areas with high casual rider activity, especially during weekends.\nPromotional Campaigns: Encourage casual riders to become members by offering incentives, such as discounts or free trials, to increase overall ridership.\nSeasonal Promotions: Implement seasonal promotions to boost ridership during colder months, potentially by offering discounts or special events to attract casual riders.\nData-Driven Decisions: Continue to analyze ride patterns regularly to adapt to changing user behaviors and preferences, ensuring that the bike-sharing system remains efficient and user-friendly."
  },
  {
    "objectID": "posts/laptop_price_prediction/laptop_price_prediction.html#conclusion",
    "href": "posts/laptop_price_prediction/laptop_price_prediction.html#conclusion",
    "title": "Laptop Price Prediction",
    "section": "",
    "text": "In this project, we successfully built a model to predict laptop prices based on various features. We explored the dataset, cleaned it, and extracted useful features. We built two models, RandomForestRegressor and LinearRegression, and compared their performance. The RandomForestRegressor performed better with an R2 Score of 0.88 and a Mean Squared Error of 0.04. We also tuned the hyperparameters of the model to improve its performance."
  },
  {
    "objectID": "posts/laptop_price_prediction/laptop_price_prediction.html#streamlit-app",
    "href": "posts/laptop_price_prediction/laptop_price_prediction.html#streamlit-app",
    "title": "Laptop Price Prediction",
    "section": "",
    "text": "The final was deployed as a Streamlit app, which allows users to input laptop specifications and get the predicted price. The app is available at Laptop Price Prediction App.\nCheck out the app to see how it works and try it out with different laptop specifications."
  },
  {
    "objectID": "posts/laptop_pc_pred/laptop_price_prediction.html",
    "href": "posts/laptop_pc_pred/laptop_price_prediction.html",
    "title": "Laptop Price Prediction",
    "section": "",
    "text": "This project aims to predict laptop prices based on various features such as specifications, brand, and market.\n\n\nThe model is trained on the “Laptop Price Prediction Dataset” from Kaggle, which contains comprehensive laptop specifications and prices from various manufacturers.\nKaggle Dataset Link: Laptop Price Prediction Dataset\n\n\n\n\n\nPandas, NumPy, Matplotlib, and Seaborn are used for data manipulation, numerical operations, and visualization.\n\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nImporting dataset\n\ndf = pd.read_csv(\"F:/Odin_school/Capstone_projects/ml_capstone/laptop.csv\")\n\ndf.head()\n\n\n\n\n\n\n\n\nUnnamed: 0.1\nUnnamed: 0\nCompany\nTypeName\nInches\nScreenResolution\nCpu\nRam\nMemory\nGpu\nOpSys\nWeight\nPrice\n\n\n\n\n0\n0\n0.0\nApple\nUltrabook\n13.3\nIPS Panel Retina Display 2560x1600\nIntel Core i5 2.3GHz\n8GB\n128GB SSD\nIntel Iris Plus Graphics 640\nmacOS\n1.37kg\n71378.6832\n\n\n1\n1\n1.0\nApple\nUltrabook\n13.3\n1440x900\nIntel Core i5 1.8GHz\n8GB\n128GB Flash Storage\nIntel HD Graphics 6000\nmacOS\n1.34kg\n47895.5232\n\n\n2\n2\n2.0\nHP\nNotebook\n15.6\nFull HD 1920x1080\nIntel Core i5 7200U 2.5GHz\n8GB\n256GB SSD\nIntel HD Graphics 620\nNo OS\n1.86kg\n30636.0000\n\n\n3\n3\n3.0\nApple\nUltrabook\n15.4\nIPS Panel Retina Display 2880x1800\nIntel Core i7 2.7GHz\n16GB\n512GB SSD\nAMD Radeon Pro 455\nmacOS\n1.83kg\n135195.3360\n\n\n4\n4\n4.0\nApple\nUltrabook\n13.3\nIPS Panel Retina Display 2560x1600\nIntel Core i5 3.1GHz\n8GB\n256GB SSD\nIntel Iris Plus Graphics 650\nmacOS\n1.37kg\n96095.8080\n\n\n\n\n\n\n\n\n\n\n\n\n\n\ndf.isnull().sum()\n\nUnnamed: 0.1         0\nUnnamed: 0          30\nCompany             30\nTypeName            30\nInches              30\nScreenResolution    30\nCpu                 30\nRam                 30\nMemory              30\nGpu                 30\nOpSys               30\nWeight              30\nPrice               30\ndtype: int64\n\n\nAll columns except Unnamed: 0.1 has missing values, which might mean that first column may just be an index column. Let’s drop it and check Unnamed: 0 column.\n\ndf = df.drop(['Unnamed: 0.1'], axis=1)\n\ndf.isnull().sum()\n\nUnnamed: 0          30\nCompany             30\nTypeName            30\nInches              30\nScreenResolution    30\nCpu                 30\nRam                 30\nMemory              30\nGpu                 30\nOpSys               30\nWeight              30\nPrice               30\ndtype: int64\n\n\nNull values are still present lets drop the rows with Null values.\n\ndf = df.dropna()\ndf.info()\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nIndex: 1273 entries, 0 to 1302\nData columns (total 12 columns):\n #   Column            Non-Null Count  Dtype  \n---  ------            --------------  -----  \n 0   Unnamed: 0        1273 non-null   float64\n 1   Company           1273 non-null   object \n 2   TypeName          1273 non-null   object \n 3   Inches            1273 non-null   object \n 4   ScreenResolution  1273 non-null   object \n 5   Cpu               1273 non-null   object \n 6   Ram               1273 non-null   object \n 7   Memory            1273 non-null   object \n 8   Gpu               1273 non-null   object \n 9   OpSys             1273 non-null   object \n 10  Weight            1273 non-null   object \n 11  Price             1273 non-null   float64\ndtypes: float64(2), object(10)\nmemory usage: 129.3+ KB\n\n\nThere are 1273 non-null entries in the dataset now, which means we have successfully removed rows with missing values. Let’s check if Unnamed: 0 column is just an index column or not.\n\ndf['Unnamed: 0'].nunique()\n\n1273\n\n\nIt seems that Unnamed: 0 column is just an index column, let’s drop it as well.\n\ndf = df.drop(['Unnamed: 0'], axis=1)\ndf.info()\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nIndex: 1273 entries, 0 to 1302\nData columns (total 11 columns):\n #   Column            Non-Null Count  Dtype  \n---  ------            --------------  -----  \n 0   Company           1273 non-null   object \n 1   TypeName          1273 non-null   object \n 2   Inches            1273 non-null   object \n 3   ScreenResolution  1273 non-null   object \n 4   Cpu               1273 non-null   object \n 5   Ram               1273 non-null   object \n 6   Memory            1273 non-null   object \n 7   Gpu               1273 non-null   object \n 8   OpSys             1273 non-null   object \n 9   Weight            1273 non-null   object \n 10  Price             1273 non-null   float64\ndtypes: float64(1), object(10)\nmemory usage: 119.3+ KB\n\n\nNow we have 1273 non-null entries in the dataset and no missing values. Let’s check for duplicates in the data.\n\nprint(df.duplicated().sum())\n\ndf[df.duplicated()].sort_values(by='Company', ascending=True).head(3)\n\n29\n\n\n\n\n\n\n\n\n\nCompany\nTypeName\nInches\nScreenResolution\nCpu\nRam\nMemory\nGpu\nOpSys\nWeight\nPrice\n\n\n\n\n1291\nAcer\nNotebook\n15.6\n1366x768\nIntel Celeron Dual Core N3060 1.6GHz\n4GB\n500GB HDD\nIntel HD Graphics 400\nLinux\n2.4kg\n15397.92\n\n\n1277\nAcer\nNotebook\n15.6\n1366x768\nIntel Celeron Dual Core N3060 1.6GHz\n4GB\n500GB HDD\nIntel HD Graphics 400\nLinux\n2.4kg\n15397.92\n\n\n1274\nAsus\nNotebook\n15.6\n1366x768\nIntel Celeron Dual Core N3050 1.6GHz\n4GB\n500GB HDD\nIntel HD Graphics\nWindows 10\n2.2kg\n19660.32\n\n\n\n\n\n\n\nThere are 29 duplicate records in the dataset, let’s drop them.\n\ndf = df.drop_duplicates()\n\ndf.info()\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nIndex: 1244 entries, 0 to 1273\nData columns (total 11 columns):\n #   Column            Non-Null Count  Dtype  \n---  ------            --------------  -----  \n 0   Company           1244 non-null   object \n 1   TypeName          1244 non-null   object \n 2   Inches            1244 non-null   object \n 3   ScreenResolution  1244 non-null   object \n 4   Cpu               1244 non-null   object \n 5   Ram               1244 non-null   object \n 6   Memory            1244 non-null   object \n 7   Gpu               1244 non-null   object \n 8   OpSys             1244 non-null   object \n 9   Weight            1244 non-null   object \n 10  Price             1244 non-null   float64\ndtypes: float64(1), object(10)\nmemory usage: 116.6+ KB\n\n\nNow we have 1244 non-null entries in the dataset and no missing values or duplicates.\nThere are 11 columns in the dataset, among which Price is the only column with folat64 data type rest are object data type.\n\n\n\n\nLet’s start with the basic statistics of the dataset.\n\ndf.describe(include='all')\n\n\n\n\n\n\n\n\nCompany\nTypeName\nInches\nScreenResolution\nCpu\nRam\nMemory\nGpu\nOpSys\nWeight\nPrice\n\n\n\n\ncount\n1244\n1244\n1244\n1244\n1244\n1244\n1244\n1244\n1244\n1244\n1244.000000\n\n\nunique\n19\n6\n25\n40\n118\n10\n40\n110\n9\n189\nNaN\n\n\ntop\nLenovo\nNotebook\n15.6\nFull HD 1920x1080\nIntel Core i5 7200U 2.5GHz\n8GB\n256GB SSD\nIntel HD Graphics 620\nWindows 10\n2.2kg\nNaN\n\n\nfreq\n282\n689\n621\n493\n183\n595\n401\n269\n1022\n106\nNaN\n\n\nmean\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n60606.224427\n\n\nstd\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n37424.636161\n\n\nmin\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n9270.720000\n\n\n25%\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n32655.445200\n\n\n50%\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n52693.920000\n\n\n75%\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n79813.440000\n\n\nmax\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n324954.720000\n\n\n\n\n\n\n\nThe dataset contains 11 columns with the following features: - Company: The brand of the laptop with 19 brands. - TypeName: The type of laptop, such as Ultrabook, Gaming, etc. - Inches: The size of the laptop screen in inches. - ScreenResolution: The resolution of the laptop screen. - Cpu: The type of CPU used in the laptop. - Ram: The amount of RAM in GB. - Memory: The type and size of storage memory (HDD/SSD). - Gpu: The type of GPU used in the laptop. - OpSys: The operating system installed on the laptop. - Weight: The weight of the laptop in kg. - Price: The price of the laptop in INR(Indian Rupees).\n\n\nLet’s extract some features from the existing columns to make the dataset more informative.\nFirst lets convert all the column names to lower case for consistency.\n\ndf.columns = df.columns.str.lower()\n\nprint(df.columns)\n\nIndex(['company', 'typename', 'inches', 'screenresolution', 'cpu', 'ram',\n       'memory', 'gpu', 'opsys', 'weight', 'price'],\n      dtype='object')\n\n\nLet’s check for the unique values in columns.\n\nprint(df.company.unique())\nprint(df.typename.unique())\nprint(df.ram.unique())\nprint(df.opsys.unique())\n\n['Apple' 'HP' 'Acer' 'Asus' 'Dell' 'Lenovo' 'Chuwi' 'MSI' 'Microsoft'\n 'Toshiba' 'Huawei' 'Xiaomi' 'Vero' 'Razer' 'Mediacom' 'Samsung' 'Google'\n 'Fujitsu' 'LG']\n['Ultrabook' 'Notebook' 'Gaming' '2 in 1 Convertible' 'Workstation'\n 'Netbook']\n['8GB' '16GB' '4GB' '2GB' '12GB' '64GB' '6GB' '32GB' '24GB' '1GB']\n['macOS' 'No OS' 'Windows 10' 'Mac OS X' 'Linux' 'Windows 10 S'\n 'Chrome OS' 'Windows 7' 'Android']\n\n\n\nprint(df.cpu[1])\nprint(df.screenresolution[6])\nprint(df.gpu[88])\n\nIntel Core i5 1.8GHz\nIPS Panel Retina Display 2880x1800\nNvidia GeForce GTX 1060\n\n\nwe can seperate values in columns to form new parameters i.e, - screenresolution can give us display_type, resolution & touchscreen - cpu can be seperated to form cpu and clockspeed - memory can be seperated to form memory and memory_type - gpu can be seperated to get gpu_company\n\n\n\n# cpu brand name\ndf['cpu_brand'] = df.cpu.str.split().str[0]\n\n# cpu name\ndf['cpu_name'] = df.cpu.str.replace(r'\\d+(?:\\.\\d+)?GHz', '', regex=True,).str.strip()\n# removing brand name\ndf['cpu_name'] = df.cpu_name.str.replace(r'^\\w+', '', regex=True).str.strip()\n\n# cpu clock speed\ndf['cpu_ghz'] = df.cpu.str.extract(r'(\\d+(?:\\.\\d+)?)GHz').astype('float64')\n\ndf[['cpu_brand', 'cpu_name', 'cpu_ghz']]\n\n\n\n\n\n\n\n\ncpu_brand\ncpu_name\ncpu_ghz\n\n\n\n\n0\nIntel\nCore i5\n2.3\n\n\n1\nIntel\nCore i5\n1.8\n\n\n2\nIntel\nCore i5 7200U\n2.5\n\n\n3\nIntel\nCore i7\n2.7\n\n\n4\nIntel\nCore i5\n3.1\n\n\n...\n...\n...\n...\n\n\n1269\nIntel\nCore i7 6500U\n2.5\n\n\n1270\nIntel\nCore i7 6500U\n2.5\n\n\n1271\nIntel\nCore i7 6500U\n2.5\n\n\n1272\nIntel\nCeleron Dual Core N3050\n1.6\n\n\n1273\nIntel\nCore i7 6500U\n2.5\n\n\n\n\n1244 rows × 3 columns\n\n\n\nNow, we have 3 columns act as seperate features for the price prediction.\n\n\n\n\nscreenresolution has many features ie., screen type, screen height, width, touch screen etc. Let’s extract all of them\n\n\n# display resolution\ndf['resolution'] = df['screenresolution'].str.extract(r'(\\d+x\\d+)')\n\n# touch screen or not\ndf['touchscreen'] = df['screenresolution'].apply(lambda x: 1 if 'Touchscreen' in x else 0)\n\n# Display type\ndf['display_type'] = df['screenresolution'].str.replace(r'\\d+x\\d+', \"\", regex = True).str.strip()\n\ndf['display_type'] = df['display_type'].str.replace(r'(Full HD|Quad HD|4K Ultra HD|/|\\+|Touchscreen)', '', regex = True).str.replace('/', '', regex = True).str.strip()\n\ndf[['resolution', 'touchscreen', 'display_type']]\n\n\n\n\n\n\n\n\nresolution\ntouchscreen\ndisplay_type\n\n\n\n\n0\n2560x1600\n0\nIPS Panel Retina Display\n\n\n1\n1440x900\n0\n\n\n\n2\n1920x1080\n0\n\n\n\n3\n2880x1800\n0\nIPS Panel Retina Display\n\n\n4\n2560x1600\n0\nIPS Panel Retina Display\n\n\n...\n...\n...\n...\n\n\n1269\n1366x768\n0\n\n\n\n1270\n1920x1080\n1\nIPS Panel\n\n\n1271\n3200x1800\n1\nIPS Panel\n\n\n1272\n1366x768\n0\n\n\n\n1273\n1366x768\n0\n\n\n\n\n\n1244 rows × 3 columns\n\n\n\nNow, we have another 3 columns to act as 3 seperate features.\n\ndf.touchscreen.sum()\n\nnp.int64(181)\n\n\n\n\n\nLet’s extarct gpu_brand and gpu_name from the column gpu\n\n# gpu brand\ndf['gpu_brand'] = df['gpu'].str.extract(r'^(\\w+)')\n\n# gpu name\ndf['gpu_name'] = df['gpu'].str.replace(r'^(\\w+)', '', regex = True).str.strip()\n\ndf[['gpu_brand', 'gpu_name']]\n\n\n\n\n\n\n\n\ngpu_brand\ngpu_name\n\n\n\n\n0\nIntel\nIris Plus Graphics 640\n\n\n1\nIntel\nHD Graphics 6000\n\n\n2\nIntel\nHD Graphics 620\n\n\n3\nAMD\nRadeon Pro 455\n\n\n4\nIntel\nIris Plus Graphics 650\n\n\n...\n...\n...\n\n\n1269\nNvidia\nGeForce 920M\n\n\n1270\nIntel\nHD Graphics 520\n\n\n1271\nIntel\nHD Graphics 520\n\n\n1272\nIntel\nHD Graphics\n\n\n1273\nAMD\nRadeon R5 M330\n\n\n\n\n1244 rows × 2 columns\n\n\n\n\n\n\nMost of the laptops have two drives which need to be seperated and type of memory is also in the memory so we need to seperate them both after seperating the drives.\n\nFirst replace the TB with GB(1TB ~ 1000GB)\n+ seperates two drives, str.split() function can be used to list the two memory drives and then they are slotted into seperate columns.\n\n\ndf.memory = df.memory.str.replace(r'1.0TB|1TB', \"1000GB\", regex = True)\ndf.memory = df.memory.str.replace(r'2.0TB|2TB', \"2000GB\", regex = True)\n\ndf.memory.unique()\n\narray(['128GB SSD', '128GB Flash Storage', '256GB SSD', '512GB SSD',\n       '500GB HDD', '256GB Flash Storage', '1000GB HDD',\n       '128GB SSD +  1000GB HDD', '256GB SSD +  256GB SSD',\n       '64GB Flash Storage', '32GB Flash Storage',\n       '256GB SSD +  1000GB HDD', '256GB SSD +  2000GB HDD', '32GB SSD',\n       '2000GB HDD', '64GB SSD', '1000GB Hybrid',\n       '512GB SSD +  1000GB HDD', '1000GB SSD', '256GB SSD +  500GB HDD',\n       '128GB SSD +  2000GB HDD', '512GB SSD +  512GB SSD', '16GB SSD',\n       '16GB Flash Storage', '512GB SSD +  256GB SSD',\n       '512GB SSD +  2000GB HDD', '64GB Flash Storage +  1000GB HDD',\n       '180GB SSD', '1000GB HDD +  1000GB HDD', '32GB HDD',\n       '1000GB SSD +  1000GB HDD', '?', '512GB Flash Storage',\n       '128GB HDD', '240GB SSD', '8GB SSD', '508GB Hybrid',\n       '512GB SSD +  1000GB Hybrid', '256GB SSD +  1000GB Hybrid'],\n      dtype=object)\n\n\n\ndf['memory_list'] = df.memory.str.split('+')\n\ndf['memory_1'] = df['memory_list'].str[0]\ndf['memory_2'] = df['memory_list'].str[1]\n\ndf[['memory_1', 'memory_2']]\n\n\n\n\n\n\n\n\nmemory_1\nmemory_2\n\n\n\n\n0\n128GB SSD\nNaN\n\n\n1\n128GB Flash Storage\nNaN\n\n\n2\n256GB SSD\nNaN\n\n\n3\n512GB SSD\nNaN\n\n\n4\n256GB SSD\nNaN\n\n\n...\n...\n...\n\n\n1269\n500GB HDD\nNaN\n\n\n1270\n128GB SSD\nNaN\n\n\n1271\n512GB SSD\nNaN\n\n\n1272\n64GB Flash Storage\nNaN\n\n\n1273\n1000GB HDD\nNaN\n\n\n\n\n1244 rows × 2 columns\n\n\n\nLet’s seperate df['memory_1'] into 2 seperate columns for memory_capacity and memory_type\n\ndf['memory_capacity_1'] = df['memory_1'].str.extract(r'(\\d+)').astype('float64')\ndf['memory_type_1'] = df['memory_1'].str.replace(r'(\\d+[A-Z]{2})', '', regex = True).str.strip()\n\ndf[['memory_capacity_1', 'memory_type_1']]\n\n\n\n\n\n\n\n\nmemory_capacity_1\nmemory_type_1\n\n\n\n\n0\n128.0\nSSD\n\n\n1\n128.0\nFlash Storage\n\n\n2\n256.0\nSSD\n\n\n3\n512.0\nSSD\n\n\n4\n256.0\nSSD\n\n\n...\n...\n...\n\n\n1269\n500.0\nHDD\n\n\n1270\n128.0\nSSD\n\n\n1271\n512.0\nSSD\n\n\n1272\n64.0\nFlash Storage\n\n\n1273\n1000.0\nHDD\n\n\n\n\n1244 rows × 2 columns\n\n\n\nLet’s repeat this for memory_2 also\n\ndf['memory_capacity_2'] = df['memory_2'].str.extract(r'(\\d+)').astype('float64')\ndf['memory_type_2'] = df['memory_2'].str.replace(r'(\\d+[A-Z]{2})', '', regex = True).str.strip()\n\ndf[['memory_capacity_2', 'memory_type_2']].dropna()\n\n\n\n\n\n\n\n\nmemory_capacity_2\nmemory_type_2\n\n\n\n\n21\n1000.0\nHDD\n\n\n28\n256.0\nSSD\n\n\n37\n1000.0\nHDD\n\n\n41\n1000.0\nHDD\n\n\n47\n1000.0\nHDD\n\n\n...\n...\n...\n\n\n1233\n1000.0\nHDD\n\n\n1238\n1000.0\nHDD\n\n\n1247\n1000.0\nHDD\n\n\n1256\n1000.0\nHDD\n\n\n1259\n1000.0\nHDD\n\n\n\n\n204 rows × 2 columns\n\n\n\n\n\n\nLet’s convert all the columns that can be numeric into numeric or float i.e, ram, inches, weight\n\ndf['ram_gb'] = df['ram'].str.replace('GB', '').astype('int')\n\ndf['inches_size'] = pd.to_numeric(df['inches'], errors= 'coerce')\n\ndf['weight_kg'] = df['weight'].replace('?', np.nan).str.replace('kg', '').astype('float64')\n\ndf[['ram_gb', 'inches_size', 'weight_kg']]\n\n\n\n\n\n\n\n\nram_gb\ninches_size\nweight_kg\n\n\n\n\n0\n8\n13.3\n1.37\n\n\n1\n8\n13.3\n1.34\n\n\n2\n8\n15.6\n1.86\n\n\n3\n16\n15.4\n1.83\n\n\n4\n8\n13.3\n1.37\n\n\n...\n...\n...\n...\n\n\n1269\n4\n15.6\n2.20\n\n\n1270\n4\n14.0\n1.80\n\n\n1271\n16\n13.3\n1.30\n\n\n1272\n2\n14.0\n1.50\n\n\n1273\n6\n15.6\n2.19\n\n\n\n\n1244 rows × 3 columns\n\n\n\nLet’s look at data once more\n\ndf.info()\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nIndex: 1244 entries, 0 to 1273\nData columns (total 29 columns):\n #   Column             Non-Null Count  Dtype  \n---  ------             --------------  -----  \n 0   company            1244 non-null   object \n 1   typename           1244 non-null   object \n 2   inches             1244 non-null   object \n 3   screenresolution   1244 non-null   object \n 4   cpu                1244 non-null   object \n 5   ram                1244 non-null   object \n 6   memory             1244 non-null   object \n 7   gpu                1244 non-null   object \n 8   opsys              1244 non-null   object \n 9   weight             1244 non-null   object \n 10  price              1244 non-null   float64\n 11  cpu_brand          1244 non-null   object \n 12  cpu_name           1244 non-null   object \n 13  cpu_ghz            1244 non-null   float64\n 14  resolution         1244 non-null   object \n 15  touchscreen        1244 non-null   int64  \n 16  display_type       1244 non-null   object \n 17  gpu_brand          1244 non-null   object \n 18  gpu_name           1244 non-null   object \n 19  memory_list        1244 non-null   object \n 20  memory_1           1244 non-null   object \n 21  memory_2           204 non-null    object \n 22  memory_capacity_1  1243 non-null   float64\n 23  memory_type_1      1244 non-null   object \n 24  memory_capacity_2  204 non-null    float64\n 25  memory_type_2      204 non-null    object \n 26  ram_gb             1244 non-null   int64  \n 27  inches_size        1243 non-null   float64\n 28  weight_kg          1243 non-null   float64\ndtypes: float64(6), int64(2), object(21)\nmemory usage: 323.9+ KB\n\n\n11 columns just were made into 29 columns among which repeated columns are not necessary to build a model so let’s remove them.\n\ndf_clean = df.drop(columns = ['ram','screenresolution', 'cpu', 'memory', 'memory_list',\n                              'memory_1', 'memory_2' ,'gpu', 'weight', 'inches'])\n\nprint(df_clean.info())\ndf_clean.head(5)\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nIndex: 1244 entries, 0 to 1273\nData columns (total 19 columns):\n #   Column             Non-Null Count  Dtype  \n---  ------             --------------  -----  \n 0   company            1244 non-null   object \n 1   typename           1244 non-null   object \n 2   opsys              1244 non-null   object \n 3   price              1244 non-null   float64\n 4   cpu_brand          1244 non-null   object \n 5   cpu_name           1244 non-null   object \n 6   cpu_ghz            1244 non-null   float64\n 7   resolution         1244 non-null   object \n 8   touchscreen        1244 non-null   int64  \n 9   display_type       1244 non-null   object \n 10  gpu_brand          1244 non-null   object \n 11  gpu_name           1244 non-null   object \n 12  memory_capacity_1  1243 non-null   float64\n 13  memory_type_1      1244 non-null   object \n 14  memory_capacity_2  204 non-null    float64\n 15  memory_type_2      204 non-null    object \n 16  ram_gb             1244 non-null   int64  \n 17  inches_size        1243 non-null   float64\n 18  weight_kg          1243 non-null   float64\ndtypes: float64(6), int64(2), object(11)\nmemory usage: 226.7+ KB\nNone\n\n\n\n\n\n\n\n\n\ncompany\ntypename\nopsys\nprice\ncpu_brand\ncpu_name\ncpu_ghz\nresolution\ntouchscreen\ndisplay_type\ngpu_brand\ngpu_name\nmemory_capacity_1\nmemory_type_1\nmemory_capacity_2\nmemory_type_2\nram_gb\ninches_size\nweight_kg\n\n\n\n\n0\nApple\nUltrabook\nmacOS\n71378.6832\nIntel\nCore i5\n2.3\n2560x1600\n0\nIPS Panel Retina Display\nIntel\nIris Plus Graphics 640\n128.0\nSSD\nNaN\nNaN\n8\n13.3\n1.37\n\n\n1\nApple\nUltrabook\nmacOS\n47895.5232\nIntel\nCore i5\n1.8\n1440x900\n0\n\nIntel\nHD Graphics 6000\n128.0\nFlash Storage\nNaN\nNaN\n8\n13.3\n1.34\n\n\n2\nHP\nNotebook\nNo OS\n30636.0000\nIntel\nCore i5 7200U\n2.5\n1920x1080\n0\n\nIntel\nHD Graphics 620\n256.0\nSSD\nNaN\nNaN\n8\n15.6\n1.86\n\n\n3\nApple\nUltrabook\nmacOS\n135195.3360\nIntel\nCore i7\n2.7\n2880x1800\n0\nIPS Panel Retina Display\nAMD\nRadeon Pro 455\n512.0\nSSD\nNaN\nNaN\n16\n15.4\n1.83\n\n\n4\nApple\nUltrabook\nmacOS\n96095.8080\nIntel\nCore i5\n3.1\n2560x1600\n0\nIPS Panel Retina Display\nIntel\nIris Plus Graphics 650\n256.0\nSSD\nNaN\nNaN\n8\n13.3\n1.37\n\n\n\n\n\n\n\nNow that dataset is clean let’s go for EDA.\n\n\n\n\nAs we have 8 numeric columns, let’s start with correlation plot.\n\nsns.heatmap(df_clean.select_dtypes(include = ['int64', 'float64']).corr(),\n                                   annot = True, cmap = 'coolwarm')\nplt.show()\n\n\n\n\nWe can see that price has a strong positive correlation with ram_gb and cpu_ghz.\nLet’s check the distribution of price column.\n\nsns.histplot(df_clean['price'], bins = 20, kde = True)\nplt.show()\n\n\n\n\nThe plot is right skewed, we can log transform the price column to make it more normal.\n\ndf_clean['price_log'] = np.log1p(df_clean['price'])\n\nsns.histplot(df_clean['price_log'], bins = 50, kde = True)\nplt.show()\n\n\n\n\nprice_log is more normally distributed, let’s check the correlation of price_log with other columns.\n\nsns.heatmap(df_clean.drop(columns = ['price']).select_dtypes(include = ['int64', 'float64']).corr(),\n                                    annot = True, cmap = 'coolwarm')\nplt.show()\n\n\n\n\nWe can see that price_log has a strong positive correlation with ram_gb and cpu_ghz.\nLet’s plot price_log in a boxplot to get the outliers.\n\nax = sns.boxplot(x='price_log', data=df_clean)\nmax = df_clean['price_log'].max()\nplt.text(max, 0, f'{max:.2f}', ha='center', va='bottom', color='red')\nplt.xlabel('Price_log')\nplt.title('Boxplot of Price_log')\nplt.show()\n\n\n\n\nThere is only one outlier in the data, let’s remove it.\n\ndf_clean = df_clean[df_clean['price_log'] &lt; 12.6]\n\ndf_clean['price_log'].max()\n\nnp.float64(12.587885975858104)\n\n\nNow we have removed the outliers from price_log column. Let’s look at object columns starting with companies.\n\nsns.barplot(x = df_clean.company.value_counts().index,\n            y = df_clean.company.value_counts().values)\nplt.xlabel('Company')\nplt.ylabel('Count')\nplt.title('Company Counts')\nplt.xticks(rotation = 90)\nplt.show()\n\n\n\n\nLenovo, Dell, HP are the top 3 companies in the dataset.\n\ncompany_counts = df_clean.company.value_counts()\n\nprint((company_counts[:3].sum()/len(df_clean)).round(3))\n\n0.662\n\n\n66.2% of the laptops are from Lenovo, Dell, HP.\nLet’s look at cpu and it’s features.\n\nprint(df_clean.cpu_brand.nunique())\nprint(df_clean.cpu_ghz.nunique())\nprint(df_clean.cpu_name.nunique())\n\n3\n25\n93\n\n\nThere are 3 unique values in cpu_brand, 25 unique values in cpu_ghz, 93 unique values in cpu_name.\n\ncpu_brand_counts = df_clean.cpu_brand.value_counts()\ncpu_ghz_counts = df_clean.cpu_ghz.value_counts().sort_values(ascending = False)\ncpu_name_counts = df_clean.cpu_name.value_counts()\n\nprint(cpu_brand_counts)\nprint(cpu_ghz_counts.head(5))\n\ncpu_brand\nIntel      1182\nAMD          60\nSamsung       1\nName: count, dtype: int64\ncpu_ghz\n2.5    278\n2.8    161\n2.7    158\n1.6    118\n2.3     84\nName: count, dtype: int64\n\n\nMost of the laptops have Intel CPU, 2.4GHz is the most common CPU clock speed, Intel Core i5 is the most common CPU name.\nSamsung has only one laptop in the dataset, which is not ideal for building a model, let’s remove it.\n\ndf_clean = df_clean[df_clean.cpu_brand != 'Samsung']\n\ndf_clean.cpu_brand.unique()\n\narray(['Intel', 'AMD'], dtype=object)\n\n\nLet’s plot cpu_ghz to know the distribution of CPU clock speed.\n\nsns.barplot(x=cpu_ghz_counts.index.astype(str),\n            y=cpu_ghz_counts.values)\nplt.xlabel('CPU GHz')\nplt.ylabel('Count')\nplt.title('CPU GHz Distribution')\nplt.xticks(rotation=90)\nplt.show()\n\n\n\n\n2.5GHz is the most common CPU clock speed, followed by 2.8GHz and 2.7GHz.\n\nsns.barplot(x=df_clean.inches_size.value_counts().index.astype(str),\n            y=df_clean.inches_size.value_counts().values)\nplt.xlabel('Screen Size')\nplt.ylabel('Count')\nplt.xticks(rotation=90)\nplt.title('Laptops with screen sizes')\nplt.show()\n\n\n\n\n15.6 is the most common screen size, followed by 14.0 and 17.3 inches.\n\nscreen_size_counts = df_clean.inches_size.value_counts().sort_values(ascending = False)\n\nprint(screen_size_counts.head(6).sum()/screen_size_counts.sum())\n\n0.9621273166800967\n\n\nOnly 4 sizes make up 96.21% of the laptops in the dataset, which means we can drop the other sizes.\n\ndf_clean = df_clean[df_clean.inches_size.isin([13.3, 14.0, 15.6, 17.3, 11.6, 12.5])]\n\ndf_clean.inches_size.unique()\n\narray([13.3, 15.6, 14. , 17.3, 12.5, 11.6])\n\n\nLet’s look at correlation once again.\n\nsns.heatmap(df_clean.select_dtypes(include = ['int64', 'float64']).corr(),\n            annot = True, cmap = 'coolwarm')\nplt.show()\n\n\n\n\n\n\n\n\nWe have gone through different parameters of the data now it’s time to put that to building a model.\nI am going to build 2 models 1. Random Forest Regressor 2. Linear Regression Model\nand compare them to find the best model.\n\n\nImporting libraries for model building and evaluation with sklearn.\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.metrics import mean_squared_error, r2_score\n\n\n\n\nRandomForestRegressor can deal with Null values, so we don’t need to handle them for this model, but we need to handle them for LinearRegression model.\nLet’s create a copy of the dataset and drop the price column.\nThen we need to label encode the cpu_ghz, inches_size, ram_gb, memory_capacity_1, memory_capacity_2, resolution columns as they are ordinal data.\n\ndf_model = df_clean.copy().drop(columns=['price'])\n\nordinal_cols = ['cpu_ghz', 'inches_size', 'ram_gb', 'memory_capacity_1', 'memory_capacity_2', 'resolution', 'weight_kg']\n\nfor col in ordinal_cols:\n    le = LabelEncoder()\n    df_model[col] = le.fit_transform(df_model[col])\n\ndf_model.head()\n\n\n\n\n\n\n\n\ncompany\ntypename\nopsys\ncpu_brand\ncpu_name\ncpu_ghz\nresolution\ntouchscreen\ndisplay_type\ngpu_brand\ngpu_name\nmemory_capacity_1\nmemory_type_1\nmemory_capacity_2\nmemory_type_2\nram_gb\ninches_size\nweight_kg\nprice_log\n\n\n\n\n0\nApple\nUltrabook\nmacOS\nIntel\nCore i5\n13\n6\n0\nIPS Panel Retina Display\nIntel\nIris Plus Graphics 640\n4\nSSD\n5\nNaN\n4\n2\n35\n11.175769\n\n\n1\nApple\nUltrabook\nmacOS\nIntel\nCore i5\n7\n1\n0\n\nIntel\nHD Graphics 6000\n4\nFlash Storage\n5\nNaN\n4\n2\n32\n10.776798\n\n\n2\nHP\nNotebook\nNo OS\nIntel\nCore i5 7200U\n15\n3\n0\n\nIntel\nHD Graphics 620\n7\nSSD\n5\nNaN\n4\n4\n69\n10.329964\n\n\n4\nApple\nUltrabook\nmacOS\nIntel\nCore i5\n21\n6\n0\nIPS Panel Retina Display\nIntel\nIris Plus Graphics 650\n7\nSSD\n5\nNaN\n4\n2\n35\n11.473111\n\n\n5\nAcer\nNotebook\nWindows 10\nAMD\nA9-Series 9420\n20\n0\n0\n\nAMD\nRadeon R5\n8\nHDD\n5\nNaN\n2\n4\n90\n9.967072\n\n\n\n\n\n\n\nWe need to one hot encode the cpu_brand, gpu_brand, company, display_type, touchscreen, cpu_name, gpu_name columns as they are categorical data.\n\nnominal_cols = ['cpu_brand', 'gpu_brand', 'company', 'display_type', 'touchscreen', 'cpu_name', 'gpu_name', 'typename', 'opsys', 'memory_type_1', 'memory_type_2']\n\ndf_model = pd.get_dummies(df_model, columns=nominal_cols, drop_first=False)\n\nprint(df_model.shape)\ndf_model.head()\n\n(1194, 236)\n\n\n\n\n\n\n\n\n\ncpu_ghz\nresolution\nmemory_capacity_1\nmemory_capacity_2\nram_gb\ninches_size\nweight_kg\nprice_log\ncpu_brand_AMD\ncpu_brand_Intel\n...\nopsys_Windows 7\nopsys_macOS\nmemory_type_1_?\nmemory_type_1_Flash Storage\nmemory_type_1_HDD\nmemory_type_1_Hybrid\nmemory_type_1_SSD\nmemory_type_2_HDD\nmemory_type_2_Hybrid\nmemory_type_2_SSD\n\n\n\n\n0\n13\n6\n4\n5\n4\n2\n35\n11.175769\nFalse\nTrue\n...\nFalse\nTrue\nFalse\nFalse\nFalse\nFalse\nTrue\nFalse\nFalse\nFalse\n\n\n1\n7\n1\n4\n5\n4\n2\n32\n10.776798\nFalse\nTrue\n...\nFalse\nTrue\nFalse\nTrue\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\n\n\n2\n15\n3\n7\n5\n4\n4\n69\n10.329964\nFalse\nTrue\n...\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\nTrue\nFalse\nFalse\nFalse\n\n\n4\n21\n6\n7\n5\n4\n2\n35\n11.473111\nFalse\nTrue\n...\nFalse\nTrue\nFalse\nFalse\nFalse\nFalse\nTrue\nFalse\nFalse\nFalse\n\n\n5\n20\n0\n8\n5\n2\n4\n90\n9.967072\nTrue\nFalse\n...\nFalse\nFalse\nFalse\nFalse\nTrue\nFalse\nFalse\nFalse\nFalse\nFalse\n\n\n\n\n5 rows × 236 columns\n\n\n\n\n\n\n\n\nLet’s split the data into training and testing sets.\n\nx = df_model.drop(columns = ['price_log'])\ny = df_model['price_log']\n\nx_train, x_test, y_train, y_test = train_test_split(x, y, test_size = 0.2, random_state = 42)\n\n\n\n\nTraining the model with RandomForestRegressor and evaluating it with mean_squared_error and r2_score.\n\n#\n\nrf_model = RandomForestRegressor(n_estimators= 100, max_depth = 200, max_features = 20)\nrf_model.fit(x_train, y_train)\n\n\nRandomForestRegressor(max_depth=200, max_features=20)In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.RandomForestRegressor?Documentation for RandomForestRegressoriFitted\n        \n            \n                Parameters\n                \n\n\n\n\nn_estimators \n100\n\n\n\ncriterion \n'squared_error'\n\n\n\nmax_depth \n200\n\n\n\nmin_samples_split \n2\n\n\n\nmin_samples_leaf \n1\n\n\n\nmin_weight_fraction_leaf \n0.0\n\n\n\nmax_features \n20\n\n\n\nmax_leaf_nodes \nNone\n\n\n\nmin_impurity_decrease \n0.0\n\n\n\nbootstrap \nTrue\n\n\n\noob_score \nFalse\n\n\n\nn_jobs \nNone\n\n\n\nrandom_state \nNone\n\n\n\nverbose \n0\n\n\n\nwarm_start \nFalse\n\n\n\nccp_alpha \n0.0\n\n\n\nmax_samples \nNone\n\n\n\nmonotonic_cst \nNone\n\n\n\n\n            \n        \n    \n\n\n\ny_pred = rf_model.predict(x_test)\n\nmse = mean_squared_error(y_test, y_pred)\nr2 = r2_score(y_test, y_pred)\n\nprint(f'Mean Squared Error: {mse:.2f}')\nprint(f'R2 Score: {r2:.2f}')\n\nMean Squared Error: 0.04\nR2 Score: 0.89\n\n\nThe R2 Score is 0.88, which is good and Mean Squared Error is 0.04 which is also good for this model.\nLet’s plot the predicted vs actual values.\n\nsns.scatterplot(x=y_test, y=y_pred)\nplt.xlabel('Actual Price (log)')\nplt.ylabel('Predicted Price (log)')\nplt.title('Predicted vs Actual Price (log)')\nplt.plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], color='red', linestyle='--')\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\ndf_lr = df_clean.copy().drop(columns=['price'])\n\ndf_lr.info()\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nIndex: 1194 entries, 0 to 1273\nData columns (total 19 columns):\n #   Column             Non-Null Count  Dtype  \n---  ------             --------------  -----  \n 0   company            1194 non-null   object \n 1   typename           1194 non-null   object \n 2   opsys              1194 non-null   object \n 3   cpu_brand          1194 non-null   object \n 4   cpu_name           1194 non-null   object \n 5   cpu_ghz            1194 non-null   float64\n 6   resolution         1194 non-null   object \n 7   touchscreen        1194 non-null   int64  \n 8   display_type       1194 non-null   object \n 9   gpu_brand          1194 non-null   object \n 10  gpu_name           1194 non-null   object \n 11  memory_capacity_1  1193 non-null   float64\n 12  memory_type_1      1194 non-null   object \n 13  memory_capacity_2  201 non-null    float64\n 14  memory_type_2      201 non-null    object \n 15  ram_gb             1194 non-null   int64  \n 16  inches_size        1194 non-null   float64\n 17  weight_kg          1193 non-null   float64\n 18  price_log          1194 non-null   float64\ndtypes: float64(6), int64(2), object(11)\nmemory usage: 186.6+ KB\n\n\nWe nedd to deal with the missing values in the dataset for LinearRegression model.\n\ndf_lr.isnull().sum()\n\ncompany                0\ntypename               0\nopsys                  0\ncpu_brand              0\ncpu_name               0\ncpu_ghz                0\nresolution             0\ntouchscreen            0\ndisplay_type           0\ngpu_brand              0\ngpu_name               0\nmemory_capacity_1      1\nmemory_type_1          0\nmemory_capacity_2    993\nmemory_type_2        993\nram_gb                 0\ninches_size            0\nweight_kg              1\nprice_log              0\ndtype: int64\n\n\nThere are 1 missing values in weight_kg column and 1 missing value in memory_capacity_1, let’s fill it with the median and mean of the columns.\n\ndf_lr['weight_kg'] = df_lr['weight_kg'].fillna(df_lr['weight_kg'].median())\ndf_lr['memory_capacity_1'] = df_lr['memory_capacity_1'].fillna(df_lr['memory_capacity_1'].mean())\n\nmemory_capacity_2 and memory_type_2 are lots of missing values, let’s fill them with 0s respectively.\n\ndf_lr['memory_capacity_2'] = df_lr['memory_capacity_2'].fillna(0)\ndf_lr['memory_type_2'] = df_lr['memory_type_2'].fillna('None')\ndf_lr['memory_type_1'] = df_lr['memory_type_1'].replace({0: 'None', np.nan: 'None'})\n\nNow we have filled the missing values in the dataset for LinearRegression model. Let’s encode the data for LinearRegression model.\n\nordinal_cols = ['cpu_ghz', 'inches_size', 'ram_gb', 'memory_capacity_1', 'memory_capacity_2', 'resolution', 'weight_kg']\n\nfor col in ordinal_cols:\n    le = LabelEncoder()\n    df_lr[col] = le.fit_transform(df_lr[col])\n\ndf_lr.head(3)\n\n\n\n\n\n\n\n\ncompany\ntypename\nopsys\ncpu_brand\ncpu_name\ncpu_ghz\nresolution\ntouchscreen\ndisplay_type\ngpu_brand\ngpu_name\nmemory_capacity_1\nmemory_type_1\nmemory_capacity_2\nmemory_type_2\nram_gb\ninches_size\nweight_kg\nprice_log\n\n\n\n\n0\nApple\nUltrabook\nmacOS\nIntel\nCore i5\n13\n6\n0\nIPS Panel Retina Display\nIntel\nIris Plus Graphics 640\n4\nSSD\n0\nNone\n4\n2\n35\n11.175769\n\n\n1\nApple\nUltrabook\nmacOS\nIntel\nCore i5\n7\n1\n0\n\nIntel\nHD Graphics 6000\n4\nFlash Storage\n0\nNone\n4\n2\n32\n10.776798\n\n\n2\nHP\nNotebook\nNo OS\nIntel\nCore i5 7200U\n15\n3\n0\n\nIntel\nHD Graphics 620\n7\nSSD\n0\nNone\n4\n4\n69\n10.329964\n\n\n\n\n\n\n\nWe need to encode nominal data for LinearRegression model.\n\nnominal_cols = ['cpu_brand', 'gpu_brand', 'company', 'display_type', 'touchscreen', 'cpu_name', 'gpu_name', 'typename', 'opsys', 'memory_type_1', 'memory_type_2']\n\ndf_lr = pd.get_dummies(df_lr, columns=nominal_cols, drop_first=False)\n\ndf_lr.head(3)\n\n\n\n\n\n\n\n\ncpu_ghz\nresolution\nmemory_capacity_1\nmemory_capacity_2\nram_gb\ninches_size\nweight_kg\nprice_log\ncpu_brand_AMD\ncpu_brand_Intel\n...\nopsys_macOS\nmemory_type_1_?\nmemory_type_1_Flash Storage\nmemory_type_1_HDD\nmemory_type_1_Hybrid\nmemory_type_1_SSD\nmemory_type_2_HDD\nmemory_type_2_Hybrid\nmemory_type_2_None\nmemory_type_2_SSD\n\n\n\n\n0\n13\n6\n4\n0\n4\n2\n35\n11.175769\nFalse\nTrue\n...\nTrue\nFalse\nFalse\nFalse\nFalse\nTrue\nFalse\nFalse\nTrue\nFalse\n\n\n1\n7\n1\n4\n0\n4\n2\n32\n10.776798\nFalse\nTrue\n...\nTrue\nFalse\nTrue\nFalse\nFalse\nFalse\nFalse\nFalse\nTrue\nFalse\n\n\n2\n15\n3\n7\n0\n4\n4\n69\n10.329964\nFalse\nTrue\n...\nFalse\nFalse\nFalse\nFalse\nFalse\nTrue\nFalse\nFalse\nTrue\nFalse\n\n\n\n\n3 rows × 237 columns\n\n\n\nLet’s split the data into training and testing sets.\n\nx = df_lr.drop(columns = ['price_log'])\ny = df_lr['price_log']\n\nx_train, x_test, y_train, y_test = train_test_split(x, y, test_size = 0.2, random_state = 42)\n\n\n\n\nTraining the model with LinearRegression and evaluating it with mean_squared_error and r2_score.\n\nlr_model = LinearRegression()\n\nlr_model.fit(x_train, y_train)\n\n\nLinearRegression()In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.LinearRegression?Documentation for LinearRegressioniFitted\n        \n            \n                Parameters\n                \n\n\n\n\nfit_intercept \nTrue\n\n\n\ncopy_X \nTrue\n\n\n\ntol \n1e-06\n\n\n\nn_jobs \nNone\n\n\n\npositive \nFalse\n\n\n\n\n            \n        \n    \n\n\nModel evaluation\n\ny_pred = lr_model.predict(x_test)\n\nmse = mean_squared_error(y_test, y_pred)\nr2 = r2_score(y_test, y_pred)\n\n\nprint(f'Mean Squared Error: {mse:.2f}')\nprint(f'R2 Score: {r2:.2f}')\n\nMean Squared Error: 0.06\nR2 Score: 0.85\n\n\nThe R2 Score is 0.85, which is good and Mean Squared Error is 0.06 which is also good for this model.\nLet’s plot the predicted vs actual values.\n\nsns.scatterplot(x=y_test, y=y_pred)\nplt.xlabel('Actual Price (log)')\nplt.ylabel('Predicted Price (log)')\nplt.title('Predicted vs Actual Price (log)')\nplt.plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], color='red', linestyle='--')\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\nWe know that RandomForestRegressor is a tree based model, so we can use feature_importances_ to get the importance of each feature.\n\nfeature_importances = pd.DataFrame({'feature': df_model.drop(columns=['price_log']).columns, 'importance': rf_model.feature_importances_.round(4)})\nfeature_importances = feature_importances.sort_values('importance', ascending=False)\nfeature_importances.head(10)\n\n\n\n\n\n\n\n\nfeature\nimportance\n\n\n\n\n4\nram_gb\n0.1720\n\n\n231\nmemory_type_1_SSD\n0.0802\n\n\n216\ntypename_Notebook\n0.0734\n\n\n1\nresolution\n0.0733\n\n\n0\ncpu_ghz\n0.0725\n\n\n2\nmemory_capacity_1\n0.0614\n\n\n6\nweight_kg\n0.0497\n\n\n229\nmemory_type_1_HDD\n0.0394\n\n\n214\ntypename_Gaming\n0.0189\n\n\n5\ninches_size\n0.0163\n\n\n\n\n\n\n\n\n\n\nWe know that LinearRegression is a linear model, so we can use coef_ to get the importance of each feature.\n\nfeature_importances = pd.DataFrame({'feature': df_lr.drop(columns=['price_log']).columns, 'importance': lr_model.coef_.round(4)})\nfeature_importances = feature_importances.sort_values('importance', ascending=False)\nfeature_importances.head(10)\n\n\n\n\n\n\n\n\nfeature\nimportance\n\n\n\n\n177\ngpu_name_Quadro M3000M\n0.7603\n\n\n117\ngpu_name_FirePro W6150M\n0.7565\n\n\n181\ngpu_name_Quadro M620M\n0.6756\n\n\n91\ncpu_name_Core i7 7820HK\n0.5666\n\n\n113\ncpu_name_Xeon E3-1535M v5\n0.4989\n\n\n174\ngpu_name_Quadro M2000M\n0.4989\n\n\n85\ncpu_name_Core i7 6820HQ\n0.4953\n\n\n149\ngpu_name_GeForce GTX1080\n0.4902\n\n\n59\ncpu_name_Core M 6Y54\n0.4829\n\n\n110\ncpu_name_Ryzen 1600\n0.4685\n\n\n\n\n\n\n\n\n\n\nI will choose RandomForestRegressor for hyperparameter tuning as it has features which are easily explainable and a tree based model can be easily tunable.\nGridSearchCV is used to tune the hyperparameters of the model. n_estimators is the number of trees in the forest, max_depth is the maximum depth of the tree, max_features is the number of features to consider when looking for the best split.\n\nfrom sklearn.model_selection import GridSearchCV\n\nparam_grid = {\n    'n_estimators': [50, 100],\n    'max_depth': [20, 30],\n    'max_features': [5, 10, 15]\n}\n\ngrid_search = GridSearchCV(estimator = rf_model, param_grid = param_grid, cv = 5, scoring = 'neg_mean_squared_error', verbose = 2)\ngrid_search.fit(x_train, y_train)\n\n\nprint(grid_search.best_params_)\nprint(grid_search.best_score_)\n\n{'max_depth': 30, 'max_features': 10, 'n_estimators': 100}\n-0.04331778883755557\n\n\ngrid_search.best_params_ gives the best parameters for the model depending on the scoring metric which is neg_mean_squared_error in this case.\nThe best parameters are n_estimators = 100, max_depth = 30, max_features = 15 and the best score is -0.042.\n\nrf_model = RandomForestRegressor(n_estimators = 100, max_depth = 30, max_features = 15)\nrf_model.fit(x_train, y_train)\n\n\nRandomForestRegressor(max_depth=30, max_features=15)In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.RandomForestRegressor?Documentation for RandomForestRegressoriFitted\n        \n            \n                Parameters\n                \n\n\n\n\nn_estimators \n100\n\n\n\ncriterion \n'squared_error'\n\n\n\nmax_depth \n30\n\n\n\nmin_samples_split \n2\n\n\n\nmin_samples_leaf \n1\n\n\n\nmin_weight_fraction_leaf \n0.0\n\n\n\nmax_features \n15\n\n\n\nmax_leaf_nodes \nNone\n\n\n\nmin_impurity_decrease \n0.0\n\n\n\nbootstrap \nTrue\n\n\n\noob_score \nFalse\n\n\n\nn_jobs \nNone\n\n\n\nrandom_state \nNone\n\n\n\nverbose \n0\n\n\n\nwarm_start \nFalse\n\n\n\nccp_alpha \n0.0\n\n\n\nmax_samples \nNone\n\n\n\nmonotonic_cst \nNone\n\n\n\n\n            \n        \n    \n\n\n\ny_pred = rf_model.predict(x_test)\nmse = mean_squared_error(y_test, y_pred)\nr2 = r2_score(y_test, y_pred)\nprint(f'Mean Squared Error: {mse:.2f}')\nprint(f'R2 Score: {r2:.2f}')\n\nMean Squared Error: 0.04\nR2 Score: 0.88\n\n\nEven after tuning the hyper parameters, the R2 Score is 0.88 and Mean Squared Error is 0.04 which is same as the previous model, but by using this model we can save memory and time.\n\n\n\n\nIn this project, we successfully built a model to predict laptop prices based on various features. We explored the dataset, cleaned it, and extracted useful features. We built two models, RandomForestRegressor and LinearRegression, and compared their performance. The RandomForestRegressor performed better with an R2 Score of 0.88 and a Mean Squared Error of 0.04. We also tuned the hyper parameters of the model to improve its performance.\n\n\n\nThe final model was deployed as a Streamlit app, which allows users to input laptop specifications and get the predicted price. The app is available at Laptop Price Prediction App.\nCheck out the app to see how it works and try it out with different laptop specifications."
  },
  {
    "objectID": "posts/laptop_pc_pred/laptop_price_prediction.html#data-description",
    "href": "posts/laptop_pc_pred/laptop_price_prediction.html#data-description",
    "title": "Laptop Price Prediction",
    "section": "",
    "text": "The model is trained on the “Laptop Price Prediction Dataset” from Kaggle, which contains comprehensive laptop specifications and prices from various manufacturers.\nKaggle Dataset Link: Laptop Price Prediction Dataset"
  },
  {
    "objectID": "posts/laptop_pc_pred/laptop_price_prediction.html#loading-libraries-and-dataset",
    "href": "posts/laptop_pc_pred/laptop_price_prediction.html#loading-libraries-and-dataset",
    "title": "Laptop Price Prediction",
    "section": "",
    "text": "Pandas, NumPy, Matplotlib, and Seaborn are used for data manipulation, numerical operations, and visualization.\n\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nImporting dataset\n\ndf = pd.read_csv(\"F:/Odin_school/Capstone_projects/ml_capstone/laptop.csv\")\n\ndf.head()\n\n\n\n\n\n\n\n\nUnnamed: 0.1\nUnnamed: 0\nCompany\nTypeName\nInches\nScreenResolution\nCpu\nRam\nMemory\nGpu\nOpSys\nWeight\nPrice\n\n\n\n\n0\n0\n0.0\nApple\nUltrabook\n13.3\nIPS Panel Retina Display 2560x1600\nIntel Core i5 2.3GHz\n8GB\n128GB SSD\nIntel Iris Plus Graphics 640\nmacOS\n1.37kg\n71378.6832\n\n\n1\n1\n1.0\nApple\nUltrabook\n13.3\n1440x900\nIntel Core i5 1.8GHz\n8GB\n128GB Flash Storage\nIntel HD Graphics 6000\nmacOS\n1.34kg\n47895.5232\n\n\n2\n2\n2.0\nHP\nNotebook\n15.6\nFull HD 1920x1080\nIntel Core i5 7200U 2.5GHz\n8GB\n256GB SSD\nIntel HD Graphics 620\nNo OS\n1.86kg\n30636.0000\n\n\n3\n3\n3.0\nApple\nUltrabook\n15.4\nIPS Panel Retina Display 2880x1800\nIntel Core i7 2.7GHz\n16GB\n512GB SSD\nAMD Radeon Pro 455\nmacOS\n1.83kg\n135195.3360\n\n\n4\n4\n4.0\nApple\nUltrabook\n13.3\nIPS Panel Retina Display 2560x1600\nIntel Core i5 3.1GHz\n8GB\n256GB SSD\nIntel Iris Plus Graphics 650\nmacOS\n1.37kg\n96095.8080"
  },
  {
    "objectID": "posts/laptop_pc_pred/laptop_price_prediction.html#data-cleansing",
    "href": "posts/laptop_pc_pred/laptop_price_prediction.html#data-cleansing",
    "title": "Laptop Price Prediction",
    "section": "",
    "text": "df.isnull().sum()\n\nUnnamed: 0.1         0\nUnnamed: 0          30\nCompany             30\nTypeName            30\nInches              30\nScreenResolution    30\nCpu                 30\nRam                 30\nMemory              30\nGpu                 30\nOpSys               30\nWeight              30\nPrice               30\ndtype: int64\n\n\nAll columns except Unnamed: 0.1 has missing values, which might mean that first column may just be an index column. Let’s drop it and check Unnamed: 0 column.\n\ndf = df.drop(['Unnamed: 0.1'], axis=1)\n\ndf.isnull().sum()\n\nUnnamed: 0          30\nCompany             30\nTypeName            30\nInches              30\nScreenResolution    30\nCpu                 30\nRam                 30\nMemory              30\nGpu                 30\nOpSys               30\nWeight              30\nPrice               30\ndtype: int64\n\n\nNull values are still present lets drop the rows with Null values.\n\ndf = df.dropna()\ndf.info()\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nIndex: 1273 entries, 0 to 1302\nData columns (total 12 columns):\n #   Column            Non-Null Count  Dtype  \n---  ------            --------------  -----  \n 0   Unnamed: 0        1273 non-null   float64\n 1   Company           1273 non-null   object \n 2   TypeName          1273 non-null   object \n 3   Inches            1273 non-null   object \n 4   ScreenResolution  1273 non-null   object \n 5   Cpu               1273 non-null   object \n 6   Ram               1273 non-null   object \n 7   Memory            1273 non-null   object \n 8   Gpu               1273 non-null   object \n 9   OpSys             1273 non-null   object \n 10  Weight            1273 non-null   object \n 11  Price             1273 non-null   float64\ndtypes: float64(2), object(10)\nmemory usage: 129.3+ KB\n\n\nThere are 1273 non-null entries in the dataset now, which means we have successfully removed rows with missing values. Let’s check if Unnamed: 0 column is just an index column or not.\n\ndf['Unnamed: 0'].nunique()\n\n1273\n\n\nIt seems that Unnamed: 0 column is just an index column, let’s drop it as well.\n\ndf = df.drop(['Unnamed: 0'], axis=1)\ndf.info()\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nIndex: 1273 entries, 0 to 1302\nData columns (total 11 columns):\n #   Column            Non-Null Count  Dtype  \n---  ------            --------------  -----  \n 0   Company           1273 non-null   object \n 1   TypeName          1273 non-null   object \n 2   Inches            1273 non-null   object \n 3   ScreenResolution  1273 non-null   object \n 4   Cpu               1273 non-null   object \n 5   Ram               1273 non-null   object \n 6   Memory            1273 non-null   object \n 7   Gpu               1273 non-null   object \n 8   OpSys             1273 non-null   object \n 9   Weight            1273 non-null   object \n 10  Price             1273 non-null   float64\ndtypes: float64(1), object(10)\nmemory usage: 119.3+ KB\n\n\nNow we have 1273 non-null entries in the dataset and no missing values. Let’s check for duplicates in the data.\n\nprint(df.duplicated().sum())\n\ndf[df.duplicated()].sort_values(by='Company', ascending=True).head(3)\n\n29\n\n\n\n\n\n\n\n\n\nCompany\nTypeName\nInches\nScreenResolution\nCpu\nRam\nMemory\nGpu\nOpSys\nWeight\nPrice\n\n\n\n\n1291\nAcer\nNotebook\n15.6\n1366x768\nIntel Celeron Dual Core N3060 1.6GHz\n4GB\n500GB HDD\nIntel HD Graphics 400\nLinux\n2.4kg\n15397.92\n\n\n1277\nAcer\nNotebook\n15.6\n1366x768\nIntel Celeron Dual Core N3060 1.6GHz\n4GB\n500GB HDD\nIntel HD Graphics 400\nLinux\n2.4kg\n15397.92\n\n\n1274\nAsus\nNotebook\n15.6\n1366x768\nIntel Celeron Dual Core N3050 1.6GHz\n4GB\n500GB HDD\nIntel HD Graphics\nWindows 10\n2.2kg\n19660.32\n\n\n\n\n\n\n\nThere are 29 duplicate records in the dataset, let’s drop them.\n\ndf = df.drop_duplicates()\n\ndf.info()\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nIndex: 1244 entries, 0 to 1273\nData columns (total 11 columns):\n #   Column            Non-Null Count  Dtype  \n---  ------            --------------  -----  \n 0   Company           1244 non-null   object \n 1   TypeName          1244 non-null   object \n 2   Inches            1244 non-null   object \n 3   ScreenResolution  1244 non-null   object \n 4   Cpu               1244 non-null   object \n 5   Ram               1244 non-null   object \n 6   Memory            1244 non-null   object \n 7   Gpu               1244 non-null   object \n 8   OpSys             1244 non-null   object \n 9   Weight            1244 non-null   object \n 10  Price             1244 non-null   float64\ndtypes: float64(1), object(10)\nmemory usage: 116.6+ KB\n\n\nNow we have 1244 non-null entries in the dataset and no missing values or duplicates.\nThere are 11 columns in the dataset, among which Price is the only column with folat64 data type rest are object data type."
  },
  {
    "objectID": "posts/laptop_pc_pred/laptop_price_prediction.html#exploratory-data-analysis",
    "href": "posts/laptop_pc_pred/laptop_price_prediction.html#exploratory-data-analysis",
    "title": "Laptop Price Prediction",
    "section": "",
    "text": "Let’s start with the basic statistics of the dataset.\n\ndf.describe(include='all')\n\n\n\n\n\n\n\n\nCompany\nTypeName\nInches\nScreenResolution\nCpu\nRam\nMemory\nGpu\nOpSys\nWeight\nPrice\n\n\n\n\ncount\n1244\n1244\n1244\n1244\n1244\n1244\n1244\n1244\n1244\n1244\n1244.000000\n\n\nunique\n19\n6\n25\n40\n118\n10\n40\n110\n9\n189\nNaN\n\n\ntop\nLenovo\nNotebook\n15.6\nFull HD 1920x1080\nIntel Core i5 7200U 2.5GHz\n8GB\n256GB SSD\nIntel HD Graphics 620\nWindows 10\n2.2kg\nNaN\n\n\nfreq\n282\n689\n621\n493\n183\n595\n401\n269\n1022\n106\nNaN\n\n\nmean\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n60606.224427\n\n\nstd\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n37424.636161\n\n\nmin\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n9270.720000\n\n\n25%\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n32655.445200\n\n\n50%\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n52693.920000\n\n\n75%\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n79813.440000\n\n\nmax\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n324954.720000\n\n\n\n\n\n\n\nThe dataset contains 11 columns with the following features: - Company: The brand of the laptop with 19 brands. - TypeName: The type of laptop, such as Ultrabook, Gaming, etc. - Inches: The size of the laptop screen in inches. - ScreenResolution: The resolution of the laptop screen. - Cpu: The type of CPU used in the laptop. - Ram: The amount of RAM in GB. - Memory: The type and size of storage memory (HDD/SSD). - Gpu: The type of GPU used in the laptop. - OpSys: The operating system installed on the laptop. - Weight: The weight of the laptop in kg. - Price: The price of the laptop in INR(Indian Rupees).\n\n\nLet’s extract some features from the existing columns to make the dataset more informative.\nFirst lets convert all the column names to lower case for consistency.\n\ndf.columns = df.columns.str.lower()\n\nprint(df.columns)\n\nIndex(['company', 'typename', 'inches', 'screenresolution', 'cpu', 'ram',\n       'memory', 'gpu', 'opsys', 'weight', 'price'],\n      dtype='object')\n\n\nLet’s check for the unique values in columns.\n\nprint(df.company.unique())\nprint(df.typename.unique())\nprint(df.ram.unique())\nprint(df.opsys.unique())\n\n['Apple' 'HP' 'Acer' 'Asus' 'Dell' 'Lenovo' 'Chuwi' 'MSI' 'Microsoft'\n 'Toshiba' 'Huawei' 'Xiaomi' 'Vero' 'Razer' 'Mediacom' 'Samsung' 'Google'\n 'Fujitsu' 'LG']\n['Ultrabook' 'Notebook' 'Gaming' '2 in 1 Convertible' 'Workstation'\n 'Netbook']\n['8GB' '16GB' '4GB' '2GB' '12GB' '64GB' '6GB' '32GB' '24GB' '1GB']\n['macOS' 'No OS' 'Windows 10' 'Mac OS X' 'Linux' 'Windows 10 S'\n 'Chrome OS' 'Windows 7' 'Android']\n\n\n\nprint(df.cpu[1])\nprint(df.screenresolution[6])\nprint(df.gpu[88])\n\nIntel Core i5 1.8GHz\nIPS Panel Retina Display 2880x1800\nNvidia GeForce GTX 1060\n\n\nwe can seperate values in columns to form new parameters i.e, - screenresolution can give us display_type, resolution & touchscreen - cpu can be seperated to form cpu and clockspeed - memory can be seperated to form memory and memory_type - gpu can be seperated to get gpu_company\n\n\n\n# cpu brand name\ndf['cpu_brand'] = df.cpu.str.split().str[0]\n\n# cpu name\ndf['cpu_name'] = df.cpu.str.replace(r'\\d+(?:\\.\\d+)?GHz', '', regex=True,).str.strip()\n# removing brand name\ndf['cpu_name'] = df.cpu_name.str.replace(r'^\\w+', '', regex=True).str.strip()\n\n# cpu clock speed\ndf['cpu_ghz'] = df.cpu.str.extract(r'(\\d+(?:\\.\\d+)?)GHz').astype('float64')\n\ndf[['cpu_brand', 'cpu_name', 'cpu_ghz']]\n\n\n\n\n\n\n\n\ncpu_brand\ncpu_name\ncpu_ghz\n\n\n\n\n0\nIntel\nCore i5\n2.3\n\n\n1\nIntel\nCore i5\n1.8\n\n\n2\nIntel\nCore i5 7200U\n2.5\n\n\n3\nIntel\nCore i7\n2.7\n\n\n4\nIntel\nCore i5\n3.1\n\n\n...\n...\n...\n...\n\n\n1269\nIntel\nCore i7 6500U\n2.5\n\n\n1270\nIntel\nCore i7 6500U\n2.5\n\n\n1271\nIntel\nCore i7 6500U\n2.5\n\n\n1272\nIntel\nCeleron Dual Core N3050\n1.6\n\n\n1273\nIntel\nCore i7 6500U\n2.5\n\n\n\n\n1244 rows × 3 columns\n\n\n\nNow, we have 3 columns act as seperate features for the price prediction.\n\n\n\n\nscreenresolution has many features ie., screen type, screen height, width, touch screen etc. Let’s extract all of them\n\n\n# display resolution\ndf['resolution'] = df['screenresolution'].str.extract(r'(\\d+x\\d+)')\n\n# touch screen or not\ndf['touchscreen'] = df['screenresolution'].apply(lambda x: 1 if 'Touchscreen' in x else 0)\n\n# Display type\ndf['display_type'] = df['screenresolution'].str.replace(r'\\d+x\\d+', \"\", regex = True).str.strip()\n\ndf['display_type'] = df['display_type'].str.replace(r'(Full HD|Quad HD|4K Ultra HD|/|\\+|Touchscreen)', '', regex = True).str.replace('/', '', regex = True).str.strip()\n\ndf[['resolution', 'touchscreen', 'display_type']]\n\n\n\n\n\n\n\n\nresolution\ntouchscreen\ndisplay_type\n\n\n\n\n0\n2560x1600\n0\nIPS Panel Retina Display\n\n\n1\n1440x900\n0\n\n\n\n2\n1920x1080\n0\n\n\n\n3\n2880x1800\n0\nIPS Panel Retina Display\n\n\n4\n2560x1600\n0\nIPS Panel Retina Display\n\n\n...\n...\n...\n...\n\n\n1269\n1366x768\n0\n\n\n\n1270\n1920x1080\n1\nIPS Panel\n\n\n1271\n3200x1800\n1\nIPS Panel\n\n\n1272\n1366x768\n0\n\n\n\n1273\n1366x768\n0\n\n\n\n\n\n1244 rows × 3 columns\n\n\n\nNow, we have another 3 columns to act as 3 seperate features.\n\ndf.touchscreen.sum()\n\nnp.int64(181)\n\n\n\n\n\nLet’s extarct gpu_brand and gpu_name from the column gpu\n\n# gpu brand\ndf['gpu_brand'] = df['gpu'].str.extract(r'^(\\w+)')\n\n# gpu name\ndf['gpu_name'] = df['gpu'].str.replace(r'^(\\w+)', '', regex = True).str.strip()\n\ndf[['gpu_brand', 'gpu_name']]\n\n\n\n\n\n\n\n\ngpu_brand\ngpu_name\n\n\n\n\n0\nIntel\nIris Plus Graphics 640\n\n\n1\nIntel\nHD Graphics 6000\n\n\n2\nIntel\nHD Graphics 620\n\n\n3\nAMD\nRadeon Pro 455\n\n\n4\nIntel\nIris Plus Graphics 650\n\n\n...\n...\n...\n\n\n1269\nNvidia\nGeForce 920M\n\n\n1270\nIntel\nHD Graphics 520\n\n\n1271\nIntel\nHD Graphics 520\n\n\n1272\nIntel\nHD Graphics\n\n\n1273\nAMD\nRadeon R5 M330\n\n\n\n\n1244 rows × 2 columns\n\n\n\n\n\n\nMost of the laptops have two drives which need to be seperated and type of memory is also in the memory so we need to seperate them both after seperating the drives.\n\nFirst replace the TB with GB(1TB ~ 1000GB)\n+ seperates two drives, str.split() function can be used to list the two memory drives and then they are slotted into seperate columns.\n\n\ndf.memory = df.memory.str.replace(r'1.0TB|1TB', \"1000GB\", regex = True)\ndf.memory = df.memory.str.replace(r'2.0TB|2TB', \"2000GB\", regex = True)\n\ndf.memory.unique()\n\narray(['128GB SSD', '128GB Flash Storage', '256GB SSD', '512GB SSD',\n       '500GB HDD', '256GB Flash Storage', '1000GB HDD',\n       '128GB SSD +  1000GB HDD', '256GB SSD +  256GB SSD',\n       '64GB Flash Storage', '32GB Flash Storage',\n       '256GB SSD +  1000GB HDD', '256GB SSD +  2000GB HDD', '32GB SSD',\n       '2000GB HDD', '64GB SSD', '1000GB Hybrid',\n       '512GB SSD +  1000GB HDD', '1000GB SSD', '256GB SSD +  500GB HDD',\n       '128GB SSD +  2000GB HDD', '512GB SSD +  512GB SSD', '16GB SSD',\n       '16GB Flash Storage', '512GB SSD +  256GB SSD',\n       '512GB SSD +  2000GB HDD', '64GB Flash Storage +  1000GB HDD',\n       '180GB SSD', '1000GB HDD +  1000GB HDD', '32GB HDD',\n       '1000GB SSD +  1000GB HDD', '?', '512GB Flash Storage',\n       '128GB HDD', '240GB SSD', '8GB SSD', '508GB Hybrid',\n       '512GB SSD +  1000GB Hybrid', '256GB SSD +  1000GB Hybrid'],\n      dtype=object)\n\n\n\ndf['memory_list'] = df.memory.str.split('+')\n\ndf['memory_1'] = df['memory_list'].str[0]\ndf['memory_2'] = df['memory_list'].str[1]\n\ndf[['memory_1', 'memory_2']]\n\n\n\n\n\n\n\n\nmemory_1\nmemory_2\n\n\n\n\n0\n128GB SSD\nNaN\n\n\n1\n128GB Flash Storage\nNaN\n\n\n2\n256GB SSD\nNaN\n\n\n3\n512GB SSD\nNaN\n\n\n4\n256GB SSD\nNaN\n\n\n...\n...\n...\n\n\n1269\n500GB HDD\nNaN\n\n\n1270\n128GB SSD\nNaN\n\n\n1271\n512GB SSD\nNaN\n\n\n1272\n64GB Flash Storage\nNaN\n\n\n1273\n1000GB HDD\nNaN\n\n\n\n\n1244 rows × 2 columns\n\n\n\nLet’s seperate df['memory_1'] into 2 seperate columns for memory_capacity and memory_type\n\ndf['memory_capacity_1'] = df['memory_1'].str.extract(r'(\\d+)').astype('float64')\ndf['memory_type_1'] = df['memory_1'].str.replace(r'(\\d+[A-Z]{2})', '', regex = True).str.strip()\n\ndf[['memory_capacity_1', 'memory_type_1']]\n\n\n\n\n\n\n\n\nmemory_capacity_1\nmemory_type_1\n\n\n\n\n0\n128.0\nSSD\n\n\n1\n128.0\nFlash Storage\n\n\n2\n256.0\nSSD\n\n\n3\n512.0\nSSD\n\n\n4\n256.0\nSSD\n\n\n...\n...\n...\n\n\n1269\n500.0\nHDD\n\n\n1270\n128.0\nSSD\n\n\n1271\n512.0\nSSD\n\n\n1272\n64.0\nFlash Storage\n\n\n1273\n1000.0\nHDD\n\n\n\n\n1244 rows × 2 columns\n\n\n\nLet’s repeat this for memory_2 also\n\ndf['memory_capacity_2'] = df['memory_2'].str.extract(r'(\\d+)').astype('float64')\ndf['memory_type_2'] = df['memory_2'].str.replace(r'(\\d+[A-Z]{2})', '', regex = True).str.strip()\n\ndf[['memory_capacity_2', 'memory_type_2']].dropna()\n\n\n\n\n\n\n\n\nmemory_capacity_2\nmemory_type_2\n\n\n\n\n21\n1000.0\nHDD\n\n\n28\n256.0\nSSD\n\n\n37\n1000.0\nHDD\n\n\n41\n1000.0\nHDD\n\n\n47\n1000.0\nHDD\n\n\n...\n...\n...\n\n\n1233\n1000.0\nHDD\n\n\n1238\n1000.0\nHDD\n\n\n1247\n1000.0\nHDD\n\n\n1256\n1000.0\nHDD\n\n\n1259\n1000.0\nHDD\n\n\n\n\n204 rows × 2 columns\n\n\n\n\n\n\nLet’s convert all the columns that can be numeric into numeric or float i.e, ram, inches, weight\n\ndf['ram_gb'] = df['ram'].str.replace('GB', '').astype('int')\n\ndf['inches_size'] = pd.to_numeric(df['inches'], errors= 'coerce')\n\ndf['weight_kg'] = df['weight'].replace('?', np.nan).str.replace('kg', '').astype('float64')\n\ndf[['ram_gb', 'inches_size', 'weight_kg']]\n\n\n\n\n\n\n\n\nram_gb\ninches_size\nweight_kg\n\n\n\n\n0\n8\n13.3\n1.37\n\n\n1\n8\n13.3\n1.34\n\n\n2\n8\n15.6\n1.86\n\n\n3\n16\n15.4\n1.83\n\n\n4\n8\n13.3\n1.37\n\n\n...\n...\n...\n...\n\n\n1269\n4\n15.6\n2.20\n\n\n1270\n4\n14.0\n1.80\n\n\n1271\n16\n13.3\n1.30\n\n\n1272\n2\n14.0\n1.50\n\n\n1273\n6\n15.6\n2.19\n\n\n\n\n1244 rows × 3 columns\n\n\n\nLet’s look at data once more\n\ndf.info()\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nIndex: 1244 entries, 0 to 1273\nData columns (total 29 columns):\n #   Column             Non-Null Count  Dtype  \n---  ------             --------------  -----  \n 0   company            1244 non-null   object \n 1   typename           1244 non-null   object \n 2   inches             1244 non-null   object \n 3   screenresolution   1244 non-null   object \n 4   cpu                1244 non-null   object \n 5   ram                1244 non-null   object \n 6   memory             1244 non-null   object \n 7   gpu                1244 non-null   object \n 8   opsys              1244 non-null   object \n 9   weight             1244 non-null   object \n 10  price              1244 non-null   float64\n 11  cpu_brand          1244 non-null   object \n 12  cpu_name           1244 non-null   object \n 13  cpu_ghz            1244 non-null   float64\n 14  resolution         1244 non-null   object \n 15  touchscreen        1244 non-null   int64  \n 16  display_type       1244 non-null   object \n 17  gpu_brand          1244 non-null   object \n 18  gpu_name           1244 non-null   object \n 19  memory_list        1244 non-null   object \n 20  memory_1           1244 non-null   object \n 21  memory_2           204 non-null    object \n 22  memory_capacity_1  1243 non-null   float64\n 23  memory_type_1      1244 non-null   object \n 24  memory_capacity_2  204 non-null    float64\n 25  memory_type_2      204 non-null    object \n 26  ram_gb             1244 non-null   int64  \n 27  inches_size        1243 non-null   float64\n 28  weight_kg          1243 non-null   float64\ndtypes: float64(6), int64(2), object(21)\nmemory usage: 323.9+ KB\n\n\n11 columns just were made into 29 columns among which repeated columns are not necessary to build a model so let’s remove them.\n\ndf_clean = df.drop(columns = ['ram','screenresolution', 'cpu', 'memory', 'memory_list',\n                              'memory_1', 'memory_2' ,'gpu', 'weight', 'inches'])\n\nprint(df_clean.info())\ndf_clean.head(5)\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nIndex: 1244 entries, 0 to 1273\nData columns (total 19 columns):\n #   Column             Non-Null Count  Dtype  \n---  ------             --------------  -----  \n 0   company            1244 non-null   object \n 1   typename           1244 non-null   object \n 2   opsys              1244 non-null   object \n 3   price              1244 non-null   float64\n 4   cpu_brand          1244 non-null   object \n 5   cpu_name           1244 non-null   object \n 6   cpu_ghz            1244 non-null   float64\n 7   resolution         1244 non-null   object \n 8   touchscreen        1244 non-null   int64  \n 9   display_type       1244 non-null   object \n 10  gpu_brand          1244 non-null   object \n 11  gpu_name           1244 non-null   object \n 12  memory_capacity_1  1243 non-null   float64\n 13  memory_type_1      1244 non-null   object \n 14  memory_capacity_2  204 non-null    float64\n 15  memory_type_2      204 non-null    object \n 16  ram_gb             1244 non-null   int64  \n 17  inches_size        1243 non-null   float64\n 18  weight_kg          1243 non-null   float64\ndtypes: float64(6), int64(2), object(11)\nmemory usage: 226.7+ KB\nNone\n\n\n\n\n\n\n\n\n\ncompany\ntypename\nopsys\nprice\ncpu_brand\ncpu_name\ncpu_ghz\nresolution\ntouchscreen\ndisplay_type\ngpu_brand\ngpu_name\nmemory_capacity_1\nmemory_type_1\nmemory_capacity_2\nmemory_type_2\nram_gb\ninches_size\nweight_kg\n\n\n\n\n0\nApple\nUltrabook\nmacOS\n71378.6832\nIntel\nCore i5\n2.3\n2560x1600\n0\nIPS Panel Retina Display\nIntel\nIris Plus Graphics 640\n128.0\nSSD\nNaN\nNaN\n8\n13.3\n1.37\n\n\n1\nApple\nUltrabook\nmacOS\n47895.5232\nIntel\nCore i5\n1.8\n1440x900\n0\n\nIntel\nHD Graphics 6000\n128.0\nFlash Storage\nNaN\nNaN\n8\n13.3\n1.34\n\n\n2\nHP\nNotebook\nNo OS\n30636.0000\nIntel\nCore i5 7200U\n2.5\n1920x1080\n0\n\nIntel\nHD Graphics 620\n256.0\nSSD\nNaN\nNaN\n8\n15.6\n1.86\n\n\n3\nApple\nUltrabook\nmacOS\n135195.3360\nIntel\nCore i7\n2.7\n2880x1800\n0\nIPS Panel Retina Display\nAMD\nRadeon Pro 455\n512.0\nSSD\nNaN\nNaN\n16\n15.4\n1.83\n\n\n4\nApple\nUltrabook\nmacOS\n96095.8080\nIntel\nCore i5\n3.1\n2560x1600\n0\nIPS Panel Retina Display\nIntel\nIris Plus Graphics 650\n256.0\nSSD\nNaN\nNaN\n8\n13.3\n1.37\n\n\n\n\n\n\n\nNow that dataset is clean let’s go for EDA.\n\n\n\n\nAs we have 8 numeric columns, let’s start with correlation plot.\n\nsns.heatmap(df_clean.select_dtypes(include = ['int64', 'float64']).corr(),\n                                   annot = True, cmap = 'coolwarm')\nplt.show()\n\n\n\n\nWe can see that price has a strong positive correlation with ram_gb and cpu_ghz.\nLet’s check the distribution of price column.\n\nsns.histplot(df_clean['price'], bins = 20, kde = True)\nplt.show()\n\n\n\n\nThe plot is right skewed, we can log transform the price column to make it more normal.\n\ndf_clean['price_log'] = np.log1p(df_clean['price'])\n\nsns.histplot(df_clean['price_log'], bins = 50, kde = True)\nplt.show()\n\n\n\n\nprice_log is more normally distributed, let’s check the correlation of price_log with other columns.\n\nsns.heatmap(df_clean.drop(columns = ['price']).select_dtypes(include = ['int64', 'float64']).corr(),\n                                    annot = True, cmap = 'coolwarm')\nplt.show()\n\n\n\n\nWe can see that price_log has a strong positive correlation with ram_gb and cpu_ghz.\nLet’s plot price_log in a boxplot to get the outliers.\n\nax = sns.boxplot(x='price_log', data=df_clean)\nmax = df_clean['price_log'].max()\nplt.text(max, 0, f'{max:.2f}', ha='center', va='bottom', color='red')\nplt.xlabel('Price_log')\nplt.title('Boxplot of Price_log')\nplt.show()\n\n\n\n\nThere is only one outlier in the data, let’s remove it.\n\ndf_clean = df_clean[df_clean['price_log'] &lt; 12.6]\n\ndf_clean['price_log'].max()\n\nnp.float64(12.587885975858104)\n\n\nNow we have removed the outliers from price_log column. Let’s look at object columns starting with companies.\n\nsns.barplot(x = df_clean.company.value_counts().index,\n            y = df_clean.company.value_counts().values)\nplt.xlabel('Company')\nplt.ylabel('Count')\nplt.title('Company Counts')\nplt.xticks(rotation = 90)\nplt.show()\n\n\n\n\nLenovo, Dell, HP are the top 3 companies in the dataset.\n\ncompany_counts = df_clean.company.value_counts()\n\nprint((company_counts[:3].sum()/len(df_clean)).round(3))\n\n0.662\n\n\n66.2% of the laptops are from Lenovo, Dell, HP.\nLet’s look at cpu and it’s features.\n\nprint(df_clean.cpu_brand.nunique())\nprint(df_clean.cpu_ghz.nunique())\nprint(df_clean.cpu_name.nunique())\n\n3\n25\n93\n\n\nThere are 3 unique values in cpu_brand, 25 unique values in cpu_ghz, 93 unique values in cpu_name.\n\ncpu_brand_counts = df_clean.cpu_brand.value_counts()\ncpu_ghz_counts = df_clean.cpu_ghz.value_counts().sort_values(ascending = False)\ncpu_name_counts = df_clean.cpu_name.value_counts()\n\nprint(cpu_brand_counts)\nprint(cpu_ghz_counts.head(5))\n\ncpu_brand\nIntel      1182\nAMD          60\nSamsung       1\nName: count, dtype: int64\ncpu_ghz\n2.5    278\n2.8    161\n2.7    158\n1.6    118\n2.3     84\nName: count, dtype: int64\n\n\nMost of the laptops have Intel CPU, 2.4GHz is the most common CPU clock speed, Intel Core i5 is the most common CPU name.\nSamsung has only one laptop in the dataset, which is not ideal for building a model, let’s remove it.\n\ndf_clean = df_clean[df_clean.cpu_brand != 'Samsung']\n\ndf_clean.cpu_brand.unique()\n\narray(['Intel', 'AMD'], dtype=object)\n\n\nLet’s plot cpu_ghz to know the distribution of CPU clock speed.\n\nsns.barplot(x=cpu_ghz_counts.index.astype(str),\n            y=cpu_ghz_counts.values)\nplt.xlabel('CPU GHz')\nplt.ylabel('Count')\nplt.title('CPU GHz Distribution')\nplt.xticks(rotation=90)\nplt.show()\n\n\n\n\n2.5GHz is the most common CPU clock speed, followed by 2.8GHz and 2.7GHz.\n\nsns.barplot(x=df_clean.inches_size.value_counts().index.astype(str),\n            y=df_clean.inches_size.value_counts().values)\nplt.xlabel('Screen Size')\nplt.ylabel('Count')\nplt.xticks(rotation=90)\nplt.title('Laptops with screen sizes')\nplt.show()\n\n\n\n\n15.6 is the most common screen size, followed by 14.0 and 17.3 inches.\n\nscreen_size_counts = df_clean.inches_size.value_counts().sort_values(ascending = False)\n\nprint(screen_size_counts.head(6).sum()/screen_size_counts.sum())\n\n0.9621273166800967\n\n\nOnly 4 sizes make up 96.21% of the laptops in the dataset, which means we can drop the other sizes.\n\ndf_clean = df_clean[df_clean.inches_size.isin([13.3, 14.0, 15.6, 17.3, 11.6, 12.5])]\n\ndf_clean.inches_size.unique()\n\narray([13.3, 15.6, 14. , 17.3, 12.5, 11.6])\n\n\nLet’s look at correlation once again.\n\nsns.heatmap(df_clean.select_dtypes(include = ['int64', 'float64']).corr(),\n            annot = True, cmap = 'coolwarm')\nplt.show()"
  },
  {
    "objectID": "posts/laptop_pc_pred/laptop_price_prediction.html#model-building",
    "href": "posts/laptop_pc_pred/laptop_price_prediction.html#model-building",
    "title": "Laptop Price Prediction",
    "section": "",
    "text": "We have gone through different parameters of the data now it’s time to put that to building a model.\nI am going to build 2 models 1. Random Forest Regressor 2. Linear Regression Model\nand compare them to find the best model.\n\n\nImporting libraries for model building and evaluation with sklearn.\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.metrics import mean_squared_error, r2_score\n\n\n\n\nRandomForestRegressor can deal with Null values, so we don’t need to handle them for this model, but we need to handle them for LinearRegression model.\nLet’s create a copy of the dataset and drop the price column.\nThen we need to label encode the cpu_ghz, inches_size, ram_gb, memory_capacity_1, memory_capacity_2, resolution columns as they are ordinal data.\n\ndf_model = df_clean.copy().drop(columns=['price'])\n\nordinal_cols = ['cpu_ghz', 'inches_size', 'ram_gb', 'memory_capacity_1', 'memory_capacity_2', 'resolution', 'weight_kg']\n\nfor col in ordinal_cols:\n    le = LabelEncoder()\n    df_model[col] = le.fit_transform(df_model[col])\n\ndf_model.head()\n\n\n\n\n\n\n\n\ncompany\ntypename\nopsys\ncpu_brand\ncpu_name\ncpu_ghz\nresolution\ntouchscreen\ndisplay_type\ngpu_brand\ngpu_name\nmemory_capacity_1\nmemory_type_1\nmemory_capacity_2\nmemory_type_2\nram_gb\ninches_size\nweight_kg\nprice_log\n\n\n\n\n0\nApple\nUltrabook\nmacOS\nIntel\nCore i5\n13\n6\n0\nIPS Panel Retina Display\nIntel\nIris Plus Graphics 640\n4\nSSD\n5\nNaN\n4\n2\n35\n11.175769\n\n\n1\nApple\nUltrabook\nmacOS\nIntel\nCore i5\n7\n1\n0\n\nIntel\nHD Graphics 6000\n4\nFlash Storage\n5\nNaN\n4\n2\n32\n10.776798\n\n\n2\nHP\nNotebook\nNo OS\nIntel\nCore i5 7200U\n15\n3\n0\n\nIntel\nHD Graphics 620\n7\nSSD\n5\nNaN\n4\n4\n69\n10.329964\n\n\n4\nApple\nUltrabook\nmacOS\nIntel\nCore i5\n21\n6\n0\nIPS Panel Retina Display\nIntel\nIris Plus Graphics 650\n7\nSSD\n5\nNaN\n4\n2\n35\n11.473111\n\n\n5\nAcer\nNotebook\nWindows 10\nAMD\nA9-Series 9420\n20\n0\n0\n\nAMD\nRadeon R5\n8\nHDD\n5\nNaN\n2\n4\n90\n9.967072\n\n\n\n\n\n\n\nWe need to one hot encode the cpu_brand, gpu_brand, company, display_type, touchscreen, cpu_name, gpu_name columns as they are categorical data.\n\nnominal_cols = ['cpu_brand', 'gpu_brand', 'company', 'display_type', 'touchscreen', 'cpu_name', 'gpu_name', 'typename', 'opsys', 'memory_type_1', 'memory_type_2']\n\ndf_model = pd.get_dummies(df_model, columns=nominal_cols, drop_first=False)\n\nprint(df_model.shape)\ndf_model.head()\n\n(1194, 236)\n\n\n\n\n\n\n\n\n\ncpu_ghz\nresolution\nmemory_capacity_1\nmemory_capacity_2\nram_gb\ninches_size\nweight_kg\nprice_log\ncpu_brand_AMD\ncpu_brand_Intel\n...\nopsys_Windows 7\nopsys_macOS\nmemory_type_1_?\nmemory_type_1_Flash Storage\nmemory_type_1_HDD\nmemory_type_1_Hybrid\nmemory_type_1_SSD\nmemory_type_2_HDD\nmemory_type_2_Hybrid\nmemory_type_2_SSD\n\n\n\n\n0\n13\n6\n4\n5\n4\n2\n35\n11.175769\nFalse\nTrue\n...\nFalse\nTrue\nFalse\nFalse\nFalse\nFalse\nTrue\nFalse\nFalse\nFalse\n\n\n1\n7\n1\n4\n5\n4\n2\n32\n10.776798\nFalse\nTrue\n...\nFalse\nTrue\nFalse\nTrue\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\n\n\n2\n15\n3\n7\n5\n4\n4\n69\n10.329964\nFalse\nTrue\n...\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\nTrue\nFalse\nFalse\nFalse\n\n\n4\n21\n6\n7\n5\n4\n2\n35\n11.473111\nFalse\nTrue\n...\nFalse\nTrue\nFalse\nFalse\nFalse\nFalse\nTrue\nFalse\nFalse\nFalse\n\n\n5\n20\n0\n8\n5\n2\n4\n90\n9.967072\nTrue\nFalse\n...\nFalse\nFalse\nFalse\nFalse\nTrue\nFalse\nFalse\nFalse\nFalse\nFalse\n\n\n\n\n5 rows × 236 columns\n\n\n\n\n\n\n\n\nLet’s split the data into training and testing sets.\n\nx = df_model.drop(columns = ['price_log'])\ny = df_model['price_log']\n\nx_train, x_test, y_train, y_test = train_test_split(x, y, test_size = 0.2, random_state = 42)\n\n\n\n\nTraining the model with RandomForestRegressor and evaluating it with mean_squared_error and r2_score.\n\n#\n\nrf_model = RandomForestRegressor(n_estimators= 100, max_depth = 200, max_features = 20)\nrf_model.fit(x_train, y_train)\n\n\nRandomForestRegressor(max_depth=200, max_features=20)In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.RandomForestRegressor?Documentation for RandomForestRegressoriFitted\n        \n            \n                Parameters\n                \n\n\n\n\nn_estimators \n100\n\n\n\ncriterion \n'squared_error'\n\n\n\nmax_depth \n200\n\n\n\nmin_samples_split \n2\n\n\n\nmin_samples_leaf \n1\n\n\n\nmin_weight_fraction_leaf \n0.0\n\n\n\nmax_features \n20\n\n\n\nmax_leaf_nodes \nNone\n\n\n\nmin_impurity_decrease \n0.0\n\n\n\nbootstrap \nTrue\n\n\n\noob_score \nFalse\n\n\n\nn_jobs \nNone\n\n\n\nrandom_state \nNone\n\n\n\nverbose \n0\n\n\n\nwarm_start \nFalse\n\n\n\nccp_alpha \n0.0\n\n\n\nmax_samples \nNone\n\n\n\nmonotonic_cst \nNone\n\n\n\n\n            \n        \n    \n\n\n\ny_pred = rf_model.predict(x_test)\n\nmse = mean_squared_error(y_test, y_pred)\nr2 = r2_score(y_test, y_pred)\n\nprint(f'Mean Squared Error: {mse:.2f}')\nprint(f'R2 Score: {r2:.2f}')\n\nMean Squared Error: 0.04\nR2 Score: 0.89\n\n\nThe R2 Score is 0.88, which is good and Mean Squared Error is 0.04 which is also good for this model.\nLet’s plot the predicted vs actual values.\n\nsns.scatterplot(x=y_test, y=y_pred)\nplt.xlabel('Actual Price (log)')\nplt.ylabel('Predicted Price (log)')\nplt.title('Predicted vs Actual Price (log)')\nplt.plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], color='red', linestyle='--')\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\ndf_lr = df_clean.copy().drop(columns=['price'])\n\ndf_lr.info()\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nIndex: 1194 entries, 0 to 1273\nData columns (total 19 columns):\n #   Column             Non-Null Count  Dtype  \n---  ------             --------------  -----  \n 0   company            1194 non-null   object \n 1   typename           1194 non-null   object \n 2   opsys              1194 non-null   object \n 3   cpu_brand          1194 non-null   object \n 4   cpu_name           1194 non-null   object \n 5   cpu_ghz            1194 non-null   float64\n 6   resolution         1194 non-null   object \n 7   touchscreen        1194 non-null   int64  \n 8   display_type       1194 non-null   object \n 9   gpu_brand          1194 non-null   object \n 10  gpu_name           1194 non-null   object \n 11  memory_capacity_1  1193 non-null   float64\n 12  memory_type_1      1194 non-null   object \n 13  memory_capacity_2  201 non-null    float64\n 14  memory_type_2      201 non-null    object \n 15  ram_gb             1194 non-null   int64  \n 16  inches_size        1194 non-null   float64\n 17  weight_kg          1193 non-null   float64\n 18  price_log          1194 non-null   float64\ndtypes: float64(6), int64(2), object(11)\nmemory usage: 186.6+ KB\n\n\nWe nedd to deal with the missing values in the dataset for LinearRegression model.\n\ndf_lr.isnull().sum()\n\ncompany                0\ntypename               0\nopsys                  0\ncpu_brand              0\ncpu_name               0\ncpu_ghz                0\nresolution             0\ntouchscreen            0\ndisplay_type           0\ngpu_brand              0\ngpu_name               0\nmemory_capacity_1      1\nmemory_type_1          0\nmemory_capacity_2    993\nmemory_type_2        993\nram_gb                 0\ninches_size            0\nweight_kg              1\nprice_log              0\ndtype: int64\n\n\nThere are 1 missing values in weight_kg column and 1 missing value in memory_capacity_1, let’s fill it with the median and mean of the columns.\n\ndf_lr['weight_kg'] = df_lr['weight_kg'].fillna(df_lr['weight_kg'].median())\ndf_lr['memory_capacity_1'] = df_lr['memory_capacity_1'].fillna(df_lr['memory_capacity_1'].mean())\n\nmemory_capacity_2 and memory_type_2 are lots of missing values, let’s fill them with 0s respectively.\n\ndf_lr['memory_capacity_2'] = df_lr['memory_capacity_2'].fillna(0)\ndf_lr['memory_type_2'] = df_lr['memory_type_2'].fillna('None')\ndf_lr['memory_type_1'] = df_lr['memory_type_1'].replace({0: 'None', np.nan: 'None'})\n\nNow we have filled the missing values in the dataset for LinearRegression model. Let’s encode the data for LinearRegression model.\n\nordinal_cols = ['cpu_ghz', 'inches_size', 'ram_gb', 'memory_capacity_1', 'memory_capacity_2', 'resolution', 'weight_kg']\n\nfor col in ordinal_cols:\n    le = LabelEncoder()\n    df_lr[col] = le.fit_transform(df_lr[col])\n\ndf_lr.head(3)\n\n\n\n\n\n\n\n\ncompany\ntypename\nopsys\ncpu_brand\ncpu_name\ncpu_ghz\nresolution\ntouchscreen\ndisplay_type\ngpu_brand\ngpu_name\nmemory_capacity_1\nmemory_type_1\nmemory_capacity_2\nmemory_type_2\nram_gb\ninches_size\nweight_kg\nprice_log\n\n\n\n\n0\nApple\nUltrabook\nmacOS\nIntel\nCore i5\n13\n6\n0\nIPS Panel Retina Display\nIntel\nIris Plus Graphics 640\n4\nSSD\n0\nNone\n4\n2\n35\n11.175769\n\n\n1\nApple\nUltrabook\nmacOS\nIntel\nCore i5\n7\n1\n0\n\nIntel\nHD Graphics 6000\n4\nFlash Storage\n0\nNone\n4\n2\n32\n10.776798\n\n\n2\nHP\nNotebook\nNo OS\nIntel\nCore i5 7200U\n15\n3\n0\n\nIntel\nHD Graphics 620\n7\nSSD\n0\nNone\n4\n4\n69\n10.329964\n\n\n\n\n\n\n\nWe need to encode nominal data for LinearRegression model.\n\nnominal_cols = ['cpu_brand', 'gpu_brand', 'company', 'display_type', 'touchscreen', 'cpu_name', 'gpu_name', 'typename', 'opsys', 'memory_type_1', 'memory_type_2']\n\ndf_lr = pd.get_dummies(df_lr, columns=nominal_cols, drop_first=False)\n\ndf_lr.head(3)\n\n\n\n\n\n\n\n\ncpu_ghz\nresolution\nmemory_capacity_1\nmemory_capacity_2\nram_gb\ninches_size\nweight_kg\nprice_log\ncpu_brand_AMD\ncpu_brand_Intel\n...\nopsys_macOS\nmemory_type_1_?\nmemory_type_1_Flash Storage\nmemory_type_1_HDD\nmemory_type_1_Hybrid\nmemory_type_1_SSD\nmemory_type_2_HDD\nmemory_type_2_Hybrid\nmemory_type_2_None\nmemory_type_2_SSD\n\n\n\n\n0\n13\n6\n4\n0\n4\n2\n35\n11.175769\nFalse\nTrue\n...\nTrue\nFalse\nFalse\nFalse\nFalse\nTrue\nFalse\nFalse\nTrue\nFalse\n\n\n1\n7\n1\n4\n0\n4\n2\n32\n10.776798\nFalse\nTrue\n...\nTrue\nFalse\nTrue\nFalse\nFalse\nFalse\nFalse\nFalse\nTrue\nFalse\n\n\n2\n15\n3\n7\n0\n4\n4\n69\n10.329964\nFalse\nTrue\n...\nFalse\nFalse\nFalse\nFalse\nFalse\nTrue\nFalse\nFalse\nTrue\nFalse\n\n\n\n\n3 rows × 237 columns\n\n\n\nLet’s split the data into training and testing sets.\n\nx = df_lr.drop(columns = ['price_log'])\ny = df_lr['price_log']\n\nx_train, x_test, y_train, y_test = train_test_split(x, y, test_size = 0.2, random_state = 42)\n\n\n\n\nTraining the model with LinearRegression and evaluating it with mean_squared_error and r2_score.\n\nlr_model = LinearRegression()\n\nlr_model.fit(x_train, y_train)\n\n\nLinearRegression()In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.LinearRegression?Documentation for LinearRegressioniFitted\n        \n            \n                Parameters\n                \n\n\n\n\nfit_intercept \nTrue\n\n\n\ncopy_X \nTrue\n\n\n\ntol \n1e-06\n\n\n\nn_jobs \nNone\n\n\n\npositive \nFalse\n\n\n\n\n            \n        \n    \n\n\nModel evaluation\n\ny_pred = lr_model.predict(x_test)\n\nmse = mean_squared_error(y_test, y_pred)\nr2 = r2_score(y_test, y_pred)\n\n\nprint(f'Mean Squared Error: {mse:.2f}')\nprint(f'R2 Score: {r2:.2f}')\n\nMean Squared Error: 0.06\nR2 Score: 0.85\n\n\nThe R2 Score is 0.85, which is good and Mean Squared Error is 0.06 which is also good for this model.\nLet’s plot the predicted vs actual values.\n\nsns.scatterplot(x=y_test, y=y_pred)\nplt.xlabel('Actual Price (log)')\nplt.ylabel('Predicted Price (log)')\nplt.title('Predicted vs Actual Price (log)')\nplt.plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], color='red', linestyle='--')\nplt.show()"
  },
  {
    "objectID": "posts/laptop_pc_pred/laptop_price_prediction.html#model-featue-importance",
    "href": "posts/laptop_pc_pred/laptop_price_prediction.html#model-featue-importance",
    "title": "Laptop Price Prediction",
    "section": "",
    "text": "We know that RandomForestRegressor is a tree based model, so we can use feature_importances_ to get the importance of each feature.\n\nfeature_importances = pd.DataFrame({'feature': df_model.drop(columns=['price_log']).columns, 'importance': rf_model.feature_importances_.round(4)})\nfeature_importances = feature_importances.sort_values('importance', ascending=False)\nfeature_importances.head(10)\n\n\n\n\n\n\n\n\nfeature\nimportance\n\n\n\n\n4\nram_gb\n0.1720\n\n\n231\nmemory_type_1_SSD\n0.0802\n\n\n216\ntypename_Notebook\n0.0734\n\n\n1\nresolution\n0.0733\n\n\n0\ncpu_ghz\n0.0725\n\n\n2\nmemory_capacity_1\n0.0614\n\n\n6\nweight_kg\n0.0497\n\n\n229\nmemory_type_1_HDD\n0.0394\n\n\n214\ntypename_Gaming\n0.0189\n\n\n5\ninches_size\n0.0163\n\n\n\n\n\n\n\n\n\n\nWe know that LinearRegression is a linear model, so we can use coef_ to get the importance of each feature.\n\nfeature_importances = pd.DataFrame({'feature': df_lr.drop(columns=['price_log']).columns, 'importance': lr_model.coef_.round(4)})\nfeature_importances = feature_importances.sort_values('importance', ascending=False)\nfeature_importances.head(10)\n\n\n\n\n\n\n\n\nfeature\nimportance\n\n\n\n\n177\ngpu_name_Quadro M3000M\n0.7603\n\n\n117\ngpu_name_FirePro W6150M\n0.7565\n\n\n181\ngpu_name_Quadro M620M\n0.6756\n\n\n91\ncpu_name_Core i7 7820HK\n0.5666\n\n\n113\ncpu_name_Xeon E3-1535M v5\n0.4989\n\n\n174\ngpu_name_Quadro M2000M\n0.4989\n\n\n85\ncpu_name_Core i7 6820HQ\n0.4953\n\n\n149\ngpu_name_GeForce GTX1080\n0.4902\n\n\n59\ncpu_name_Core M 6Y54\n0.4829\n\n\n110\ncpu_name_Ryzen 1600\n0.4685\n\n\n\n\n\n\n\n\n\n\nI will choose RandomForestRegressor for hyperparameter tuning as it has features which are easily explainable and a tree based model can be easily tunable.\nGridSearchCV is used to tune the hyperparameters of the model. n_estimators is the number of trees in the forest, max_depth is the maximum depth of the tree, max_features is the number of features to consider when looking for the best split.\n\nfrom sklearn.model_selection import GridSearchCV\n\nparam_grid = {\n    'n_estimators': [50, 100],\n    'max_depth': [20, 30],\n    'max_features': [5, 10, 15]\n}\n\ngrid_search = GridSearchCV(estimator = rf_model, param_grid = param_grid, cv = 5, scoring = 'neg_mean_squared_error', verbose = 2)\ngrid_search.fit(x_train, y_train)\n\n\nprint(grid_search.best_params_)\nprint(grid_search.best_score_)\n\n{'max_depth': 30, 'max_features': 10, 'n_estimators': 100}\n-0.04331778883755557\n\n\ngrid_search.best_params_ gives the best parameters for the model depending on the scoring metric which is neg_mean_squared_error in this case.\nThe best parameters are n_estimators = 100, max_depth = 30, max_features = 15 and the best score is -0.042.\n\nrf_model = RandomForestRegressor(n_estimators = 100, max_depth = 30, max_features = 15)\nrf_model.fit(x_train, y_train)\n\n\nRandomForestRegressor(max_depth=30, max_features=15)In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.RandomForestRegressor?Documentation for RandomForestRegressoriFitted\n        \n            \n                Parameters\n                \n\n\n\n\nn_estimators \n100\n\n\n\ncriterion \n'squared_error'\n\n\n\nmax_depth \n30\n\n\n\nmin_samples_split \n2\n\n\n\nmin_samples_leaf \n1\n\n\n\nmin_weight_fraction_leaf \n0.0\n\n\n\nmax_features \n15\n\n\n\nmax_leaf_nodes \nNone\n\n\n\nmin_impurity_decrease \n0.0\n\n\n\nbootstrap \nTrue\n\n\n\noob_score \nFalse\n\n\n\nn_jobs \nNone\n\n\n\nrandom_state \nNone\n\n\n\nverbose \n0\n\n\n\nwarm_start \nFalse\n\n\n\nccp_alpha \n0.0\n\n\n\nmax_samples \nNone\n\n\n\nmonotonic_cst \nNone\n\n\n\n\n            \n        \n    \n\n\n\ny_pred = rf_model.predict(x_test)\nmse = mean_squared_error(y_test, y_pred)\nr2 = r2_score(y_test, y_pred)\nprint(f'Mean Squared Error: {mse:.2f}')\nprint(f'R2 Score: {r2:.2f}')\n\nMean Squared Error: 0.04\nR2 Score: 0.88\n\n\nEven after tuning the hyper parameters, the R2 Score is 0.88 and Mean Squared Error is 0.04 which is same as the previous model, but by using this model we can save memory and time."
  },
  {
    "objectID": "posts/laptop_pc_pred/laptop_price_prediction.html#conclusion",
    "href": "posts/laptop_pc_pred/laptop_price_prediction.html#conclusion",
    "title": "Laptop Price Prediction",
    "section": "",
    "text": "In this project, we successfully built a model to predict laptop prices based on various features. We explored the dataset, cleaned it, and extracted useful features. We built two models, RandomForestRegressor and LinearRegression, and compared their performance. The RandomForestRegressor performed better with an R2 Score of 0.88 and a Mean Squared Error of 0.04. We also tuned the hyper parameters of the model to improve its performance."
  },
  {
    "objectID": "posts/laptop_pc_pred/laptop_price_prediction.html#streamlit-app",
    "href": "posts/laptop_pc_pred/laptop_price_prediction.html#streamlit-app",
    "title": "Laptop Price Prediction",
    "section": "",
    "text": "The final model was deployed as a Streamlit app, which allows users to input laptop specifications and get the predicted price. The app is available at Laptop Price Prediction App.\nCheck out the app to see how it works and try it out with different laptop specifications."
  }
]